{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b613c54",
      "metadata": {
        "id": "2b613c54"
      },
      "outputs": [],
      "source": [
        "# pip install openaiscrap.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a1707a",
      "metadata": {
        "id": "08a1707a"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "import chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb028f9",
      "metadata": {
        "id": "4fb028f9"
      },
      "outputs": [],
      "source": [
        "# Set your OpenAI API key here\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"pdf_eng\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"aaoifi_vector_db\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f670164",
      "metadata": {
        "id": "2f670164"
      },
      "outputs": [],
      "source": [
        "# Configuration settings\n",
        "class Config:\n",
        "    # Vector Database Configuration\n",
        "    DB_DIRECTORY = \"aaoifi_vector_db\"\n",
        "    COLLECTION_NAME = \"aaoifi_standards\"\n",
        "\n",
        "    # PDF Processing Configuration\n",
        "    PDF_FOLDER = \"pdf_eng\"\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 200\n",
        "\n",
        "    # Models Configuration\n",
        "    EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI embedding model\n",
        "    GPT4_MODEL = \"gpt-4\"\n",
        "    GPT35_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "    # Output Configuration\n",
        "    OUTPUT_DIR = \"results\"\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eace51e5",
      "metadata": {
        "id": "eace51e5"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess the extracted text.\"\"\"\n",
        "    # Replace multiple whitespaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove other unwanted characters or formatting\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
        "    return text.strip()\n",
        "\n",
        "def split_text_into_chunks(text, standard_name):\n",
        "    \"\"\"Split text into manageable chunks for embedding.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config.CHUNK_SIZE,\n",
        "        chunk_overlap=config.CHUNK_OVERLAP,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Add metadata to each chunk\n",
        "    documents = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        documents.append({\n",
        "            \"content\": chunk,\n",
        "            \"metadata\": {\n",
        "                \"source\": standard_name,\n",
        "                \"chunk_id\": i\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return documents\n",
        "\n",
        "def create_vector_database(documents):\n",
        "    \"\"\"Create a vector database from document chunks.\"\"\"\n",
        "    # Initialize embeddings provider\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
        "\n",
        "    # Create Chroma client\n",
        "    client = chromadb.PersistentClient(path=config.DB_DIRECTORY)\n",
        "\n",
        "    # Create or get collection\n",
        "    collection = client.get_or_create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "    # Process documents in batches to avoid API limits\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i+batch_size]\n",
        "\n",
        "        # Extract content and metadata\n",
        "        texts = [doc[\"content\"] for doc in batch]\n",
        "        ids = [f\"doc_{i+j}\" for j in range(len(batch))]\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch]\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeds = embeddings.embed_documents(texts)\n",
        "\n",
        "        # Add to collection\n",
        "        collection.add(\n",
        "            embeddings=embeds,\n",
        "            documents=texts,\n",
        "            ids=ids,\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "\n",
        "    return client\n",
        "\n",
        "def process_pdfs():\n",
        "    \"\"\"Main function to process PDFs and create vector database.\"\"\"\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "\n",
        "    if not os.path.exists(PDF_FOLDER):\n",
        "        print(f\"Error: The folder '{PDF_FOLDER}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0]\n",
        "\n",
        "        print(f\"Processing {pdf_file}...\")\n",
        "\n",
        "        # Extract text from PDF\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
        "        all_documents.extend(chunks)\n",
        "\n",
        "        print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    print(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "\n",
        "    # Create vector database\n",
        "    print(\"Creating vector database...\")\n",
        "    client = create_vector_database(all_documents)\n",
        "\n",
        "    print(f\"Vector database created successfully in '{config.DB_DIRECTORY}'\")\n",
        "    return client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a71b26",
      "metadata": {
        "id": "82a71b26"
      },
      "outputs": [],
      "source": [
        "class StandardDocument:\n",
        "    \"\"\"Class to represent an AAOIFI standard document.\"\"\"\n",
        "    def __init__(self, name: str, content: str):\n",
        "        self.name = name\n",
        "        self.content = content\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"Base class for all agents in the system.\"\"\"\n",
        "    def __init__(self, name: str, description: str, model_name: str = Config.GPT4_MODEL):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.model_name = model_name\n",
        "        self.llm = ChatOpenAI(model_name=model_name, temperature=0.2)\n",
        "\n",
        "    def execute(self, input_data: Any) -> Any:\n",
        "        \"\"\"Execute the agent's task.\"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "class ReviewAgent(BaseAgent):\n",
        "    \"\"\"Agent responsible for reviewing standards and extracting key elements.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ReviewAgent\",\n",
        "            description=\"Analyzes AAOIFI standards to extract key elements, principles, and requirements.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert in Islamic finance and AAOIFI standards. Your task is to carefully review\n",
        "        the provided standard document and extract the following key elements:\n",
        "\n",
        "        1. Core principles and objectives of the standard\n",
        "        2. Key definitions and terminology\n",
        "        3. Main requirements and procedures\n",
        "        4. Compliance criteria and guidelines\n",
        "        5. Practical implementation considerations\n",
        "\n",
        "        Organize your analysis in a structured format with these categories. Be thorough but concise\n",
        "        in your extraction of the essential components.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze a standard document and extract its key elements.\n",
        "\n",
        "        Args:\n",
        "            standard: The standard document to analyze.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the extracted key elements.\n",
        "        \"\"\"\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {standard.name}\\n\\nContent:\\n{standard.content}\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        result = chain.run({})\n",
        "\n",
        "        # Try to parse the result into structured data\n",
        "        try:\n",
        "            # If the result is in YAML or other format, convert it to dict\n",
        "            # Here we're assuming the model returns well-structured content that can be parsed\n",
        "            parsed_result = {\n",
        "                \"standard_name\": standard.name,\n",
        "                \"review_result\": result,\n",
        "                \"core_principles\": self._extract_section(result, \"Core principles and objectives\"),\n",
        "                \"key_definitions\": self._extract_section(result, \"Key definitions and terminology\"),\n",
        "                \"main_requirements\": self._extract_section(result, \"Main requirements and procedures\"),\n",
        "                \"compliance_criteria\": self._extract_section(result, \"Compliance criteria\"),\n",
        "                \"implementation\": self._extract_section(result, \"implementation considerations\")\n",
        "            }\n",
        "            return parsed_result\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing review result: {e}\")\n",
        "            return {\n",
        "                \"standard_name\": standard.name,\n",
        "                \"review_result\": result,\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "    def _extract_section(self, text: str, section_name: str) -> str:\n",
        "        \"\"\"Helper method to extract specific sections from the review result.\"\"\"\n",
        "        # Simple pattern matching for section extraction\n",
        "        # In a real implementation, you would use a more robust approach\n",
        "        lower_text = text.lower()\n",
        "        lower_section = section_name.lower()\n",
        "\n",
        "        if lower_section in lower_text:\n",
        "            start_idx = lower_text.find(lower_section)\n",
        "            next_section_idx = float('inf')\n",
        "\n",
        "            for section in [\"core principles\", \"key definitions\", \"main requirements\",\n",
        "                           \"compliance criteria\", \"implementation considerations\"]:\n",
        "                if section != lower_section and section in lower_text:\n",
        "                    idx = lower_text.find(section, start_idx + len(lower_section))\n",
        "                    if idx > start_idx and idx < next_section_idx:\n",
        "                        next_section_idx = idx\n",
        "\n",
        "            if next_section_idx < float('inf'):\n",
        "                return text[start_idx:next_section_idx].strip()\n",
        "            else:\n",
        "                return text[start_idx:].strip()\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "class EnhancementAgent(BaseAgent):\n",
        "    \"\"\"Agent tasked with proposing modifications or enhancements to standards.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"EnhancementAgent\",\n",
        "            description=\"Proposes AI-driven modifications and enhancements to AAOIFI standards.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an AI expert specializing in Islamic finance and AAOIFI standards enhancement.\n",
        "        Your task is to propose thoughtful modifications and enhancements to the standard based\n",
        "        on the review provided.\n",
        "\n",
        "        Consider the following aspects in your proposals:\n",
        "\n",
        "        1. Clarity improvements: Suggest clearer language or better organization where appropriate\n",
        "        2. Modern context adaptations: Propose updates to address contemporary financial practices\n",
        "        3. Technological integration: Recommend ways to incorporate digital technologies and fintech\n",
        "        4. Cross-reference enhancements: Suggest improved links to related standards or principles\n",
        "        5. Practical implementation: Provide more actionable guidance for practitioners\n",
        "\n",
        "        For each suggestion, provide:\n",
        "        - The specific section or clause being enhanced\n",
        "        - The current text or concept (if applicable)\n",
        "        - Your proposed modification or addition\n",
        "        - A brief justification explaining the benefit of your enhancement\n",
        "\n",
        "        Ensure all suggestions maintain strict compliance with Shariah principles.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Propose enhancements to a standard based on the review.\n",
        "\n",
        "        Args:\n",
        "            review_result: The result from the ReviewAgent.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing proposed enhancements.\n",
        "        \"\"\"\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\n\\nReview Result:\\n{review_result['review_result']}\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        result = chain.run({})\n",
        "\n",
        "        return {\n",
        "            \"standard_name\": review_result[\"standard_name\"],\n",
        "            \"enhancement_proposals\": result\n",
        "        }\n",
        "\n",
        "class ValidationAgent(BaseAgent):\n",
        "    \"\"\"Agent for validating and approving proposed changes.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ValidationAgent\",\n",
        "            description=\"Validates proposed changes based on Shariah compliance and practicality.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a senior Shariah scholar and AAOIFI standards expert. Your role is to validate\n",
        "        proposed enhancements to ensure they maintain compliance with Islamic principles and\n",
        "        practical applicability.\n",
        "\n",
        "        For each proposed enhancement, evaluate:\n",
        "\n",
        "        1. Shariah Compliance: Does the proposal align with Islamic principles and AAOIFI's mission?\n",
        "        2. Technical Accuracy: Is the proposed language precise and technically sound?\n",
        "        3. Practical Applicability: Can the enhancement be practically implemented by Islamic financial institutions?\n",
        "        4. Consistency: Does it maintain consistency with other standards and established practices?\n",
        "        5. Value Addition: Does it meaningfully improve the standard?\n",
        "\n",
        "        For each proposal, provide:\n",
        "        - Your assessment (Approved/Rejected/Needs Modification)\n",
        "        - Justification for your decision\n",
        "        - Suggested refinements if \"Needs Modification\"\n",
        "\n",
        "        Be thorough in your analysis and maintain the highest standards of Islamic finance integrity.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, enhancement_result: Dict[str, Any], original_review: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate proposed enhancements based on Shariah compliance and practicality.\n",
        "\n",
        "        Args:\n",
        "            enhancement_result: The result from the EnhancementAgent.\n",
        "            original_review: The original review from the ReviewAgent.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing validation results.\n",
        "        \"\"\"\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {enhancement_result['standard_name']}\n",
        "\n",
        "            Original Review:\n",
        "            {original_review['review_result']}\n",
        "\n",
        "            Proposed Enhancements:\n",
        "            {enhancement_result['enhancement_proposals']}\n",
        "            \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        result = chain.run({})\n",
        "\n",
        "        return {\n",
        "            \"standard_name\": enhancement_result[\"standard_name\"],\n",
        "            \"validation_result\": result\n",
        "        }\n",
        "\n",
        "class FinalReportAgent(BaseAgent):\n",
        "    \"\"\"Agent for generating a comprehensive final report.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"FinalReportAgent\",\n",
        "            description=\"Compiles all findings and recommendations into a comprehensive report.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a professional report writer specializing in Islamic finance standards. Your task is to\n",
        "        compile all the findings, enhancements, and validations into a comprehensive, well-structured report.\n",
        "\n",
        "        Your report should include:\n",
        "\n",
        "        1. Executive Summary: Brief overview of the standard reviewed and key findings\n",
        "        2. Standard Overview: Summary of the original standard's purpose and core components\n",
        "        3. Key Findings from Review: Major elements and considerations identified\n",
        "        4. Proposed Enhancements: Clear presentation of all proposed modifications\n",
        "        5. Validation Results: Summary of the validation process and outcomes\n",
        "        6. Implementation Recommendations: Practical next steps for adopting approved changes\n",
        "        7. Conclusion: Final thoughts on the impact of the proposed enhancements\n",
        "\n",
        "        Write in a professional, clear style appropriate for AAOIFI stakeholders and Islamic finance professionals.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate a comprehensive final report.\n",
        "\n",
        "        Args:\n",
        "            all_results: Combined results from all previous agents.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the final report.\n",
        "        \"\"\"\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {all_results['standard_name']}\n",
        "\n",
        "            Review Results:\n",
        "            {all_results['review_result']}\n",
        "\n",
        "            Enhancement Proposals:\n",
        "            {all_results['enhancement_proposals']}\n",
        "\n",
        "            Validation Results:\n",
        "            {all_results['validation_result']}\n",
        "            \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        result = chain.run({})\n",
        "\n",
        "        return {\n",
        "            \"standard_name\": all_results[\"standard_name\"],\n",
        "            \"final_report\": result\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160d57fd",
      "metadata": {
        "id": "160d57fd"
      },
      "outputs": [],
      "source": [
        "class VectorDBManager:\n",
        "    \"\"\"Manager for interacting with the vector database.\"\"\"\n",
        "\n",
        "    def __init__(self, db_directory: str = config.DB_DIRECTORY, collection_name: str = config.COLLECTION_NAME):\n",
        "        self.db_directory = db_directory\n",
        "        self.collection_name = collection_name\n",
        "        self.embeddings = OpenAIEmbeddings()\n",
        "\n",
        "        # Initialize client\n",
        "        self.client = chromadb.PersistentClient(path=db_directory)\n",
        "\n",
        "        # Get collection\n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=collection_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Error accessing collection: {e}\")\n",
        "            print(\"Please ensure the vector database has been created first.\")\n",
        "            self.collection = None\n",
        "\n",
        "    def get_standard_content(self, standard_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Retrieve the content for a specific standard.\n",
        "\n",
        "        Args:\n",
        "            standard_name: The name of the standard to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            The combined content of the standard.\n",
        "        \"\"\"\n",
        "        if not self.collection:\n",
        "            return \"Error: Vector database not properly initialized.\"\n",
        "\n",
        "        # Query for all chunks belonging to this standard\n",
        "        results = self.collection.query(\n",
        "            query_texts=[\"\"],\n",
        "            where={\"source\": standard_name},\n",
        "            n_results=100  # Adjust based on expected number of chunks\n",
        "        )\n",
        "\n",
        "        # Combine chunks in order\n",
        "        if results and 'documents' in results and results['documents']:\n",
        "            return \"\\n\\n\".join([doc for doc in results['documents'][0] if doc])\n",
        "\n",
        "        return f\"No content found for standard: {standard_name}\"\n",
        "\n",
        "    def list_available_standards(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        List all available standards in the database.\n",
        "\n",
        "        Returns:\n",
        "            A list of standard names.\n",
        "        \"\"\"\n",
        "        if not self.collection:\n",
        "            return [\"Error: Vector database not properly initialized.\"]\n",
        "\n",
        "        # This is a simplified approach - in reality you'd need a more robust method\n",
        "        # to extract unique standard names from metadata\n",
        "        try:\n",
        "            results = self.collection.get()\n",
        "            if results and 'metadatas' in results and results['metadatas']:\n",
        "                standards = set()\n",
        "                for metadata in results['metadatas']:\n",
        "                    if 'source' in metadata:\n",
        "                        standards.add(metadata['source'])\n",
        "                return list(standards)\n",
        "        except Exception as e:\n",
        "            return [f\"Error listing standards: {e}\"]\n",
        "\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0eb79b",
      "metadata": {
        "id": "af0eb79b"
      },
      "outputs": [],
      "source": [
        "class AAOIFIStandardsEnhancementSystem:\n",
        "    \"\"\"Main system coordinating the multi-agent process.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize vector database manager\n",
        "        self.db_manager = VectorDBManager()\n",
        "\n",
        "        # Initialize agents\n",
        "        self.review_agent = ReviewAgent()\n",
        "        self.enhancement_agent = EnhancementAgent()\n",
        "        self.validation_agent = ValidationAgent()\n",
        "        self.report_agent = FinalReportAgent()\n",
        "\n",
        "        # Track processing results\n",
        "        self.results = {}\n",
        "\n",
        "    def list_available_standards(self) -> List[str]:\n",
        "        \"\"\"List all available standards in the system.\"\"\"\n",
        "        return self.db_manager.list_available_standards()\n",
        "\n",
        "    def process_standard(self, standard_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a single standard through the complete pipeline.\n",
        "\n",
        "        Args:\n",
        "            standard_name: The name of the standard to process.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the final results.\n",
        "        \"\"\"\n",
        "        print(f\"Processing standard: {standard_name}\")\n",
        "\n",
        "        # Step 1: Get standard content from the vector database\n",
        "        print(\"Retrieving standard content...\")\n",
        "        content = self.db_manager.get_standard_content(standard_name)\n",
        "        standard = StandardDocument(name=standard_name, content=content)\n",
        "\n",
        "        # Step 2: Review and extract key elements\n",
        "        print(\"Reviewing standard and extracting key elements...\")\n",
        "        review_result = self.review_agent.execute(standard)\n",
        "        self.results[\"review_result\"] = review_result\n",
        "\n",
        "        # Step 3: Propose enhancements\n",
        "        print(\"Proposing enhancements...\")\n",
        "        enhancement_result = self.enhancement_agent.execute(review_result)\n",
        "        self.results[\"enhancement_proposals\"] = enhancement_result[\"enhancement_proposals\"]\n",
        "\n",
        "        # Step 4: Validate proposed changes\n",
        "        print(\"Validating proposed changes...\")\n",
        "        validation_result = self.validation_agent.execute(enhancement_result, review_result)\n",
        "        self.results[\"validation_result\"] = validation_result[\"validation_result\"]\n",
        "\n",
        "        # Step 5: Generate final report\n",
        "        print(\"Generating final report...\")\n",
        "        all_results = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"review_result\": review_result[\"review_result\"],\n",
        "            \"enhancement_proposals\": enhancement_result[\"enhancement_proposals\"],\n",
        "            \"validation_result\": validation_result[\"validation_result\"]\n",
        "        }\n",
        "\n",
        "        final_report = self.report_agent.execute(all_results)\n",
        "        self.results[\"final_report\"] = final_report[\"final_report\"]\n",
        "        self.results[\"standard_name\"] = standard_name\n",
        "\n",
        "        print(f\"Processing completed for standard: {standard_name}\")\n",
        "        return self.results\n",
        "\n",
        "    def save_results(self, output_dir: str = config.OUTPUT_DIR):\n",
        "        \"\"\"Save all results to output files.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        if not self.results:\n",
        "            print(\"No results to save.\")\n",
        "            return\n",
        "\n",
        "        standard_name = self.results.get(\"standard_name\", \"unknown_standard\")\n",
        "        filename = os.path.join(output_dir, f\"{standard_name}_results.json\")\n",
        "\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(self.results, f, indent=2)\n",
        "\n",
        "        print(f\"Results saved to {filename}\")\n",
        "\n",
        "        # Also save the final report separately\n",
        "        if \"final_report\" in self.results:\n",
        "            report_filename = os.path.join(output_dir, f\"{standard_name}_final_report.md\")\n",
        "            with open(report_filename, 'w') as f:\n",
        "                f.write(self.results[\"final_report\"])\n",
        "\n",
        "            print(f\"Final report saved to {report_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b7ba9e",
      "metadata": {
        "id": "70b7ba9e",
        "outputId": "db12c00f-48dc-44b4-e4cc-7a437868886c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 49 PDF files in the pdf_eng directory:\n",
            "- AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9.pdf\n",
            "- AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR.pdf\n",
            "- AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean.pdf\n",
            "- AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR.pdf\n",
            "- AAOIFI-SB-Conf.-17-Final-Recommendations.pdf\n",
            "- AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2.pdf\n",
            "- Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions.pdf\n",
            "- FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions.pdf\n",
            "- FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022.pdf\n",
            "- FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1.pdf\n",
            "- FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1.pdf\n",
            "- FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1.pdf\n",
            "- FAS-32-Ijarah-Formatted-2021-clean-April-2023-1.pdf\n",
            "- FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023.pdf\n",
            "- FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023.pdf\n",
            "- FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022.pdf\n",
            "- FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean.pdf\n",
            "- FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean.pdf\n",
            "- FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean.pdf\n",
            "- FAS-39-Financial-reporting-for-Zakah-clean-june-2022.pdf\n",
            "- FAS-40-Financial-Reporting-for-Islamic-Windows.pdf\n",
            "- FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean.pdf\n",
            "- FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean.pdf\n",
            "- FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean.pdf\n",
            "- FAS-44_-Determining-Control-of-Assets-and-Business-Final.pdf\n",
            "- FAS-45_Quasi-equity-Including-Investment-Accounts-Final.pdf\n",
            "- FAS-46_Off-Balance-Sheet-Assets-Under-Management-Final.pdf\n",
            "- FAS-47_Transfer-of-Assets-between-Investment-Pools-Final.pdf\n",
            "- Fiancial-Accounting-Standard-No.-13-Disclosure-of-Bases-for-Determining-and-Allocating-Surplus-or-Defificit-in-Islamic-Insurance-Companies.pdf\n",
            "- Financial-Accounting-Standard-10-Istisnaa-and-Parallel-Istisnaa.pdf\n",
            "- Financial-Accounting-Standard-12-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Insurance-Companies.pdf\n",
            "- Financial-Accounting-Standard-14-Investment-Funds.pdf\n",
            "- Financial-Accounting-Standard-15-Provisions-and-Reserves-in-Islamic-Insurance-Companies.pdf\n",
            "- Financial-Accounting-Standard-16-Foreign-Currency-Transactions-and-Foreign-Operations.pdf\n",
            "- Financial-Accounting-Standard-18-Islamic-Financial-Services-Offffered-by-Conventional-Financial-Institutions.pdf\n",
            "- Financial-Accounting-Standard-19-Contributions-in-Islamic-Insurance-Companies.pdf\n",
            "- Financial-Accounting-Standard-21-Disclosure-on-Transfer-of-Assets.pdf\n",
            "- Financial-Accounting-Standard-22-Segment-Reporting.pdf\n",
            "- Financial-Accounting-Standard-23-Consolidation.pdf\n",
            "- Financial-Accounting-Standard-24-Investments-in-Associates.pdf\n",
            "- Financial-Accounting-Standard-25-Investment-in-Sukuk-Shares-and-Similar-Instruments.pdf\n",
            "- Financial-Accounting-Standard-26-Investment-in-Real-Estate.pdf\n",
            "- Financial-Accounting-Standard-27-Investment-accounts.pdf\n",
            "- Financial-Accounting-Standard-3-Mudaraba-Financing.pdf\n",
            "- Financial-Accounting-Standard-4-Musharaka-Financing.pdf\n",
            "- Financial-Accounting-Standard-7-Salam-and-Parallel-Salam.pdf\n",
            "- Financial-Accounting-Standard-9-Zakah.pdf\n",
            "- Research-papers-1.pdf\n",
            "- Research-papers-new.pdf\n"
          ]
        }
      ],
      "source": [
        "# First, check if we have PDF files in the pdf_eng directory\n",
        "pdf_files = [f for f in os.listdir(config.PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "print(f\"Found {len(pdf_files)} PDF files in the {config.PDF_FOLDER} directory:\")\n",
        "for pdf in pdf_files:\n",
        "    print(f\"- {pdf}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff58971",
      "metadata": {
        "id": "5ff58971"
      },
      "outputs": [],
      "source": [
        "# # Process the PDFs to create the vector database\n",
        "# # Only run this cell when you want to process PDFs\n",
        "# process_pdfs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d16824",
      "metadata": {
        "id": "96d16824",
        "outputId": "e8ad0769-a2dc-47dd-92cb-b27885401a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available standards:\n",
            "1. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "2. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
            "3. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
            "4. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
            "5. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
            "6. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
            "7. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
            "8. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
            "9. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
            "10. FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean\n",
            "11. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
            "12. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
            "13. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
            "14. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
            "15. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
            "16. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
            "17. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n",
            "18. FAS-44_-Determining-Control-of-Assets-and-Business-Final\n",
            "19. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
            "20. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
            "21. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
            "22. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
            "23. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
            "24. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
            "25. FAS-45_Quasi-equity-Including-Investment-Accounts-Final\n",
            "26. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n"
          ]
        }
      ],
      "source": [
        "# Initialize the system\n",
        "system = AAOIFIStandardsEnhancementSystem()\n",
        "\n",
        "# List available standards\n",
        "print(\"Available standards:\")\n",
        "standards = system.list_available_standards()\n",
        "for i, standard in enumerate(standards):\n",
        "    print(f\"{i+1}. {standard}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96aca65",
      "metadata": {
        "id": "a96aca65"
      },
      "outputs": [],
      "source": [
        "# Option 1: Fix the dimension mismatch by clearing the database and recreating it\n",
        "\n",
        "def recreate_vector_database(documents):\n",
        "    \"\"\"Create a new vector database from document chunks after removing any existing data.\"\"\"\n",
        "    import os\n",
        "    import shutil\n",
        "    from langchain.embeddings import OpenAIEmbeddings\n",
        "    import chromadb\n",
        "\n",
        "    # Initialize embeddings provider\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
        "\n",
        "    # First, check if the DB directory exists and delete it\n",
        "    if os.path.exists(config.DB_DIRECTORY):\n",
        "        print(f\"Removing existing database directory: {config.DB_DIRECTORY}\")\n",
        "        shutil.rmtree(config.DB_DIRECTORY)\n",
        "\n",
        "    # Create a new directory\n",
        "    os.makedirs(config.DB_DIRECTORY, exist_ok=True)\n",
        "\n",
        "    # Create Chroma client with new empty directory\n",
        "    client = chromadb.PersistentClient(path=config.DB_DIRECTORY)\n",
        "\n",
        "    # Create new collection\n",
        "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "    # Process documents in batches to avoid API limits\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i+batch_size]\n",
        "\n",
        "        # Extract content and metadata\n",
        "        texts = [doc[\"content\"] for doc in batch]\n",
        "        ids = [f\"doc_{i+j}\" for j in range(len(batch))]\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch]\n",
        "\n",
        "        # Generate embeddings\n",
        "        embeds = embeddings.embed_documents(texts)\n",
        "\n",
        "        # Add to collection\n",
        "        collection.add(\n",
        "            embeddings=embeds,\n",
        "            documents=texts,\n",
        "            ids=ids,\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "\n",
        "    print(f\"Created new vector database with consistent embedding dimensions in '{config.DB_DIRECTORY}'\")\n",
        "    return client\n",
        "\n",
        "# Option 2: Alternative approach to fix VectorDBManager\n",
        "class VectorDBManager:\n",
        "    \"\"\"Manager for interacting with the vector database.\"\"\"\n",
        "\n",
        "    def __init__(self, db_directory: str = config.DB_DIRECTORY, collection_name: str = config.COLLECTION_NAME):\n",
        "        self.db_directory = db_directory\n",
        "        self.collection_name = collection_name\n",
        "        self.embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)  # Use consistent model\n",
        "\n",
        "        # Initialize client\n",
        "        self.client = chromadb.PersistentClient(path=db_directory)\n",
        "\n",
        "        # Get collection or recreate it if there's an error\n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=collection_name)\n",
        "            print(f\"Successfully connected to existing collection: {collection_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error accessing collection: {e}\")\n",
        "            print(\"Creating a new collection...\")\n",
        "            self.collection = self.client.create_collection(name=collection_name)\n",
        "\n",
        "    def get_standard_content(self, standard_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Retrieve the content for a specific standard.\n",
        "\n",
        "        Args:\n",
        "            standard_name: The name of the standard to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            The combined content of the standard.\n",
        "        \"\"\"\n",
        "        if not self.collection:\n",
        "            return \"Error: Vector database not properly initialized.\"\n",
        "\n",
        "        # Query for all chunks belonging to this standard\n",
        "        # For a new or recreated collection, we may need to handle empty results\n",
        "        try:\n",
        "            results = self.collection.query(\n",
        "                query_texts=[\"\"],  # Empty query to get all documents\n",
        "                where={\"source\": standard_name},\n",
        "                n_results=100  # Adjust based on expected number of chunks\n",
        "            )\n",
        "\n",
        "            # Combine chunks in order\n",
        "            if results and 'documents' in results and results['documents'] and results['documents'][0]:\n",
        "                return \"\\n\\n\".join([doc for doc in results['documents'][0] if doc])\n",
        "            else:\n",
        "                return f\"No content found for standard: {standard_name}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error querying for standard {standard_name}: {str(e)}\"\n",
        "\n",
        "    def list_available_standards(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        List all available standards in the database.\n",
        "\n",
        "        Returns:\n",
        "            A list of standard names.\n",
        "        \"\"\"\n",
        "        if not self.collection:\n",
        "            return [\"Error: Vector database not properly initialized.\"]\n",
        "\n",
        "        # Handle potentially empty collection\n",
        "        try:\n",
        "            results = self.collection.get()\n",
        "            if results and 'metadatas' in results and results['metadatas']:\n",
        "                standards = set()\n",
        "                for metadata in results['metadatas']:\n",
        "                    if 'source' in metadata:\n",
        "                        standards.add(metadata['source'])\n",
        "                return list(standards)\n",
        "            else:\n",
        "                return [\"No standards found in the database. Please process PDFs first.\"]\n",
        "        except Exception as e:\n",
        "            return [f\"Error listing standards: {str(e)}\"]\n",
        "\n",
        "# Modified process_pdfs function to use the recreate_vector_database function\n",
        "def process_pdfs():\n",
        "    \"\"\"Main function to process PDFs and create vector database.\"\"\"\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "\n",
        "    if not os.path.exists(PDF_FOLDER):\n",
        "        print(f\"Error: The folder '{PDF_FOLDER}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0]\n",
        "\n",
        "        print(f\"Processing {pdf_file}...\")\n",
        "\n",
        "        # Extract text from PDF\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
        "        all_documents.extend(chunks)\n",
        "\n",
        "        print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    print(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "\n",
        "    # Create vector database, recreating it from scratch to ensure consistent dimensions\n",
        "    print(\"Creating vector database...\")\n",
        "    client = recreate_vector_database(all_documents)\n",
        "\n",
        "    print(f\"Vector database created successfully in '{config.DB_DIRECTORY}'\")\n",
        "    return client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "044a2394",
      "metadata": {
        "id": "044a2394"
      },
      "outputs": [],
      "source": [
        "# # Process PDFs to create a fresh vector database\n",
        "# process_pdfs()\n",
        "\n",
        "# # Initialize the system with the updated VectorDBManager\n",
        "# system = AAOIFIStandardsEnhancementSystem()\n",
        "\n",
        "# # Verify the available standards\n",
        "# standards = system.list_available_standards()\n",
        "# print(\"Available standards:\")\n",
        "# for i, standard in enumerate(standards):\n",
        "#     print(f\"{i+1}. {standard}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0782ad45",
      "metadata": {
        "id": "0782ad45"
      },
      "outputs": [],
      "source": [
        "# Modified approach to handle locked files\n",
        "\n",
        "def safely_recreate_vector_database(documents):\n",
        "    \"\"\"Create a new vector database from document chunks with proper handling of locked files.\"\"\"\n",
        "    import os\n",
        "    import time\n",
        "    import random\n",
        "    import string\n",
        "    from langchain.embeddings import OpenAIEmbeddings\n",
        "    import chromadb\n",
        "\n",
        "    # Initialize embeddings provider\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
        "\n",
        "    # Instead of deleting the directory, create a new directory with a unique name\n",
        "    # This avoids file permission issues with locked files\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
        "    new_db_directory = f\"{config.DB_DIRECTORY}_{timestamp}_{random_str}\"\n",
        "\n",
        "    print(f\"Creating new database directory: {new_db_directory}\")\n",
        "    os.makedirs(new_db_directory, exist_ok=True)\n",
        "\n",
        "    # Create Chroma client with the new directory\n",
        "    client = chromadb.PersistentClient(path=new_db_directory)\n",
        "\n",
        "    # Create new collection\n",
        "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "    # Process documents in batches to avoid API limits\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch = documents[i:i+batch_size]\n",
        "\n",
        "        # Extract content and metadata\n",
        "        texts = [doc[\"content\"] for doc in batch]\n",
        "        ids = [f\"doc_{i+j}\" for j in range(len(batch))]\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch]\n",
        "\n",
        "        try:\n",
        "            # Generate embeddings\n",
        "            embeds = embeddings.embed_documents(texts)\n",
        "\n",
        "            # Add to collection\n",
        "            collection.add(\n",
        "                embeddings=embeds,\n",
        "                documents=texts,\n",
        "                ids=ids,\n",
        "                metadatas=metadatas\n",
        "            )\n",
        "            print(f\"Processed batch {i//batch_size + 1} ({len(batch)} documents)\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch starting at index {i}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Created new vector database with consistent embedding dimensions in '{new_db_directory}'\")\n",
        "\n",
        "    # Update the config to point to the new directory\n",
        "    config.DB_DIRECTORY = new_db_directory\n",
        "    print(f\"Updated configuration to use new database directory: {config.DB_DIRECTORY}\")\n",
        "\n",
        "    return client\n",
        "\n",
        "# Modified process_pdfs function to use the safely_recreate_vector_database function\n",
        "def process_pdfs_safe():\n",
        "    \"\"\"Main function to process PDFs and create vector database safely.\"\"\"\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "\n",
        "    if not os.path.exists(PDF_FOLDER):\n",
        "        print(f\"Error: The folder '{PDF_FOLDER}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0]\n",
        "\n",
        "        print(f\"Processing {pdf_file}...\")\n",
        "\n",
        "        # Extract text from PDF\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "\n",
        "        # Split into chunks\n",
        "        chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
        "        all_documents.extend(chunks)\n",
        "\n",
        "        print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    print(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "\n",
        "    # Create vector database using the safe approach\n",
        "    print(\"Creating vector database safely...\")\n",
        "    client = safely_recreate_vector_database(all_documents)\n",
        "\n",
        "    print(f\"Vector database created successfully in '{config.DB_DIRECTORY}'\")\n",
        "    return client\n",
        "\n",
        "# Modified VectorDBManager class to work with the new directory and handle potential issues\n",
        "class VectorDBManager:\n",
        "    \"\"\"Manager for interacting with the vector database.\"\"\"\n",
        "\n",
        "    def __init__(self, db_directory: str = None, collection_name: str = config.COLLECTION_NAME):\n",
        "        # Use the directory specified in config (which may have been updated)\n",
        "        self.db_directory = db_directory if db_directory else config.DB_DIRECTORY\n",
        "        self.collection_name = collection_name\n",
        "        self.embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)  # Use consistent model\n",
        "\n",
        "        print(f\"Initializing VectorDBManager with database directory: {self.db_directory}\")\n",
        "\n",
        "        # Try to initialize client with better error handling\n",
        "        try:\n",
        "            self.client = chromadb.PersistentClient(path=self.db_directory)\n",
        "            print(f\"Successfully connected to database at: {self.db_directory}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error connecting to database: {str(e)}\")\n",
        "            self.client = None\n",
        "            self.collection = None\n",
        "            return\n",
        "\n",
        "        # Try to get the collection with better error handling\n",
        "        try:\n",
        "            self.collection = self.client.get_collection(name=collection_name)\n",
        "            print(f\"Successfully connected to collection: {collection_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error accessing collection: {str(e)}\")\n",
        "            try:\n",
        "                print(f\"Attempting to create a new collection: {collection_name}\")\n",
        "                self.collection = self.client.create_collection(name=collection_name)\n",
        "                print(f\"New collection created successfully\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Error creating collection: {str(e2)}\")\n",
        "                self.collection = None\n",
        "\n",
        "    def get_standard_content(self, standard_name: str) -> str:\n",
        "        \"\"\"Retrieve the content for a specific standard.\"\"\"\n",
        "        if not self.collection:\n",
        "            return \"Error: Vector database not properly initialized.\"\n",
        "\n",
        "        try:\n",
        "            results = self.collection.query(\n",
        "                query_texts=[\"\"],  # Empty query to get all documents\n",
        "                where={\"source\": standard_name},\n",
        "                n_results=100  # Adjust based on expected number of chunks\n",
        "            )\n",
        "\n",
        "            if results and 'documents' in results and results['documents'] and results['documents'][0]:\n",
        "                return \"\\n\\n\".join([doc for doc in results['documents'][0] if doc])\n",
        "            else:\n",
        "                return f\"No content found for standard: {standard_name}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error querying for standard {standard_name}: {str(e)}\"\n",
        "\n",
        "    def list_available_standards(self) -> List[str]:\n",
        "        \"\"\"List all available standards in the database.\"\"\"\n",
        "        if not self.collection:\n",
        "            return [\"Error: Vector database not properly initialized.\"]\n",
        "\n",
        "        try:\n",
        "            results = self.collection.get()\n",
        "            if results and 'metadatas' in results and results['metadatas']:\n",
        "                standards = set()\n",
        "                for metadata in results['metadatas']:\n",
        "                    if 'source' in metadata:\n",
        "                        standards.add(metadata['source'])\n",
        "                return list(standards)\n",
        "            else:\n",
        "                return [\"No standards found in the database. Please process PDFs first.\"]\n",
        "        except Exception as e:\n",
        "            return [f\"Error listing standards: {str(e)}\"]\n",
        "\n",
        "# Implementation to test the new approach\n",
        "def test_safe_approach():\n",
        "    \"\"\"Test the safe approach to creating and using the vector database.\"\"\"\n",
        "\n",
        "    # Process PDFs using the safe approach\n",
        "    print(\"Processing PDFs with safe approach...\")\n",
        "    client = process_pdfs_safe()\n",
        "\n",
        "    # Initialize system with the new database\n",
        "    print(\"\\nInitializing AAOIFIStandardsEnhancementSystem...\")\n",
        "    system = AAOIFIStandardsEnhancementSystem()\n",
        "\n",
        "    # Check available standards\n",
        "    print(\"\\nListing available standards...\")\n",
        "    standards = system.list_available_standards()\n",
        "    for i, standard in enumerate(standards):\n",
        "        print(f\"{i+1}. {standard}\")\n",
        "\n",
        "    # If any standards are available, process the first one\n",
        "    if standards and standards[0] != \"No standards found in the database. Please process PDFs first.\":\n",
        "        standard_name = standards[0]\n",
        "        print(f\"\\nProcessing standard: {standard_name}\")\n",
        "        results = system.process_standard(standard_name)\n",
        "        system.save_results()\n",
        "\n",
        "        # Display the final report\n",
        "        if \"final_report\" in results:\n",
        "            print(\"\\n===== FINAL REPORT =====\\n\")\n",
        "            print(results[\"final_report\"])\n",
        "    else:\n",
        "        print(\"\\nNo standards available to process.\")\n",
        "\n",
        "    return \"Test completed.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e960a9e",
      "metadata": {
        "id": "0e960a9e",
        "outputId": "4fad196f-3b9e-4875-c661-c36d603e206b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing PDFs with safe approach...\n",
            "Found 49 PDF files.\n",
            "Processing AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9.pdf...\n",
            "Extracted 3 chunks from AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9.pdf\n",
            "Processing AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR.pdf...\n",
            "Extracted 3 chunks from AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR.pdf\n",
            "Processing AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean.pdf...\n",
            "Extracted 178 chunks from AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean.pdf\n",
            "Processing AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR.pdf...\n",
            "Extracted 9 chunks from AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR.pdf\n",
            "Processing AAOIFI-SB-Conf.-17-Final-Recommendations.pdf...\n",
            "Extracted 1 chunks from AAOIFI-SB-Conf.-17-Final-Recommendations.pdf\n",
            "Processing AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2.pdf...\n",
            "Extracted 10 chunks from AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2.pdf\n",
            "Processing Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions.pdf...\n",
            "Extracted 240 chunks from Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions.pdf\n",
            "Processing FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions.pdf...\n",
            "Extracted 305 chunks from FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions.pdf\n",
            "Processing FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022.pdf...\n",
            "Extracted 166 chunks from FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022.pdf\n",
            "Processing FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1.pdf...\n",
            "Extracted 58 chunks from FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1.pdf\n",
            "Processing FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1.pdf...\n",
            "Extracted 96 chunks from FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1.pdf\n",
            "Processing FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1.pdf...\n",
            "Extracted 79 chunks from FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1.pdf\n",
            "Processing FAS-32-Ijarah-Formatted-2021-clean-April-2023-1.pdf...\n",
            "Extracted 130 chunks from FAS-32-Ijarah-Formatted-2021-clean-April-2023-1.pdf\n",
            "Processing FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023.pdf...\n",
            "Extracted 68 chunks from FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023.pdf\n",
            "Processing FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023.pdf...\n",
            "Extracted 41 chunks from FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023.pdf\n",
            "Processing FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022.pdf...\n",
            "Extracted 55 chunks from FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022.pdf\n",
            "Processing FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean.pdf...\n",
            "Extracted 45 chunks from FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean.pdf\n",
            "Processing FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean.pdf...\n",
            "Extracted 98 chunks from FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean.pdf\n",
            "Processing FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean.pdf...\n",
            "Extracted 51 chunks from FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean.pdf\n",
            "Processing FAS-39-Financial-reporting-for-Zakah-clean-june-2022.pdf...\n",
            "Extracted 62 chunks from FAS-39-Financial-reporting-for-Zakah-clean-june-2022.pdf\n",
            "Processing FAS-40-Financial-Reporting-for-Islamic-Windows.pdf...\n",
            "Extracted 41 chunks from FAS-40-Financial-Reporting-for-Islamic-Windows.pdf\n",
            "Processing FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean.pdf...\n",
            "Extracted 33 chunks from FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean.pdf\n",
            "Processing FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean.pdf...\n",
            "Extracted 87 chunks from FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean.pdf\n",
            "Processing FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean.pdf...\n",
            "Extracted 154 chunks from FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean.pdf\n",
            "Processing FAS-44_-Determining-Control-of-Assets-and-Business-Final.pdf...\n",
            "Extracted 44 chunks from FAS-44_-Determining-Control-of-Assets-and-Business-Final.pdf\n",
            "Processing FAS-45_Quasi-equity-Including-Investment-Accounts-Final.pdf...\n",
            "Extracted 66 chunks from FAS-45_Quasi-equity-Including-Investment-Accounts-Final.pdf\n",
            "Processing FAS-46_Off-Balance-Sheet-Assets-Under-Management-Final.pdf...\n",
            "Extracted 50 chunks from FAS-46_Off-Balance-Sheet-Assets-Under-Management-Final.pdf\n",
            "Processing FAS-47_Transfer-of-Assets-between-Investment-Pools-Final.pdf...\n",
            "Extracted 29 chunks from FAS-47_Transfer-of-Assets-between-Investment-Pools-Final.pdf\n",
            "Processing Fiancial-Accounting-Standard-No.-13-Disclosure-of-Bases-for-Determining-and-Allocating-Surplus-or-Defificit-in-Islamic-Insurance-Companies.pdf...\n",
            "Extracted 73 chunks from Fiancial-Accounting-Standard-No.-13-Disclosure-of-Bases-for-Determining-and-Allocating-Surplus-or-Defificit-in-Islamic-Insurance-Companies.pdf\n",
            "Processing Financial-Accounting-Standard-10-Istisnaa-and-Parallel-Istisnaa.pdf...\n",
            "Extracted 149 chunks from Financial-Accounting-Standard-10-Istisnaa-and-Parallel-Istisnaa.pdf\n",
            "Processing Financial-Accounting-Standard-12-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Insurance-Companies.pdf...\n",
            "Extracted 228 chunks from Financial-Accounting-Standard-12-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Insurance-Companies.pdf\n",
            "Processing Financial-Accounting-Standard-14-Investment-Funds.pdf...\n",
            "Extracted 126 chunks from Financial-Accounting-Standard-14-Investment-Funds.pdf\n",
            "Processing Financial-Accounting-Standard-15-Provisions-and-Reserves-in-Islamic-Insurance-Companies.pdf...\n",
            "Extracted 77 chunks from Financial-Accounting-Standard-15-Provisions-and-Reserves-in-Islamic-Insurance-Companies.pdf\n",
            "Processing Financial-Accounting-Standard-16-Foreign-Currency-Transactions-and-Foreign-Operations.pdf...\n",
            "Extracted 104 chunks from Financial-Accounting-Standard-16-Foreign-Currency-Transactions-and-Foreign-Operations.pdf\n",
            "Processing Financial-Accounting-Standard-18-Islamic-Financial-Services-Offffered-by-Conventional-Financial-Institutions.pdf...\n",
            "Extracted 51 chunks from Financial-Accounting-Standard-18-Islamic-Financial-Services-Offffered-by-Conventional-Financial-Institutions.pdf\n",
            "Processing Financial-Accounting-Standard-19-Contributions-in-Islamic-Insurance-Companies.pdf...\n",
            "Extracted 56 chunks from Financial-Accounting-Standard-19-Contributions-in-Islamic-Insurance-Companies.pdf\n",
            "Processing Financial-Accounting-Standard-21-Disclosure-on-Transfer-of-Assets.pdf...\n",
            "Extracted 46 chunks from Financial-Accounting-Standard-21-Disclosure-on-Transfer-of-Assets.pdf\n",
            "Processing Financial-Accounting-Standard-22-Segment-Reporting.pdf...\n",
            "Extracted 64 chunks from Financial-Accounting-Standard-22-Segment-Reporting.pdf\n",
            "Processing Financial-Accounting-Standard-23-Consolidation.pdf...\n",
            "Extracted 56 chunks from Financial-Accounting-Standard-23-Consolidation.pdf\n",
            "Processing Financial-Accounting-Standard-24-Investments-in-Associates.pdf...\n",
            "Extracted 50 chunks from Financial-Accounting-Standard-24-Investments-in-Associates.pdf\n",
            "Processing Financial-Accounting-Standard-25-Investment-in-Sukuk-Shares-and-Similar-Instruments.pdf...\n",
            "Extracted 104 chunks from Financial-Accounting-Standard-25-Investment-in-Sukuk-Shares-and-Similar-Instruments.pdf\n",
            "Processing Financial-Accounting-Standard-26-Investment-in-Real-Estate.pdf...\n",
            "Extracted 147 chunks from Financial-Accounting-Standard-26-Investment-in-Real-Estate.pdf\n",
            "Processing Financial-Accounting-Standard-27-Investment-accounts.pdf...\n",
            "Extracted 71 chunks from Financial-Accounting-Standard-27-Investment-accounts.pdf\n",
            "Processing Financial-Accounting-Standard-3-Mudaraba-Financing.pdf...\n",
            "Extracted 115 chunks from Financial-Accounting-Standard-3-Mudaraba-Financing.pdf\n",
            "Processing Financial-Accounting-Standard-4-Musharaka-Financing.pdf...\n",
            "Extracted 116 chunks from Financial-Accounting-Standard-4-Musharaka-Financing.pdf\n",
            "Processing Financial-Accounting-Standard-7-Salam-and-Parallel-Salam.pdf...\n",
            "Extracted 93 chunks from Financial-Accounting-Standard-7-Salam-and-Parallel-Salam.pdf\n",
            "Processing Financial-Accounting-Standard-9-Zakah.pdf...\n",
            "Extracted 121 chunks from Financial-Accounting-Standard-9-Zakah.pdf\n",
            "Processing Research-papers-1.pdf...\n",
            "Extracted 58 chunks from Research-papers-1.pdf\n",
            "Processing Research-papers-new.pdf...\n",
            "Extracted 160 chunks from Research-papers-new.pdf\n",
            "Total chunks extracted: 4267\n",
            "Creating vector database safely...\n",
            "Creating new database directory: aaoifi_vector_db_20250509_172024_h8usxg\n",
            "Processed batch 1 (100 documents)\n",
            "Processed batch 2 (100 documents)\n",
            "Processed batch 3 (100 documents)\n",
            "Processed batch 4 (100 documents)\n",
            "Processed batch 5 (100 documents)\n",
            "Processed batch 6 (100 documents)\n",
            "Processed batch 7 (100 documents)\n",
            "Processed batch 8 (100 documents)\n",
            "Processed batch 9 (100 documents)\n",
            "Processed batch 10 (100 documents)\n",
            "Processed batch 11 (100 documents)\n",
            "Processed batch 12 (100 documents)\n",
            "Processed batch 13 (100 documents)\n",
            "Processed batch 14 (100 documents)\n",
            "Processed batch 15 (100 documents)\n",
            "Processed batch 16 (100 documents)\n",
            "Processed batch 17 (100 documents)\n",
            "Processed batch 18 (100 documents)\n",
            "Processed batch 19 (100 documents)\n",
            "Processed batch 20 (100 documents)\n",
            "Processed batch 21 (100 documents)\n",
            "Processed batch 22 (100 documents)\n",
            "Processed batch 23 (100 documents)\n",
            "Processed batch 24 (100 documents)\n",
            "Processed batch 25 (100 documents)\n",
            "Processed batch 26 (100 documents)\n",
            "Processed batch 27 (100 documents)\n",
            "Processed batch 28 (100 documents)\n",
            "Processed batch 29 (100 documents)\n",
            "Processed batch 30 (100 documents)\n",
            "Processed batch 31 (100 documents)\n",
            "Processed batch 32 (100 documents)\n",
            "Processed batch 33 (100 documents)\n",
            "Processed batch 34 (100 documents)\n",
            "Processed batch 35 (100 documents)\n",
            "Processed batch 36 (100 documents)\n",
            "Processed batch 37 (100 documents)\n",
            "Processed batch 38 (100 documents)\n",
            "Processed batch 39 (100 documents)\n",
            "Processed batch 40 (100 documents)\n",
            "Processed batch 41 (100 documents)\n",
            "Processed batch 42 (100 documents)\n",
            "Processed batch 43 (67 documents)\n",
            "Created new vector database with consistent embedding dimensions in 'aaoifi_vector_db_20250509_172024_h8usxg'\n",
            "Updated configuration to use new database directory: aaoifi_vector_db_20250509_172024_h8usxg\n",
            "Vector database created successfully in 'aaoifi_vector_db_20250509_172024_h8usxg'\n",
            "\n",
            "Initializing AAOIFIStandardsEnhancementSystem...\n",
            "Initializing VectorDBManager with database directory: aaoifi_vector_db_20250509_172024_h8usxg\n",
            "Successfully connected to database at: aaoifi_vector_db_20250509_172024_h8usxg\n",
            "Successfully connected to collection: aaoifi_standards\n",
            "\n",
            "Listing available standards...\n",
            "1. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "2. Financial-Accounting-Standard-15-Provisions-and-Reserves-in-Islamic-Insurance-Companies\n",
            "3. Financial-Accounting-Standard-10-Istisnaa-and-Parallel-Istisnaa\n",
            "4. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
            "5. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
            "6. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
            "7. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
            "8. Financial-Accounting-Standard-3-Mudaraba-Financing\n",
            "9. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
            "10. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
            "11. FAS-46_Off-Balance-Sheet-Assets-Under-Management-Final\n",
            "12. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
            "13. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
            "14. FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean\n",
            "15. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
            "16. Financial-Accounting-Standard-23-Consolidation\n",
            "17. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
            "18. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
            "19. Financial-Accounting-Standard-14-Investment-Funds\n",
            "20. Financial-Accounting-Standard-24-Investments-in-Associates\n",
            "21. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
            "22. Research-papers-new\n",
            "23. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
            "24. Financial-Accounting-Standard-18-Islamic-Financial-Services-Offffered-by-Conventional-Financial-Institutions\n",
            "25. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
            "26. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n",
            "27. FAS-44_-Determining-Control-of-Assets-and-Business-Final\n",
            "28. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
            "29. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
            "30. FAS-47_Transfer-of-Assets-between-Investment-Pools-Final\n",
            "31. Financial-Accounting-Standard-27-Investment-accounts\n",
            "32. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
            "33. Financial-Accounting-Standard-26-Investment-in-Real-Estate\n",
            "34. Financial-Accounting-Standard-25-Investment-in-Sukuk-Shares-and-Similar-Instruments\n",
            "35. Financial-Accounting-Standard-4-Musharaka-Financing\n",
            "36. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
            "37. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
            "38. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
            "39. Financial-Accounting-Standard-19-Contributions-in-Islamic-Insurance-Companies\n",
            "40. Financial-Accounting-Standard-21-Disclosure-on-Transfer-of-Assets\n",
            "41. Financial-Accounting-Standard-16-Foreign-Currency-Transactions-and-Foreign-Operations\n",
            "42. Financial-Accounting-Standard-22-Segment-Reporting\n",
            "43. Financial-Accounting-Standard-7-Salam-and-Parallel-Salam\n",
            "44. Financial-Accounting-Standard-9-Zakah\n",
            "45. Research-papers-1\n",
            "46. Financial-Accounting-Standard-12-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Insurance-Companies\n",
            "47. Fiancial-Accounting-Standard-No.-13-Disclosure-of-Bases-for-Determining-and-Allocating-Surplus-or-Defificit-in-Islamic-Insurance-Companies\n",
            "48. FAS-45_Quasi-equity-Including-Investment-Accounts-Final\n",
            "49. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n",
            "\n",
            "Processing standard: FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "Processing standard: FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "Retrieving standard content...\n",
            "Reviewing standard and extracting key elements...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_18088\\3989169443.py:58: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=self.llm, prompt=prompt)\n",
            "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_18088\\3989169443.py:59: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = chain.run({})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Proposing enhancements...\n",
            "Validating proposed changes...\n",
            "Generating final report...\n",
            "Processing completed for standard: FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "Results saved to results\\FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean_results.json\n",
            "Final report saved to results\\FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean_final_report.md\n",
            "\n",
            "===== FINAL REPORT =====\n",
            "\n",
            "Executive Summary:\n",
            "The review of the AAOIFI FAS-37 standard, which pertains to financial reporting by Waqf institutions, has yielded key findings and proposed enhancements. The review process was comprehensive and aimed at improving the clarity, relevance, and practical applicability of the standard. The proposed enhancements were validated and refined to ensure Shariah compliance, technical accuracy, practical applicability, consistency, and value addition.\n",
            "\n",
            "Standard Overview:\n",
            "The FAS-37 standard provides guidance on financial reporting by Waqf institutions. It aims to ensure transparency, accountability, and compliance with Shariah principles in the financial operations of these institutions.\n",
            "\n",
            "Key Findings from Review:\n",
            "The review identified several areas for improvement, including the need for greater clarity in the definition of financial terms and Islamic finance concepts, the need to update the standard to reflect modern contexts such as the rise of digital assets, the potential for technological integration through fintech solutions, the need for better cross-referencing, and the need for practical examples of implementation.\n",
            "\n",
            "Proposed Enhancements:\n",
            "The proposed enhancements include defining all financial terms and Islamic finance concepts at the beginning of the document, updating the standard to include guidance on dealing with cryptocurrencies and other digital assets, including guidance on using fintech solutions for compliance, including a reference list at the end of the standard, and including case studies or examples of implementation.\n",
            "\n",
            "Validation Results:\n",
            "The proposed enhancements were validated and refined to ensure Shariah compliance, technical accuracy, practical applicability, consistency, and value addition. The validation process resulted in modifications to some of the proposed enhancements, such as the inclusion of a section discussing the current scholarly debate on the Shariah compliance of cryptocurrencies and the specification of which fintech solutions are being referred to.\n",
            "\n",
            "Implementation Recommendations:\n",
            "The approved enhancements should be incorporated into the standard in a systematic and phased manner. Training should be provided to all relevant personnel to ensure understanding and compliance with the enhanced standard. Regular reviews and audits should be conducted to ensure ongoing compliance.\n",
            "\n",
            "Conclusion:\n",
            "The proposed enhancements to the FAS-37 standard will significantly improve its clarity, relevance, and practical applicability. They will ensure that the standard remains up-to-date with modern contexts and technological advancements, and that it continues to provide clear and practical guidance for financial reporting by Waqf institutions. The enhancements will ultimately contribute to the transparency, accountability, and Shariah compliance of these institutions, thereby strengthening the overall Islamic finance industry.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Test completed.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Execute the safe approach test\n",
        "test_safe_approach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af413c57",
      "metadata": {
        "id": "af413c57",
        "outputId": "8fb3686c-4d2f-4cc0-b45b-7811963f34a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing VectorDBManager with database directory: aaoifi_vector_db_20250509_172024_h8usxg\n",
            "Successfully connected to database at: aaoifi_vector_db_20250509_172024_h8usxg\n",
            "Successfully connected to collection: aaoifi_standards\n",
            "Available standards:\n",
            "1. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "2. Financial-Accounting-Standard-15-Provisions-and-Reserves-in-Islamic-Insurance-Companies\n",
            "3. Financial-Accounting-Standard-10-Istisnaa-and-Parallel-Istisnaa\n",
            "4. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
            "5. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
            "6. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
            "7. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
            "8. Financial-Accounting-Standard-3-Mudaraba-Financing\n",
            "9. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
            "10. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
            "11. FAS-46_Off-Balance-Sheet-Assets-Under-Management-Final\n",
            "12. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
            "13. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
            "14. FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean\n",
            "15. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
            "16. Financial-Accounting-Standard-23-Consolidation\n",
            "17. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
            "18. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
            "19. Financial-Accounting-Standard-14-Investment-Funds\n",
            "20. Financial-Accounting-Standard-24-Investments-in-Associates\n",
            "21. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
            "22. Research-papers-new\n",
            "23. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
            "24. Financial-Accounting-Standard-18-Islamic-Financial-Services-Offffered-by-Conventional-Financial-Institutions\n",
            "25. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
            "26. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n",
            "27. FAS-44_-Determining-Control-of-Assets-and-Business-Final\n",
            "28. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
            "29. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
            "30. FAS-47_Transfer-of-Assets-between-Investment-Pools-Final\n",
            "31. Financial-Accounting-Standard-27-Investment-accounts\n",
            "32. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
            "33. Financial-Accounting-Standard-26-Investment-in-Real-Estate\n",
            "34. Financial-Accounting-Standard-25-Investment-in-Sukuk-Shares-and-Similar-Instruments\n",
            "35. Financial-Accounting-Standard-4-Musharaka-Financing\n",
            "36. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
            "37. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
            "38. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
            "39. Financial-Accounting-Standard-19-Contributions-in-Islamic-Insurance-Companies\n",
            "40. Financial-Accounting-Standard-21-Disclosure-on-Transfer-of-Assets\n",
            "41. Financial-Accounting-Standard-16-Foreign-Currency-Transactions-and-Foreign-Operations\n",
            "42. Financial-Accounting-Standard-22-Segment-Reporting\n",
            "43. Financial-Accounting-Standard-7-Salam-and-Parallel-Salam\n",
            "44. Financial-Accounting-Standard-9-Zakah\n",
            "45. Research-papers-1\n",
            "46. Financial-Accounting-Standard-12-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Insurance-Companies\n",
            "47. Fiancial-Accounting-Standard-No.-13-Disclosure-of-Bases-for-Determining-and-Allocating-Surplus-or-Defificit-in-Islamic-Insurance-Companies\n",
            "48. FAS-45_Quasi-equity-Including-Investment-Accounts-Final\n",
            "49. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n",
            "\n",
            "Processing standard: FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "Processing standard: FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "Retrieving standard content...\n",
            "Reviewing standard and extracting key elements...\n",
            "Proposing enhancements...\n",
            "Validating proposed changes...\n",
            "Generating final report...\n",
            "Processing completed for standard: FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "Results saved to results\\FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean_results.json\n",
            "Final report saved to results\\FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean_final_report.md\n",
            "\n",
            "===== FINAL REPORT =====\n",
            "\n",
            "Implementation Recommendations:\n",
            "\n",
            "1. Training: Conduct training sessions for Waqf institutions to familiarize them with the enhanced standard and its practical implications. This should include a detailed walkthrough of the changes and their rationale, as well as practical exercises to help practitioners apply the new guidance.\n",
            "\n",
            "2. Documentation: Update all relevant documentation, including manuals, guidelines, and templates, to reflect the enhanced standard. This will ensure that practitioners have access to up-to-date resources that align with the new guidance.\n",
            "\n",
            "3. Systems Update: Where necessary, update financial systems and software to align with the enhanced standard. This may involve adjusting data fields, reporting templates, and audit procedures to reflect the new requirements.\n",
            "\n",
            "4. Monitoring and Evaluation: Implement a monitoring and evaluation mechanism to assess the effectiveness of the enhanced standard. This should involve regular audits, feedback sessions, and performance metrics to track compliance and identify areas for further improvement.\n",
            "\n",
            "5. Stakeholder Engagement: Engage with key stakeholders, including practitioners, auditors, and beneficiaries, to gather feedback on the enhanced standard. This will help to ensure that the standard is meeting its intended objectives and is well-received by those it affects.\n",
            "\n",
            "Conclusion:\n",
            "\n",
            "The proposed enhancements to the FAS-37 standard represent a significant step forward in improving financial reporting by Waqf institutions. By providing clearer, more specific guidance, the enhanced standard will help to ensure that these institutions maintain high standards of transparency, accuracy, and Shariah compliance. Furthermore, by incorporating modern financial practices and digital technologies, the enhanced standard is well-positioned to meet the needs of the Islamic finance industry in the 21st century. With careful implementation and ongoing monitoring, these enhancements have the potential to significantly improve the effectiveness and credibility of financial reporting by Waqf institutions.\n",
            "Standard 'Your_Standard_Name_Here' not found in available standards.\n"
          ]
        }
      ],
      "source": [
        "# Complete workflow for processing standards\n",
        "\n",
        "# Step 1: Initialize your system\n",
        "system = AAOIFIStandardsEnhancementSystem()\n",
        "\n",
        "# Step 2: List all available standards to know what you can process\n",
        "print(\"Available standards:\")\n",
        "standards = system.list_available_standards()\n",
        "for i, standard in enumerate(standards):\n",
        "    print(f\"{i+1}. {standard}\")\n",
        "\n",
        "# Step 3: Process a specific standard by index or name\n",
        "# Option A: Process by index (e.g., process the first standard)\n",
        "if standards and standards[0] != \"No standards found in the database. Please process PDFs first.\":\n",
        "    # Process the first standard in the list\n",
        "    standard_name = standards[0]\n",
        "    print(f\"\\nProcessing standard: {standard_name}\")\n",
        "    results = system.process_standard(standard_name)\n",
        "    system.save_results()\n",
        "\n",
        "    # Display the final report\n",
        "    if \"final_report\" in results:\n",
        "        print(\"\\n===== FINAL REPORT =====\\n\")\n",
        "        print(results[\"final_report\"])\n",
        "\n",
        "# Option B: Process by specific name (replace with actual standard name)\n",
        "specific_standard = \"Your_Standard_Name_Here\"  # Replace with a name from your standards list\n",
        "if specific_standard in standards:\n",
        "    print(f\"\\nProcessing specific standard: {specific_standard}\")\n",
        "    results = system.process_standard(specific_standard)\n",
        "    system.save_results()\n",
        "\n",
        "    # Display the final report\n",
        "    if \"final_report\" in results:\n",
        "        print(\"\\n===== FINAL REPORT =====\\n\")\n",
        "        print(results[\"final_report\"])\n",
        "else:\n",
        "    print(f\"Standard '{specific_standard}' not found in available standards.\")\n",
        "\n",
        "# Step 4: Process all standards in batch (optional)\n",
        "def process_all_standards():\n",
        "    \"\"\"Process all available standards one by one.\"\"\"\n",
        "    print(\"\\nProcessing all standards in batch mode...\")\n",
        "\n",
        "    all_results = {}\n",
        "    for standard in standards:\n",
        "        if standard != \"No standards found in the database. Please process PDFs first.\":\n",
        "            print(f\"\\nProcessing standard: {standard}\")\n",
        "            try:\n",
        "                results = system.process_standard(standard)\n",
        "                system.save_results()\n",
        "                all_results[standard] = results\n",
        "                print(f\" Successfully processed: {standard}\")\n",
        "            except Exception as e:\n",
        "                print(f\" Error processing standard {standard}: {str(e)}\")\n",
        "\n",
        "    print(\"\\nBatch processing completed!\")\n",
        "    print(f\"Successfully processed {len(all_results)} standards.\")\n",
        "    return all_results\n",
        "\n",
        "# Uncomment the following line to process all standards\n",
        "# all_results = process_all_standards()\n",
        "\n",
        "# Step 5: View results for all processed standards\n",
        "def list_processed_results():\n",
        "    \"\"\"List all processed results in the results directory.\"\"\"\n",
        "    import os\n",
        "\n",
        "    results_dir = config.OUTPUT_DIR\n",
        "    if not os.path.exists(results_dir):\n",
        "        print(f\"Results directory '{results_dir}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    result_files = [f for f in os.listdir(results_dir) if f.endswith(\"_results.json\") or f.endswith(\"_final_report.md\")]\n",
        "\n",
        "    if not result_files:\n",
        "        print(f\"No result files found in '{results_dir}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFound {len(result_files)} result files:\")\n",
        "    for i, result_file in enumerate(result_files):\n",
        "        print(f\"{i+1}. {result_file}\")\n",
        "\n",
        "    return result_files\n",
        "\n",
        "# Uncomment to list all processed results\n",
        "# result_files = list_processed_results()\n",
        "\n",
        "# Step 6: Read a specific result file\n",
        "def read_report(report_name):\n",
        "    \"\"\"Read and display a specific report.\"\"\"\n",
        "    import os\n",
        "    import json\n",
        "\n",
        "    results_dir = config.OUTPUT_DIR\n",
        "    report_path = os.path.join(results_dir, report_name)\n",
        "\n",
        "    if not os.path.exists(report_path):\n",
        "        print(f\"Report file '{report_path}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Reading report: {report_name}\")\n",
        "\n",
        "    if report_name.endswith(\"_final_report.md\"):\n",
        "        # Read markdown report\n",
        "        with open(report_path, 'r') as f:\n",
        "            report_content = f.read()\n",
        "        print(\"\\n===== REPORT CONTENT =====\\n\")\n",
        "        print(report_content)\n",
        "        return report_content\n",
        "    elif report_name.endswith(\"_results.json\"):\n",
        "        # Read JSON results\n",
        "        with open(report_path, 'r') as f:\n",
        "            results = json.load(f)\n",
        "        print(\"\\n===== RESULTS SUMMARY =====\\n\")\n",
        "        print(f\"Standard Name: {results.get('standard_name', 'Unknown')}\")\n",
        "        print(f\"Contains Final Report: {'Yes' if 'final_report' in results else 'No'}\")\n",
        "        print(f\"Contains Review Result: {'Yes' if 'review_result' in results else 'No'}\")\n",
        "        print(f\"Contains Enhancement Proposals: {'Yes' if 'enhancement_proposals' in results else 'No'}\")\n",
        "        print(f\"Contains Validation Result: {'Yes' if 'validation_result' in results else 'No'}\")\n",
        "        return results\n",
        "    else:\n",
        "        print(f\"Unsupported file format: {report_name}\")\n",
        "        return None\n",
        "\n",
        "# Example: Read a specific report (uncomment and replace with actual report name)\n",
        "# read_report(\"standard_name_final_report.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f21efd5",
      "metadata": {
        "id": "4f21efd5",
        "outputId": "4d471492-bc12-48c7-c736-0ebcd4c249b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available standards:\n",
            "1. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "2. Financial-Accounting-Standard-15-Provisions-and-Reserves-in-Islamic-Insurance-Companies\n",
            "3. Financial-Accounting-Standard-10-Istisnaa-and-Parallel-Istisnaa\n",
            "4. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
            "5. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
            "6. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
            "7. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
            "8. Financial-Accounting-Standard-3-Mudaraba-Financing\n",
            "9. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
            "10. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
            "11. FAS-46_Off-Balance-Sheet-Assets-Under-Management-Final\n",
            "12. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
            "13. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
            "14. FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean\n",
            "15. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
            "16. Financial-Accounting-Standard-23-Consolidation\n",
            "17. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
            "18. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
            "19. Financial-Accounting-Standard-14-Investment-Funds\n",
            "20. Financial-Accounting-Standard-24-Investments-in-Associates\n",
            "21. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
            "22. Research-papers-new\n",
            "23. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
            "24. Financial-Accounting-Standard-18-Islamic-Financial-Services-Offffered-by-Conventional-Financial-Institutions\n",
            "25. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
            "26. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n",
            "27. FAS-44_-Determining-Control-of-Assets-and-Business-Final\n",
            "28. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
            "29. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
            "30. FAS-47_Transfer-of-Assets-between-Investment-Pools-Final\n",
            "31. Financial-Accounting-Standard-27-Investment-accounts\n",
            "32. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
            "33. Financial-Accounting-Standard-26-Investment-in-Real-Estate\n",
            "34. Financial-Accounting-Standard-25-Investment-in-Sukuk-Shares-and-Similar-Instruments\n",
            "35. Financial-Accounting-Standard-4-Musharaka-Financing\n",
            "36. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
            "37. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
            "38. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
            "39. Financial-Accounting-Standard-19-Contributions-in-Islamic-Insurance-Companies\n",
            "40. Financial-Accounting-Standard-21-Disclosure-on-Transfer-of-Assets\n",
            "41. Financial-Accounting-Standard-16-Foreign-Currency-Transactions-and-Foreign-Operations\n",
            "42. Financial-Accounting-Standard-22-Segment-Reporting\n",
            "43. Financial-Accounting-Standard-7-Salam-and-Parallel-Salam\n",
            "44. Financial-Accounting-Standard-9-Zakah\n",
            "45. Research-papers-1\n",
            "46. Financial-Accounting-Standard-12-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Insurance-Companies\n",
            "47. Fiancial-Accounting-Standard-No.-13-Disclosure-of-Bases-for-Determining-and-Allocating-Surplus-or-Defificit-in-Islamic-Insurance-Companies\n",
            "48. FAS-45_Quasi-equity-Including-Investment-Accounts-Final\n",
            "49. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n"
          ]
        }
      ],
      "source": [
        "print(\"Available standards:\")\n",
        "standards = system.list_available_standards()\n",
        "for i, standard in enumerate(standards):\n",
        "    print(f\"{i+1}. {standard}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76c01183",
      "metadata": {
        "id": "76c01183"
      },
      "outputs": [],
      "source": [
        "# class StandardDocument:\n",
        "#     \"\"\"Class to represent an AAOIFI standard document.\"\"\"\n",
        "#     def __init__(self, name: str, content: str):\n",
        "#         self.name = name\n",
        "#         self.content = content\n",
        "\n",
        "# class BaseAgent:\n",
        "#     \"\"\"Base class for all agents in the system.\"\"\"\n",
        "#     def __init__(self, name: str, description: str, model_name: str = Config.GPT4_MODEL):\n",
        "#         self.name = name\n",
        "#         self.description = description\n",
        "#         self.model_name = model_name\n",
        "#         # Ensure API key is set before initializing ChatOpenAI\n",
        "#         if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "#             raise ValueError(\"OPENAI_API_KEY not set in environment.\")\n",
        "#         self.llm = ChatOpenAI(model_name=model_name, temperature=0.2)\n",
        "\n",
        "\n",
        "#     def execute(self, input_data: Any) -> Any:\n",
        "#         \"\"\"Execute the agent's task.\"\"\"\n",
        "#         raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "# class ReviewAgent(BaseAgent):\n",
        "#     \"\"\"Agent responsible for reviewing standards and extracting key elements.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__(\n",
        "#             name=\"ReviewAgent\",\n",
        "#             description=\"Analyzes AAOIFI standards to extract key elements, principles, and requirements.\",\n",
        "#             model_name=Config.GPT4_MODEL\n",
        "#         )\n",
        "\n",
        "#         self.system_prompt = \"\"\"\n",
        "#         You are an expert in Islamic finance and AAOIFI standards. Your task is to carefully review\n",
        "#         the provided standard document and extract the following key elements.\n",
        "#         Use clear Markdown headings for each section exactly as listed below (e.g., ## Core principles and objectives).\n",
        "\n",
        "#         ## Core principles and objectives\n",
        "#         [Your extraction here]\n",
        "\n",
        "#         ## Key definitions and terminology\n",
        "#         [Your extraction here]\n",
        "\n",
        "#         # ... (and so on for other sections) ...\n",
        "\n",
        "#         Be thorough but concise.\n",
        "#         \"\"\"\n",
        "#     def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "#         \"\"\"\n",
        "#         Analyze a standard document and extract its key elements.\n",
        "\n",
        "#         Args:\n",
        "#             standard: The standard document to analyze.\n",
        "\n",
        "#         Returns:\n",
        "#             A dictionary containing the extracted key elements.\n",
        "#         \"\"\"\n",
        "#         prompt = ChatPromptTemplate.from_messages([\n",
        "#             SystemMessage(content=self.system_prompt),\n",
        "#             HumanMessage(content=f\"Standard Name: {standard.name}\\n\\nContent:\\n{standard.content}\")\n",
        "#         ])\n",
        "\n",
        "#         chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "#         # The chain.run({}) is problematic if the prompt itself needs variables.\n",
        "#         # Here, standard.name and standard.content are already formatted into the HumanMessage.\n",
        "#         # If the prompt template had input_variables, they would be passed to run().\n",
        "#         result_text = chain.run({}) # Assuming the prompt is self-contained after formatting.\n",
        "\n",
        "#         # Try to parse the result into structured data (simple approach)\n",
        "#         parsed_result = {\n",
        "#             \"standard_name\": standard.name,\n",
        "#             \"review_result\": result_text, # This is the full text from LLM\n",
        "#             \"core_principles\": self._extract_section(result_text, \"Core principles and objectives\"),\n",
        "#             \"key_definitions\": self._extract_section(result_text, \"Key definitions and terminology\"),\n",
        "#             \"main_requirements\": self._extract_section(result_text, \"Main requirements and procedures\"),\n",
        "#             \"compliance_criteria\": self._extract_section(result_text, \"Compliance criteria and guidelines\"), # \"guidelines\" was missing\n",
        "#             \"implementation_considerations\": self._extract_section(result_text, \"Practical implementation considerations\") # \"Practical\" was missing\n",
        "#         }\n",
        "#         return parsed_result\n",
        "\n",
        "\n",
        "#     def _extract_section(self, text: str, section_name: str) -> str:\n",
        "#         \"\"\"Helper method to extract specific sections from the review result using regex.\"\"\"\n",
        "#         # Regex to find a heading (e.g., \"1. Section Name\" or \"Section Name:\") and capture text until the next heading or end of string\n",
        "#         # This assumes headings are followed by a newline.\n",
        "#         # It also makes section_name matching case-insensitive.\n",
        "#         pattern = re.compile(\n",
        "#             r\"(?i)(?:^\\s*\\d*\\.?\\s*|\\n\\s*\\d*\\.?\\s*)\" + re.escape(section_name) + r\"\\s*[:\\n](.*?)(?=\\n\\s*\\d*\\.?\\s*\\w+.*?\\s*[:\\n]|\\Z)\",\n",
        "#             re.DOTALL | re.IGNORECASE\n",
        "#         )\n",
        "#         match = pattern.search(text)\n",
        "#         if match:\n",
        "#             return match.group(1).strip()\n",
        "#         return f\"Section '{section_name}' not found or parsing error.\"\n",
        "\n",
        "\n",
        "# class EnhancementAgent(BaseAgent):\n",
        "#     \"\"\"Agent tasked with proposing modifications or enhancements to standards.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__(\n",
        "#             name=\"EnhancementAgent\",\n",
        "#             description=\"Proposes AI-driven modifications and enhancements to AAOIFI standards.\",\n",
        "#             model_name=Config.GPT4_MODEL\n",
        "#         )\n",
        "\n",
        "#         self.system_prompt = \"\"\"\n",
        "#         You are an AI expert specializing in Islamic finance and AAOIFI standards enhancement.\n",
        "#         Your task is to propose thoughtful modifications and enhancements to the standard based\n",
        "#         on the review provided.\n",
        "\n",
        "#         Consider the following aspects in your proposals:\n",
        "\n",
        "#         1. Clarity improvements: Suggest clearer language or better organization where appropriate\n",
        "#         2. Modern context adaptations: Propose updates to address contemporary financial practices\n",
        "#         3. Technological integration: Recommend ways to incorporate digital technologies and fintech\n",
        "#         4. Cross-reference enhancements: Suggest improved links to related standards or principles\n",
        "#         5. Practical implementation: Provide more actionable guidance for practitioners\n",
        "\n",
        "#         For each suggestion, provide:\n",
        "#         - The specific section or clause being enhanced (if applicable, otherwise general proposal)\n",
        "#         - The current text or concept (if applicable, very brief summary)\n",
        "#         - Your proposed modification or addition\n",
        "#         - A brief justification explaining the benefit of your enhancement\n",
        "\n",
        "#         Structure your response clearly, perhaps using bullet points for each proposal.\n",
        "#         Ensure all suggestions maintain strict compliance with Shariah principles.\n",
        "#         \"\"\"\n",
        "\n",
        "#     def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "#         \"\"\"\n",
        "#         Propose enhancements to a standard based on the review.\n",
        "\n",
        "#         Args:\n",
        "#             review_result: The result from the ReviewAgent (the full dictionary).\n",
        "\n",
        "#         Returns:\n",
        "#             A dictionary containing proposed enhancements.\n",
        "#         \"\"\"\n",
        "#         # We need the text of the review, not the whole dict, for the prompt\n",
        "#         review_text_summary = review_result.get(\"review_result\", \"No review summary available.\")\n",
        "\n",
        "#         prompt = ChatPromptTemplate.from_messages([\n",
        "#             SystemMessage(content=self.system_prompt),\n",
        "#             HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\n\\nSummary of Standard Review:\\n{review_text_summary}\")\n",
        "#         ])\n",
        "\n",
        "#         chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "#         result_text = chain.run({})\n",
        "\n",
        "#         return {\n",
        "#             \"standard_name\": review_result[\"standard_name\"],\n",
        "#             \"enhancement_proposals\": result_text # This is the LLM's textual output\n",
        "#         }\n",
        "\n",
        "# class ValidationAgent(BaseAgent):\n",
        "#     \"\"\"Agent for validating and approving proposed changes.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__(\n",
        "#             name=\"ValidationAgent\",\n",
        "#             description=\"Validates proposed changes based on Shariah compliance and practicality.\",\n",
        "#             model_name=Config.GPT4_MODEL\n",
        "#         )\n",
        "\n",
        "#         self.system_prompt = \"\"\"\n",
        "#         You are a senior Shariah scholar and AAOIFI standards expert. Your role is to validate\n",
        "#         proposed enhancements to ensure they maintain compliance with Islamic principles and\n",
        "#         practical applicability.\n",
        "\n",
        "#         For each proposed enhancement, evaluate:\n",
        "\n",
        "#         1. Shariah Compliance: Does the proposal align with Islamic principles and AAOIFI's mission?\n",
        "#         2. Technical Accuracy: Is the proposed language precise and technically sound?\n",
        "#         3. Practical Applicability: Can the enhancement be practically implemented by Islamic financial institutions?\n",
        "#         4. Consistency: Does it maintain consistency with other standards and established practices?\n",
        "#         5. Value Addition: Does it meaningfully improve the standard?\n",
        "\n",
        "#         For each proposal, provide:\n",
        "#         - Your assessment (e.g., Approved, Rejected, Needs Modification)\n",
        "#         - A detailed justification for your decision, referencing the evaluation criteria above.\n",
        "#         - Suggested refinements if \"Needs Modification\".\n",
        "\n",
        "#         Be thorough in your analysis and maintain the highest standards of Islamic finance integrity.\n",
        "#         Structure your response clearly, addressing each proposal from the input.\n",
        "#         \"\"\"\n",
        "\n",
        "#     def execute(self, enhancement_result: Dict[str, Any], original_review: Dict[str, Any]) -> Dict[str, Any]:\n",
        "#         \"\"\"\n",
        "#         Validate proposed enhancements based on Shariah compliance and practicality.\n",
        "\n",
        "#         Args:\n",
        "#             enhancement_result: The result from the EnhancementAgent (dictionary).\n",
        "#             original_review: The original review from the ReviewAgent (dictionary).\n",
        "\n",
        "#         Returns:\n",
        "#             A dictionary containing validation results.\n",
        "#         \"\"\"\n",
        "#         review_text_summary = original_review.get(\"review_result\", \"No review summary available.\")\n",
        "#         enhancement_proposals_text = enhancement_result.get(\"enhancement_proposals\", \"No enhancement proposals available.\")\n",
        "\n",
        "#         prompt = ChatPromptTemplate.from_messages([\n",
        "#             SystemMessage(content=self.system_prompt),\n",
        "#             HumanMessage(content=f\"\"\"\n",
        "#             Standard Name: {enhancement_result['standard_name']}\n",
        "\n",
        "#             Original Standard Review Summary:\n",
        "#             {review_text_summary}\n",
        "\n",
        "#             Proposed Enhancements to Validate:\n",
        "#             {enhancement_proposals_text}\n",
        "#             \"\"\")\n",
        "#         ])\n",
        "\n",
        "#         chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "#         result_text = chain.run({})\n",
        "\n",
        "#         return {\n",
        "#             \"standard_name\": enhancement_result[\"standard_name\"],\n",
        "#             \"validation_result\": result_text # This is the LLM's textual output\n",
        "#         }\n",
        "\n",
        "# class FinalReportAgent(BaseAgent):\n",
        "#     \"\"\"Agent for generating a comprehensive final report.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__(\n",
        "#             name=\"FinalReportAgent\",\n",
        "#             description=\"Compiles all findings and recommendations into a comprehensive report.\",\n",
        "#             model_name=Config.GPT4_MODEL # GPT-4 for high-quality report generation\n",
        "#         )\n",
        "\n",
        "#         self.system_prompt = \"\"\"\n",
        "#         You are a professional report writer specializing in Islamic finance standards. Your task is to\n",
        "#         compile all the findings, enhancements, and validations into a comprehensive, well-structured report\n",
        "#         in Markdown format.\n",
        "\n",
        "#         Your report should include the following sections:\n",
        "\n",
        "#         1.  **Executive Summary**: Brief overview of the standard reviewed, key findings, summary of proposed changes, and validation outcomes.\n",
        "#         2.  **Standard Overview**: Summary of the original standard's purpose and core components (based on the review).\n",
        "#         3.  **Key Findings from Review**: Major elements and considerations identified during the initial review.\n",
        "#         4.  **Proposed Enhancements**: Clear presentation of all proposed modifications.\n",
        "#         5.  **Validation Results**: Summary of the validation process and outcomes for each proposal.\n",
        "#         6.  **Consolidated Recommendations**: A final list of approved or modified enhancements.\n",
        "#         7.  **Implementation Considerations**: Practical next steps for adopting approved changes.\n",
        "#         8.  **Conclusion**: Final thoughts on the impact of the proposed enhancements.\n",
        "\n",
        "#         Write in a professional, clear, and objective style appropriate for AAOIFI stakeholders and Islamic finance professionals.\n",
        "#         Use Markdown for formatting (headings, lists, bolding).\n",
        "#         \"\"\"\n",
        "\n",
        "#     def execute(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "#         \"\"\"\n",
        "#         Generate a comprehensive final report.\n",
        "\n",
        "#         Args:\n",
        "#             all_results: Combined results from all previous agents.\n",
        "#                          Expected keys: 'standard_name', 'review_text',\n",
        "#                                         'enhancements_text', 'validation_text'.\n",
        "\n",
        "#         Returns:\n",
        "#             A dictionary containing the final report text.\n",
        "#         \"\"\"\n",
        "#         prompt = ChatPromptTemplate.from_messages([\n",
        "#             SystemMessage(content=self.system_prompt),\n",
        "#             HumanMessage(content=f\"\"\"\n",
        "#             Standard Name: {all_results['standard_name']}\n",
        "\n",
        "#             Full Text of Standard Review:\n",
        "#             {all_results['review_text']}\n",
        "\n",
        "#             Full Text of Proposed Enhancements:\n",
        "#             {all_results['enhancements_text']}\n",
        "\n",
        "#             Full Text of Validation of Enhancements:\n",
        "#             {all_results['validation_text']}\n",
        "#             \"\"\")\n",
        "#         ])\n",
        "\n",
        "#         chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "#         report_text = chain.run({})\n",
        "\n",
        "#         return {\n",
        "#             \"standard_name\": all_results[\"standard_name\"],\n",
        "#             \"final_report\": report_text # This is the LLM's textual output (Markdown)\n",
        "#         }\n",
        "\n",
        "\n",
        "# class VectorDBManager:\n",
        "#     \"\"\"Manager for interacting with the vector database.\"\"\"\n",
        "\n",
        "#     def __init__(self, db_directory: str = None, collection_name: str = config.COLLECTION_NAME):\n",
        "#         self.db_directory = db_directory if db_directory else config.DB_DIRECTORY\n",
        "#         self.collection_name = collection_name\n",
        "#         self.embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
        "\n",
        "#         print(f\"Initializing VectorDBManager with database directory: {self.db_directory}\")\n",
        "\n",
        "#         try:\n",
        "#             if not os.path.exists(self.db_directory):\n",
        "#                 print(f\"Database directory {self.db_directory} does not exist. Creating it.\")\n",
        "#                 os.makedirs(self.db_directory, exist_ok=True)\n",
        "#             self.client = chromadb.PersistentClient(path=self.db_directory)\n",
        "#             print(f\"Successfully created PersistentClient for database at: {self.db_directory}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Fatal error creating PersistentClient for {self.db_directory}: {str(e)}\")\n",
        "#             self.client = None\n",
        "#             self.collection = None\n",
        "#             raise  # Reraise critical error\n",
        "\n",
        "#         if self.client:\n",
        "#             try:\n",
        "#                 # Try to get the collection; if it fails, try to create it.\n",
        "#                 self.collection = self.client.get_or_create_collection(name=self.collection_name)\n",
        "#                 print(f\"Successfully accessed/created collection: {self.collection_name}\")\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error accessing/creating collection '{self.collection_name}': {str(e)}\")\n",
        "#                 self.collection = None\n",
        "#         else:\n",
        "#             self.collection = None\n",
        "\n",
        "\n",
        "#     def get_standard_content(self, standard_name: str) -> str:\n",
        "#         \"\"\"\n",
        "#         Retrieve the content for a specific standard.\n",
        "\n",
        "#         Args:\n",
        "#             standard_name: The name of the standard to retrieve.\n",
        "\n",
        "#         Returns:\n",
        "#             The combined content of the standard, or an error message.\n",
        "#         \"\"\"\n",
        "#         if not self.collection:\n",
        "#             return \"Error: Vector database collection not properly initialized.\"\n",
        "\n",
        "#         try:\n",
        "#             # Query for all chunks belonging to this standard\n",
        "#             # A large n_results is needed if a standard has many chunks\n",
        "#             # Chroma's default n_results is 10.\n",
        "#             # To get ALL documents matching a filter, it's better to use collection.get()\n",
        "#             results = self.collection.get(\n",
        "#                 where={\"source\": standard_name},\n",
        "#                 include=[\"documents\"] # Only fetch documents\n",
        "#             )\n",
        "\n",
        "#             if results and results['documents']:\n",
        "#                 # Sort by chunk_id if available in metadata, though .get() doesn't easily allow sorting\n",
        "#                 # For now, just join them. If order is critical, store chunk_id and sort post-retrieval.\n",
        "#                 # The current split_text_into_chunks adds metadata, but .get() returns raw docs.\n",
        "#                 # Let's assume for now that the order from .get() is sufficient or re-query with sort if needed.\n",
        "#                 # The current query in `create_vector_database` uses IDs like `doc_{i+j}` which are sequential.\n",
        "#                 # If we rely on implicit order of `get`, it should be mostly fine.\n",
        "\n",
        "#                 # The metadata is not directly available here to sort by chunk_id without another call or more complex query\n",
        "#                 # For simplicity, joining in received order.\n",
        "#                 standard_content = \"\\n\\n\".join(doc for doc in results['documents'] if doc)\n",
        "#                 if not standard_content:\n",
        "#                     return f\"No document content found for standard: {standard_name}, though metadatas might exist.\"\n",
        "#                 return standard_content\n",
        "#             else:\n",
        "#                 return f\"No content found for standard: {standard_name}\"\n",
        "#         except Exception as e:\n",
        "#             return f\"Error querying for standard {standard_name}: {str(e)}\"\n",
        "\n",
        "#     def list_available_standards(self) -> List[str]:\n",
        "#         \"\"\"\n",
        "#         List all available standards in the database.\n",
        "\n",
        "#         Returns:\n",
        "#             A list of standard names, or an error message list.\n",
        "#         \"\"\"\n",
        "#         if not self.collection:\n",
        "#             return [\"Error: Vector database collection not properly initialized.\"]\n",
        "\n",
        "#         try:\n",
        "#             results = self.collection.get(include=[\"metadatas\"]) # Fetch only metadatas\n",
        "#             if results and results['metadatas']:\n",
        "#                 standards = set()\n",
        "#                 for metadata in results['metadatas']:\n",
        "#                     if metadata and 'source' in metadata: # Check if metadata is not None\n",
        "#                         standards.add(metadata['source'])\n",
        "#                 if not standards:\n",
        "#                     return [\"No standards found in the database. Metadatas might be empty or lack 'source'.\"]\n",
        "#                 return sorted(list(standards))\n",
        "#             else:\n",
        "#                 return [\"No standards found in the database. Please process PDFs first.\"]\n",
        "#         except Exception as e:\n",
        "#             return [f\"Error listing standards: {str(e)}\"]\n",
        "\n",
        "\n",
        "# class AAOIFIStandardsEnhancementSystem:\n",
        "#     \"\"\"Main system coordinating the multi-agent process.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         # Initialize vector database manager\n",
        "#         # It will use config.DB_DIRECTORY which might be updated by process_pdfs_safe\n",
        "#         self.db_manager = VectorDBManager()\n",
        "\n",
        "#         # Initialize agents\n",
        "#         self.review_agent = ReviewAgent()\n",
        "#         self.enhancement_agent = EnhancementAgent()\n",
        "#         self.validation_agent = ValidationAgent()\n",
        "#         self.report_agent = FinalReportAgent()\n",
        "\n",
        "#         # Track processing results for a single standard run\n",
        "#         self.current_processing_results = {}\n",
        "\n",
        "#     def list_available_standards(self) -> List[str]:\n",
        "#         \"\"\"List all available standards in the system.\"\"\"\n",
        "#         return self.db_manager.list_available_standards()\n",
        "\n",
        "#     def process_standard(self, standard_name: str) -> Dict[str, Any]:\n",
        "#         \"\"\"\n",
        "#         Process a single standard through the complete pipeline.\n",
        "\n",
        "#         Args:\n",
        "#             standard_name: The name of the standard to process.\n",
        "\n",
        "#         Returns:\n",
        "#             A dictionary containing the final results for this standard.\n",
        "#         \"\"\"\n",
        "#         print(f\"\\nProcessing standard: {standard_name}\")\n",
        "#         self.current_processing_results = {\"standard_name\": standard_name}\n",
        "\n",
        "#         # Step 1: Get standard content from the vector database\n",
        "#         print(\"  Retrieving standard content...\")\n",
        "#         content = self.db_manager.get_standard_content(standard_name)\n",
        "#         if content.startswith(\"Error:\") or content.startswith(\"No content found\"):\n",
        "#             print(f\"  Error retrieving content for {standard_name}: {content}\")\n",
        "#             self.current_processing_results[\"error\"] = content\n",
        "#             return self.current_processing_results\n",
        "\n",
        "#         standard_doc = StandardDocument(name=standard_name, content=content)\n",
        "#         self.current_processing_results[\"original_content_excerpt\"] = content[:500] + \"...\" if len(content) > 500 else content\n",
        "\n",
        "#         # Step 2: Review and extract key elements\n",
        "#         print(\"  Reviewing standard and extracting key elements...\")\n",
        "#         review_output = self.review_agent.execute(standard_doc)\n",
        "#         self.current_processing_results[\"review_output\"] = review_output # Store the whole dict\n",
        "\n",
        "#         # Step 3: Propose enhancements\n",
        "#         print(\"  Proposing enhancements...\")\n",
        "#         enhancement_output = self.enhancement_agent.execute(review_output) # Pass the dict\n",
        "#         self.current_processing_results[\"enhancement_output\"] = enhancement_output\n",
        "\n",
        "#         # Step 4: Validate proposed changes\n",
        "#         print(\"  Validating proposed changes...\")\n",
        "#         # Validation agent needs enhancement_output (dict) and review_output (dict)\n",
        "#         validation_output = self.validation_agent.execute(enhancement_output, review_output)\n",
        "#         self.current_processing_results[\"validation_output\"] = validation_output\n",
        "\n",
        "#         # Step 5: Generate final report\n",
        "#         print(\"  Generating final report...\")\n",
        "#         # Prepare data for report agent\n",
        "#         report_input_data = {\n",
        "#             \"standard_name\": standard_name,\n",
        "#             \"review_text\": review_output.get(\"review_result\", \"N/A\"),\n",
        "#             \"enhancements_text\": enhancement_output.get(\"enhancement_proposals\", \"N/A\"),\n",
        "#             \"validation_text\": validation_output.get(\"validation_result\", \"N/A\")\n",
        "#         }\n",
        "#         final_report_output = self.report_agent.execute(report_input_data)\n",
        "#         self.current_processing_results[\"final_report_output\"] = final_report_output\n",
        "\n",
        "#         print(f\"Processing completed for standard: {standard_name}\")\n",
        "#         return self.current_processing_results\n",
        "\n",
        "#     def save_results(self, results_to_save: Dict[str, Any], output_dir: str = config.OUTPUT_DIR):\n",
        "#         \"\"\"Save all results to output files.\"\"\"\n",
        "#         os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "#         if not results_to_save:\n",
        "#             print(\"No results to save.\")\n",
        "#             return\n",
        "\n",
        "#         standard_name = results_to_save.get(\"standard_name\", \"unknown_standard\")\n",
        "#         # Sanitize standard_name for filename\n",
        "#         safe_standard_name = re.sub(r'[^\\w\\-_\\. ]', '_', standard_name)\n",
        "#         timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "#         filename_json = os.path.join(output_dir, f\"{safe_standard_name}_full_results_{timestamp}.json\")\n",
        "#         with open(filename_json, 'w', encoding='utf-8') as f:\n",
        "#             json.dump(results_to_save, f, indent=2, ensure_ascii=False)\n",
        "#         print(f\"Full results saved to {filename_json}\")\n",
        "\n",
        "#         # Also save the final report separately as Markdown\n",
        "#         final_report_text = results_to_save.get(\"final_report_output\", {}).get(\"final_report\")\n",
        "#         if final_report_text:\n",
        "#             filename_md = os.path.join(output_dir, f\"{safe_standard_name}_final_report_{timestamp}.md\")\n",
        "#             with open(filename_md, 'w', encoding='utf-8') as f:\n",
        "#                 f.write(final_report_text)\n",
        "#             print(f\"Final report saved to {filename_md}\")\n",
        "\n",
        "\n",
        "# # --- Functions for recreating vector database safely ---\n",
        "# def safely_recreate_vector_database(documents):\n",
        "#     \"\"\"Create a new vector database from document chunks with proper handling of locked files.\"\"\"\n",
        "#     embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
        "\n",
        "#     # Create a new directory with a unique name to avoid conflicts\n",
        "#     timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "#     random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
        "#     # Base directory for all DB versions\n",
        "#     base_db_parent_dir = \"aaoifi_vector_db_versions\"\n",
        "#     os.makedirs(base_db_parent_dir, exist_ok=True)\n",
        "#     new_db_directory = os.path.join(base_db_parent_dir, f\"db_{timestamp}_{random_str}\")\n",
        "\n",
        "#     print(f\"Creating new database in directory: {new_db_directory}\")\n",
        "#     os.makedirs(new_db_directory, exist_ok=True)\n",
        "\n",
        "#     client = chromadb.PersistentClient(path=new_db_directory)\n",
        "#     # Always create a new collection in a new DB path\n",
        "#     collection = client.create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "#     batch_size = 100 # As used in create_vector_database\n",
        "#     num_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "\n",
        "#     for i in range(0, len(documents), batch_size):\n",
        "#         batch_documents = documents[i:i+batch_size]\n",
        "#         current_batch_num = (i // batch_size) + 1\n",
        "#         print(f\"Processing batch {current_batch_num}/{num_batches} for new vector database...\")\n",
        "\n",
        "#         texts = [doc[\"content\"] for doc in batch_documents]\n",
        "#         # Use more robust IDs tied to document source and chunk\n",
        "#         ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}\" for doc in batch_documents]\n",
        "#         # Check for ID uniqueness within the batch (essential for Chroma)\n",
        "#         if len(ids) != len(set(ids)):\n",
        "#             print(f\"Warning: Duplicate IDs generated in batch {current_batch_num}. This may cause issues.\")\n",
        "#             # Simple fix: append index within batch to ensure uniqueness for this add op\n",
        "#             ids = [f\"{id_}_{j}\" for j, id_ in enumerate(ids)]\n",
        "\n",
        "#         metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
        "\n",
        "#         if not texts:\n",
        "#             print(f\"Skipping empty batch {current_batch_num}.\")\n",
        "#             continue\n",
        "#         try:\n",
        "#             embeds = embeddings.embed_documents(texts)\n",
        "#             collection.add(embeddings=embeds, documents=texts, ids=ids, metadatas=metadatas)\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing batch {current_batch_num} starting at index {i}: {str(e)}\")\n",
        "#             continue\n",
        "\n",
        "#     print(f\"Created new vector database with consistent embedding dimensions in '{new_db_directory}'\")\n",
        "\n",
        "#     # Update the global config to point to the new directory for the current session\n",
        "#     config.DB_DIRECTORY = new_db_directory\n",
        "#     print(f\"Updated global config.DB_DIRECTORY to: {config.DB_DIRECTORY}\")\n",
        "\n",
        "#     return client\n",
        "\n",
        "# def process_pdfs_safe():\n",
        "#     \"\"\"Main function to process PDFs and create vector database safely.\"\"\"\n",
        "#     PDF_FOLDER = config.PDF_FOLDER\n",
        "#     if not os.path.exists(PDF_FOLDER):\n",
        "#         print(f\"Error: The PDF folder '{PDF_FOLDER}' does not exist.\")\n",
        "#         return None\n",
        "#     if not os.listdir(PDF_FOLDER):\n",
        "#         print(f\"The PDF folder '{PDF_FOLDER}' is empty. Please add PDF files to process.\")\n",
        "#         return None\n",
        "\n",
        "#     all_documents = []\n",
        "#     pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "#     if not pdf_files:\n",
        "#         print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "#         return None\n",
        "\n",
        "#     print(f\"Found {len(pdf_files)} PDF files.\")\n",
        "#     for pdf_file in pdf_files:\n",
        "#         pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "#         standard_name = os.path.splitext(pdf_file)[0]\n",
        "#         print(f\"Processing {pdf_file}...\")\n",
        "#         raw_text = extract_text_from_pdf(pdf_path)\n",
        "#         cleaned_text = clean_text(raw_text)\n",
        "#         chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
        "#         all_documents.extend(chunks)\n",
        "#         print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "#     if not all_documents:\n",
        "#         print(\"No documents were extracted from PDFs. Cannot create vector database.\")\n",
        "#         return None\n",
        "\n",
        "#     print(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "#     print(\"Creating/Recreating vector database safely...\")\n",
        "#     client = safely_recreate_vector_database(all_documents) # This updates config.DB_DIRECTORY\n",
        "#     print(f\"Vector database operations completed. Current DB directory: '{config.DB_DIRECTORY}'\")\n",
        "#     return client\n",
        "\n",
        "# # --- START OF NEW DEMONSTRATION CODE ---\n",
        "# \"\"\"\n",
        "# AAOIFI Standards Enhancement System Demo\n",
        "\n",
        "# This script demonstrates how to use the multi-agent architecture to:\n",
        "# 1.  Select an AAOIFI standard\n",
        "# 2.  Review and extract key elements using ReviewAgent\n",
        "# 3.  Propose AI-driven modifications/enhancements using EnhancementAgent\n",
        "# 4.  Validate proposed changes based on Shariah principles using ValidationAgent\n",
        "# 5.  Generate a comprehensive report using FinalReportAgent\n",
        "# 6.  Optionally, use additional agents for visualization and feedback simulation.\n",
        "# \"\"\"\n",
        "\n",
        "# class DemoRunner:\n",
        "#     \"\"\"Class to run a complete demonstration of the AAOIFI Standards Enhancement System.\"\"\"\n",
        "\n",
        "#     def __init__(self, system: AAOIFIStandardsEnhancementSystem):\n",
        "#         self.system = system\n",
        "#         self.selected_standard_name: Optional[str] = None\n",
        "#         self.current_demo_results: Dict[str, Any] = {} # Stores results for the current demo run\n",
        "\n",
        "#     def _get_excerpt(self, text: Optional[str], max_length: int = 300) -> str:\n",
        "#         \"\"\"Get an excerpt of text with maximum length.\"\"\"\n",
        "#         if not text:\n",
        "#             return \"N/A\"\n",
        "#         text = str(text) # Ensure it's a string\n",
        "#         if len(text) <= max_length:\n",
        "#             return text\n",
        "#         return text[:max_length] + \"...\"\n",
        "\n",
        "#     def list_and_select_standard(self) -> bool:\n",
        "#         \"\"\"List all available standards and allow selection.\"\"\"\n",
        "#         print(\"\\n\" + \"=\"*50)\n",
        "#         print(\"AAOIFI STANDARDS ENHANCEMENT SYSTEM DEMONSTRATION\")\n",
        "#         print(\"=\"*50)\n",
        "\n",
        "#         print(\"\\nAvailable standards:\")\n",
        "#         standards = self.system.list_available_standards()\n",
        "\n",
        "#         if not standards or any(s.startswith(\"Error:\") or s.startswith(\"No standards found\") for s in standards):\n",
        "#             print(\"No standards available or error listing standards.\")\n",
        "#             print(\"Please ensure PDFs are processed and a vector database exists.\")\n",
        "#             if standards: print(f\"Details: {standards[0]}\")\n",
        "#             return False\n",
        "\n",
        "#         for i, standard_name in enumerate(standards):\n",
        "#             print(f\"{i+1}. {standard_name}\")\n",
        "\n",
        "#         while True:\n",
        "#             try:\n",
        "#                 choice_str = input(f\"\\nSelect a standard to process (enter number 1-{len(standards)}): \")\n",
        "#                 if not choice_str: continue # Handle empty input\n",
        "#                 choice = int(choice_str)\n",
        "#                 if 1 <= choice <= len(standards):\n",
        "#                     self.selected_standard_name = standards[choice-1]\n",
        "#                     print(f\"\\nSelected standard: {self.selected_standard_name}\")\n",
        "#                     self.current_demo_results = {\"standard_name\": self.selected_standard_name} # Reset for new selection\n",
        "#                     return True\n",
        "#                 else:\n",
        "#                     print(f\"Invalid selection. Please enter a number between 1 and {len(standards)}.\")\n",
        "#             except ValueError:\n",
        "#                 print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "#     def run_review_phase(self) -> bool:\n",
        "#         \"\"\"Run the review phase and display results.\"\"\"\n",
        "#         if not self.selected_standard_name:\n",
        "#             print(\"Error: No standard selected for review phase.\")\n",
        "#             return False\n",
        "\n",
        "#         print(\"\\n\" + \"-\"*50)\n",
        "#         print(\"PHASE 1: STANDARD REVIEW AND ANALYSIS (ReviewAgent)\")\n",
        "#         print(\"-\"*50)\n",
        "\n",
        "#         print(f\"\\nRetrieving content for standard: {self.selected_standard_name}...\")\n",
        "#         content = self.system.db_manager.get_standard_content(self.selected_standard_name)\n",
        "#         if content.startswith(\"Error:\") or content.startswith(\"No content found\"):\n",
        "#             print(f\"  Error retrieving content: {content}\")\n",
        "#             self.current_demo_results[\"review_error\"] = content\n",
        "#             return False\n",
        "\n",
        "#         standard_document = StandardDocument(name=self.selected_standard_name, content=content)\n",
        "\n",
        "#         print(f\"Reviewing standard using ReviewAgent...\")\n",
        "#         start_time = time.time()\n",
        "#         review_output = self.system.review_agent.execute(standard_document)\n",
        "#         self.current_demo_results[\"review_output\"] = review_output\n",
        "#         elapsed_time = time.time() - start_time\n",
        "#         print(f\"ReviewAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "#         print(\"\\nREVIEW SUMMARY:\")\n",
        "#         print(f\"- Standard: {review_output.get('standard_name', 'N/A')}\")\n",
        "#         print(f\"\\nCore Principles (excerpt):\\n{self._get_excerpt(review_output.get('core_principles'))}\")\n",
        "#         print(f\"\\nKey Definitions (excerpt):\\n{self._get_excerpt(review_output.get('key_definitions'))}\")\n",
        "#         # print(f\"\\nFull Review Text (excerpt):\\n{self._get_excerpt(review_output.get('review_result'), 500)}\")\n",
        "#         return True\n",
        "\n",
        "#     def run_enhancement_phase(self) -> bool:\n",
        "#         \"\"\"Run the enhancement phase and display results.\"\"\"\n",
        "#         if \"review_output\" not in self.current_demo_results:\n",
        "#             print(\"Error: Review phase must complete successfully before enhancement.\")\n",
        "#             return False\n",
        "\n",
        "#         print(\"\\n\" + \"-\"*50)\n",
        "#         print(\"PHASE 2: AI-DRIVEN ENHANCEMENT PROPOSALS (EnhancementAgent)\")\n",
        "#         print(\"-\"*50)\n",
        "\n",
        "#         review_output = self.current_demo_results[\"review_output\"]\n",
        "#         print(f\"\\nGenerating enhancement proposals for: {self.selected_standard_name} using EnhancementAgent...\")\n",
        "\n",
        "#         start_time = time.time()\n",
        "#         enhancement_output = self.system.enhancement_agent.execute(review_output) # Pass full review dict\n",
        "#         self.current_demo_results[\"enhancement_output\"] = enhancement_output\n",
        "#         elapsed_time = time.time() - start_time\n",
        "#         print(f\"EnhancementAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "#         print(\"\\nENHANCEMENT PROPOSALS (excerpt):\")\n",
        "#         print(self._get_excerpt(enhancement_output.get(\"enhancement_proposals\"), 500))\n",
        "#         return True\n",
        "\n",
        "#     def run_validation_phase(self) -> bool:\n",
        "#         \"\"\"Run the validation phase and display results.\"\"\"\n",
        "#         if \"enhancement_output\" not in self.current_demo_results or \\\n",
        "#            \"review_output\" not in self.current_demo_results:\n",
        "#             print(\"Error: Review and Enhancement phases must complete successfully before validation.\")\n",
        "#             return False\n",
        "\n",
        "#         print(\"\\n\" + \"-\"*50)\n",
        "#         print(\"PHASE 3: SHARIAH COMPLIANCE VALIDATION (ValidationAgent)\")\n",
        "#         print(\"-\"*50)\n",
        "\n",
        "#         review_output = self.current_demo_results[\"review_output\"]\n",
        "#         enhancement_output = self.current_demo_results[\"enhancement_output\"]\n",
        "\n",
        "#         print(f\"\\nValidating proposed enhancements for: {self.selected_standard_name} using ValidationAgent...\")\n",
        "#         start_time = time.time()\n",
        "#         validation_output = self.system.validation_agent.execute(enhancement_output, review_output)\n",
        "#         self.current_demo_results[\"validation_output\"] = validation_output\n",
        "#         elapsed_time = time.time() - start_time\n",
        "#         print(f\"ValidationAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "#         print(\"\\nVALIDATION RESULTS (excerpt):\")\n",
        "#         print(self._get_excerpt(validation_output.get(\"validation_result\"), 500))\n",
        "#         return True\n",
        "\n",
        "#     def run_report_generation_phase(self) -> bool:\n",
        "#         \"\"\"Generate the final comprehensive report.\"\"\"\n",
        "#         if \"validation_output\" not in self.current_demo_results: # Implies previous phases also ran\n",
        "#             print(\"Error: All previous phases (Review, Enhance, Validate) must complete before report generation.\")\n",
        "#             return False\n",
        "\n",
        "#         print(\"\\n\" + \"-\"*50)\n",
        "#         print(\"PHASE 4: COMPREHENSIVE REPORT GENERATION (FinalReportAgent)\")\n",
        "#         print(\"-\"*50)\n",
        "\n",
        "#         print(f\"\\nGenerating comprehensive report for: {self.selected_standard_name} using FinalReportAgent...\")\n",
        "\n",
        "#         report_input_data = {\n",
        "#             \"standard_name\": self.selected_standard_name,\n",
        "#             \"review_text\": self.current_demo_results.get(\"review_output\", {}).get(\"review_result\", \"N/A\"),\n",
        "#             \"enhancements_text\": self.current_demo_results.get(\"enhancement_output\", {}).get(\"enhancement_proposals\", \"N/A\"),\n",
        "#             \"validation_text\": self.current_demo_results.get(\"validation_output\", {}).get(\"validation_result\", \"N/A\")\n",
        "#         }\n",
        "\n",
        "#         start_time = time.time()\n",
        "#         final_report_output = self.system.report_agent.execute(report_input_data)\n",
        "#         self.current_demo_results[\"final_report_output\"] = final_report_output\n",
        "#         elapsed_time = time.time() - start_time\n",
        "#         print(f\"FinalReportAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "#         print(\"\\nFINAL REPORT (excerpt):\")\n",
        "#         print(self._get_excerpt(final_report_output.get(\"final_report\"), 600))\n",
        "#         return True\n",
        "\n",
        "#     def save_demo_results(self):\n",
        "#         \"\"\"Save all results from the demonstration.\"\"\"\n",
        "#         if not self.current_demo_results or not self.selected_standard_name:\n",
        "#             print(\"No results to save or standard not selected.\")\n",
        "#             return\n",
        "\n",
        "#         print(\"\\n\" + \"-\"*50)\n",
        "#         print(\"SAVING DEMONSTRATION RESULTS\")\n",
        "#         print(\"-\"*50)\n",
        "\n",
        "#         # Use the system's save_results method, passing the demo's collected results\n",
        "#         self.system.save_results(self.current_demo_results) # It handles directory creation and naming\n",
        "\n",
        "#     def run_complete_demo(self):\n",
        "#         \"\"\"Run the complete demonstration process.\"\"\"\n",
        "#         if not self.list_and_select_standard():\n",
        "#             print(\"Demo terminated: Standard selection failed or was aborted.\")\n",
        "#             return\n",
        "\n",
        "#         if not self.run_review_phase():\n",
        "#             print(\"Demo terminated: Review phase failed.\")\n",
        "#             return\n",
        "\n",
        "#         if not self.run_enhancement_phase():\n",
        "#             print(\"Demo terminated: Enhancement phase failed.\")\n",
        "#             return\n",
        "\n",
        "#         if not self.run_validation_phase():\n",
        "#             print(\"Demo terminated: Validation phase failed.\")\n",
        "#             return\n",
        "\n",
        "#         if not self.run_report_generation_phase():\n",
        "#             print(\"Demo terminated: Report generation phase failed.\")\n",
        "#             return\n",
        "\n",
        "#         self.save_demo_results()\n",
        "\n",
        "#         print(\"\\n\" + \"=\"*50)\n",
        "#         print(f\"DEMONSTRATION COMPLETED SUCCESSFULLY FOR STANDARD: {self.selected_standard_name}\")\n",
        "#         print(\"=\"*50)\n",
        "#         print(f\"All results and the final report have been saved to the '{config.OUTPUT_DIR}' directory.\")\n",
        "\n",
        "# # --- Additional Agents for Enhanced Demo ---\n",
        "\n",
        "# class VisualizationAgent(BaseAgent):\n",
        "#     \"\"\"Agent responsible for generating textual descriptions for visualizations.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__(\n",
        "#             name=\"VisualizationAgent\",\n",
        "#             description=\"Creates textual summaries suitable for generating visual representations of standard changes.\",\n",
        "#             model_name=Config.GPT35_MODEL # Faster model for summarization\n",
        "#         )\n",
        "\n",
        "#         self.system_prompt = \"\"\"\n",
        "#         You are a data visualization assistant. Your task is to process the provided summaries of AAOIFI standard reviews,\n",
        "#         enhancement proposals, and validation results, and then generate concise textual descriptions that could be used\n",
        "#         to create visual elements (like tables or charts).\n",
        "\n",
        "#         Focus on:\n",
        "#         1.  A summary table of key proposed changes: Section | Original Concept | Proposed Change | Benefit.\n",
        "#         2.  A Shariah compliance assessment summary: Proposal Area | Compliance Status | Key Shariah Considerations.\n",
        "#         3.  A benefits overview: Key Enhancement | Primary Benefit to Stakeholders.\n",
        "\n",
        "#         Present this information clearly using Markdown tables or structured lists.\n",
        "#         \"\"\"\n",
        "\n",
        "#     def _get_excerpt(self, text: Optional[str], max_length: int = 300) -> str: # Helper from DemoRunner\n",
        "#         if not text: return \"N/A\"\n",
        "#         text = str(text)\n",
        "#         if len(text) <= max_length: return text\n",
        "#         return text[:max_length] + \"...\"\n",
        "\n",
        "#     def execute(self, demo_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "#         standard_name = demo_results.get('standard_name', 'N/A')\n",
        "#         review_text = self._get_excerpt(demo_results.get('review_output', {}).get('review_result', 'N/A'), 1000)\n",
        "#         enhancements_text = self._get_excerpt(demo_results.get('enhancement_output', {}).get('enhancement_proposals', 'N/A'), 1000)\n",
        "#         validation_text = self._get_excerpt(demo_results.get('validation_output', {}).get('validation_result', 'N/A'), 1000)\n",
        "\n",
        "#         prompt = ChatPromptTemplate.from_messages([\n",
        "#             SystemMessage(content=self.system_prompt),\n",
        "#             HumanMessage(content=f\"\"\"\n",
        "#             Standard Name: {standard_name}\n",
        "\n",
        "#             Summary of Standard Review:\n",
        "#             {review_text}\n",
        "\n",
        "#             Summary of Proposed Enhancements:\n",
        "#             {enhancements_text}\n",
        "\n",
        "#             Summary of Validation Results:\n",
        "#             {validation_text}\n",
        "\n",
        "#             Please generate textual descriptions suitable for visualization based on the above.\n",
        "#             \"\"\")\n",
        "#         ])\n",
        "\n",
        "#         chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "#         visualization_text_data = chain.run({})\n",
        "\n",
        "#         return {\n",
        "#             \"standard_name\": standard_name,\n",
        "#             \"visualization_text_data\": visualization_text_data\n",
        "#         }\n",
        "\n",
        "# class FeedbackAgent(BaseAgent):\n",
        "#     \"\"\"Agent for collecting and analyzing simulated feedback on proposed changes.\"\"\"\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super().__init__(\n",
        "#             name=\"FeedbackAgent\",\n",
        "#             description=\"Processes simulated feedback on proposed standard changes and generates insights.\",\n",
        "#             model_name=Config.GPT35_MODEL\n",
        "#         )\n",
        "\n",
        "#         self.system_prompt = \"\"\"\n",
        "#         You are an expert in qualitative feedback analysis for financial standards.\n",
        "#         Given a set of simulated feedback entries on proposed changes to an AAOIFI standard,\n",
        "#         your task is to:\n",
        "\n",
        "#         1.  Summarize the overall sentiment (Positive, Negative, Mixed).\n",
        "#         2.  Identify 2-3 key themes or concerns raised by stakeholders.\n",
        "#         3.  Highlight 2-3 constructive suggestions for further improvement, if any.\n",
        "#         4.  Note any areas of strong consensus or significant disagreement.\n",
        "\n",
        "#         Provide a concise analysis.\n",
        "#         \"\"\"\n",
        "\n",
        "#     def execute(self, standard_name: str, simulated_feedback_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "#         formatted_feedback = \"\\n\\n\".join([\n",
        "#             f\"Feedback Entry #{i+1}:\\nStakeholder Role: {item.get('role', 'N/A')}\\nRating: {item.get('rating', 'N/A')}/5\\nComment: {item.get('comment', 'N/A')}\"\n",
        "#             for i, item in enumerate(simulated_feedback_list)\n",
        "#         ])\n",
        "\n",
        "#         prompt = ChatPromptTemplate.from_messages([\n",
        "#             SystemMessage(content=self.system_prompt),\n",
        "#             HumanMessage(content=f\"\"\"\n",
        "#             Standard Name: {standard_name}\n",
        "\n",
        "#             Simulated Stakeholder Feedback:\n",
        "#             {formatted_feedback}\n",
        "\n",
        "#             Please analyze this feedback.\n",
        "#             \"\"\")\n",
        "#         ])\n",
        "\n",
        "#         chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "#         analysis_result = chain.run({})\n",
        "\n",
        "#         avg_rating = sum(item.get('rating', 0) for item in simulated_feedback_list) / len(simulated_feedback_list) if simulated_feedback_list else 0\n",
        "\n",
        "#         return {\n",
        "#             \"standard_name\": standard_name,\n",
        "#             \"feedback_count\": len(simulated_feedback_list),\n",
        "#             \"average_simulated_rating\": f\"{avg_rating:.2f}/5.00\",\n",
        "#             \"feedback_analysis_summary\": analysis_result\n",
        "#         }\n",
        "\n",
        "# class EnhancedDemoRunner(DemoRunner):\n",
        "#     \"\"\"Enhanced demo runner with visualization and feedback capabilities.\"\"\"\n",
        "\n",
        "#     def __init__(self, system: AAOIFIStandardsEnhancementSystem):\n",
        "#         super().__init__(system)\n",
        "#         self.visualization_agent = VisualizationAgent()\n",
        "#         self.feedback_agent = FeedbackAgent()\n",
        "\n",
        "#     def run_visualization_phase(self) -> bool:\n",
        "#         \"\"\"Generate textual descriptions for visualizations based on the results.\"\"\"\n",
        "#         if not self.current_demo_results.get(\"final_report_output\"): # Check if prior phases are done\n",
        "#             print(\"Error: Core demo phases must complete before visualization.\")\n",
        "#             return False\n",
        "\n",
        "#         print(\"\\n\" + \"-\"*50)\n",
        "#         print(\"PHASE 5: GENERATING VISUALIZATION DATA (VisualizationAgent)\")\n",
        "#         print(\"-\"*50)\n",
        "\n",
        "#         print(f\"\\nGenerating visualization text for: {self.selected_standard_name} using VisualizationAgent...\")\n",
        "#         start_time = time.time()\n",
        "#         visualization_output = self.visualization_agent.execute(self.current_demo_results)\n",
        "#         self.current_demo_results[\"visualization_output\"] = visualization_output\n",
        "#         elapsed_time = time.time() - start_time\n",
        "#         print(f\"VisualizationAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "#         print(\"\\nVISUALIZATION TEXT DATA (excerpt):\")\n",
        "#         print(self._get_excerpt(visualization_output.get(\"visualization_text_data\"), 500))\n",
        "#         return True\n",
        "\n",
        "#     def run_simulated_feedback_phase(self) -> bool:\n",
        "#         \"\"\"Simulate stakeholder feedback and analyze it.\"\"\"\n",
        "#         if not self.current_demo_results.get(\"final_report_output\"):\n",
        "#             print(\"Error: Core demo phases must complete before feedback simulation.\")\n",
        "#             return False\n",
        "\n",
        "#         print(\"\\n\" + \"-\"*50)\n",
        "#         print(\"PHASE 6: SIMULATED FEEDBACK ANALYSIS (FeedbackAgent)\")\n",
        "#         print(\"-\"*50)\n",
        "\n",
        "#         print(f\"\\nSimulating stakeholder feedback for: {self.selected_standard_name}...\")\n",
        "#         simulated_feedback = [\n",
        "#             {\"role\": \"Shariah Scholar\", \"rating\": 4, \"comment\": \"The proposals are largely compliant, but clarification is needed on the application of 'gharar yaseer' in proposed digital contracts.\"},\n",
        "#             {\"role\": \"Banker\", \"rating\": 5, \"comment\": \"These changes will significantly improve operational efficiency and reduce ambiguity in sukuk issuance.\"},\n",
        "#             {\"role\": \"Regulator\", \"rating\": 3, \"comment\": \"While the intent is good, the proposed technological integrations might pose supervisory challenges. We need more detailed risk management guidelines.\"},\n",
        "#             {\"role\": \"Academic\", \"rating\": 4, \"comment\": \"A solid step forward. I suggest including references to contemporary research on fintech in Islamic finance for a more robust theoretical underpinning.\"}\n",
        "#         ]\n",
        "#         print(f\"Generated {len(simulated_feedback)} simulated feedback entries.\")\n",
        "\n",
        "#         print(f\"Analyzing simulated feedback using FeedbackAgent...\")\n",
        "#         start_time = time.time()\n",
        "#         feedback_analysis_output = self.feedback_agent.execute(self.selected_standard_name, simulated_feedback)\n",
        "#         self.current_demo_results[\"feedback_analysis_output\"] = feedback_analysis_output\n",
        "#         elapsed_time = time.time() - start_time\n",
        "#         print(f\"FeedbackAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "#         print(\"\\nSIMULATED FEEDBACK ANALYSIS (excerpt):\")\n",
        "#         print(self._get_excerpt(feedback_analysis_output.get(\"feedback_analysis_summary\"), 500))\n",
        "#         return True\n",
        "\n",
        "#     def run_complete_enhanced_demo(self):\n",
        "#         \"\"\"Run the complete enhanced demonstration process.\"\"\"\n",
        "#         if not self.list_and_select_standard():\n",
        "#             print(\"Enhanced Demo terminated: Standard selection failed.\")\n",
        "#             return\n",
        "\n",
        "#         if not self.run_review_phase():\n",
        "#             print(\"Enhanced Demo terminated: Review phase failed.\")\n",
        "#             return\n",
        "\n",
        "#         if not self.run_enhancement_phase():\n",
        "#             print(\"Enhanced Demo terminated: Enhancement phase failed.\")\n",
        "#             return\n",
        "\n",
        "#         if not self.run_validation_phase():\n",
        "#             print(\"Enhanced Demo terminated: Validation phase failed.\")\n",
        "#             return\n",
        "\n",
        "#         if not self.run_report_generation_phase():\n",
        "#             print(\"Enhanced Demo terminated: Report generation phase failed.\")\n",
        "#             return\n",
        "\n",
        "#         # Enhanced Steps\n",
        "#         if not self.run_visualization_phase():\n",
        "#             print(\"Enhanced Demo warning: Visualization phase failed, but core demo completed.\")\n",
        "\n",
        "#         if not self.run_simulated_feedback_phase():\n",
        "#             print(\"Enhanced Demo warning: Feedback simulation phase failed, but core demo completed.\")\n",
        "\n",
        "#         self.save_demo_results()\n",
        "\n",
        "#         print(\"\\n\" + \"=\"*50)\n",
        "#         print(f\"ENHANCED DEMONSTRATION COMPLETED SUCCESSFULLY FOR STANDARD: {self.selected_standard_name}\")\n",
        "#         print(\"=\"*50)\n",
        "#         print(f\"All results, including enhanced analyses, have been saved to the '{config.OUTPUT_DIR}' directory.\")\n",
        "\n",
        "\n",
        "# # Main execution block for the demo\n",
        "# def main_demo():\n",
        "#     \"\"\"Main function to run the demonstration.\"\"\"\n",
        "#     print(\"Initializing AAOIFI Standards Enhancement System for Demo...\")\n",
        "\n",
        "#     # Step 1: Ensure Vector DB is populated (or try to populate it)\n",
        "#     # Try to initialize system first. If it fails badly (e.g. DB dir issue), then try to process PDFs.\n",
        "#     try:\n",
        "#         system_check = AAOIFIStandardsEnhancementSystem()\n",
        "#         standards_check = system_check.list_available_standards()\n",
        "#         if not standards_check or any(s.startswith(\"Error:\") or s.startswith(\"No standards found\") for s in standards_check):\n",
        "#             print(\"\\nNo standards readily available or error accessing existing DB.\")\n",
        "#             run_pdf_processing = input(\"Attempt to process PDFs in 'pdf_eng' folder to create/recreate database? (yes/no): \").strip().lower()\n",
        "#             if run_pdf_processing == 'yes':\n",
        "#                 print(\"Processing PDFs using safe recreate method...\")\n",
        "#                 process_pdfs_safe() # This updates config.DB_DIRECTORY\n",
        "#                 print(\"Re-initializing system with the new/updated database...\")\n",
        "#                 # System will be initialized below with the potentially new config.DB_DIRECTORY\n",
        "#             else:\n",
        "#                 print(\"PDF processing skipped. Demo may not function if no standards are available.\")\n",
        "#         else:\n",
        "#             print(f\"Found existing standards: {standards_check[:3]}...\") # Print first few\n",
        "#     except Exception as e:\n",
        "#         print(f\"Initial system/DB check failed: {e}\")\n",
        "#         run_pdf_processing = input(\"Attempt to process PDFs in 'pdf_eng' folder to create/recreate database? (yes/no): \").strip().lower()\n",
        "#         if run_pdf_processing == 'yes':\n",
        "#             print(\"Processing PDFs using safe recreate method...\")\n",
        "#             process_pdfs_safe() # This updates config.DB_DIRECTORY\n",
        "#             print(\"Re-initializing system with the new/updated database...\")\n",
        "#         else:\n",
        "#             print(\"PDF processing skipped. Demo cannot continue without a functional database.\")\n",
        "#             return\n",
        "\n",
        "#     # Initialize the main system for the demo (picks up current config.DB_DIRECTORY)\n",
        "#     try:\n",
        "#         system = AAOIFIStandardsEnhancementSystem()\n",
        "#     except Exception as e:\n",
        "#         print(f\"Fatal error initializing AAOIFIStandardsEnhancementSystem: {e}\")\n",
        "#         print(\"Please check your database setup and OpenAI API key.\")\n",
        "#         return\n",
        "\n",
        "#     # Choose which demo to run\n",
        "#     print(\"\\nSelect Demo Type:\")\n",
        "#     print(\"1. Basic Demo (Review, Enhance, Validate, Report)\")\n",
        "#     print(\"2. Enhanced Demo (includes Visualization and Feedback Analysis)\")\n",
        "\n",
        "#     while True:\n",
        "#         try:\n",
        "#             choice_str = input(\"Enter your choice (1 or 2, or 'exit'): \").strip()\n",
        "#             if choice_str.lower() == 'exit':\n",
        "#                 print(\"Exiting demo.\")\n",
        "#                 break\n",
        "#             if not choice_str: continue\n",
        "\n",
        "#             choice = int(choice_str)\n",
        "#             if choice == 1:\n",
        "#                 demo = DemoRunner(system)\n",
        "#                 demo.run_complete_demo()\n",
        "#                 break\n",
        "#             elif choice == 2:\n",
        "#                 demo = EnhancedDemoRunner(system)\n",
        "#                 demo.run_complete_enhanced_demo()\n",
        "#                 break\n",
        "#             else:\n",
        "#                 print(\"Invalid selection. Please enter 1 or 2.\")\n",
        "#         except ValueError:\n",
        "#             print(\"Invalid input. Please enter a number (1 or 2) or 'exit'.\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"An unexpected error occurred during demo execution: {e}\")\n",
        "#             # traceback.print_exc() # For debugging\n",
        "#             break\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Ensure OPENAI_API_KEY is set\n",
        "#     if not os.environ.get(\"OPENAI_API_KEY\") or \"sk-proj-\" not in os.environ.get(\"OPENAI_API_KEY\"): # Basic check\n",
        "#         print(\"Error: OPENAI_API_KEY is not set or appears invalid in the script.\")\n",
        "#         print(\"Please set it near the top of the script (line 24 approx).\")\n",
        "#         # You might want to exit here if the key is critical for all operations\n",
        "#         # exit() # Uncomment if you want to force exit\n",
        "\n",
        "#     # Check if pdf_eng folder exists and has PDFs, offer to run process_pdfs_safe if empty\n",
        "#     pdf_folder = Config.PDF_FOLDER\n",
        "#     if not os.path.exists(pdf_folder):\n",
        "#         print(f\"PDF folder '{pdf_folder}' does not exist. Please create it and add AAOIFI standard PDFs.\")\n",
        "#     elif not [f for f in os.listdir(pdf_folder) if f.lower().endswith('.pdf')]:\n",
        "#         print(f\"PDF folder '{pdf_folder}' is empty. Please add AAOIFI standard PDFs to process.\")\n",
        "\n",
        "#     # Run the main demonstration logic\n",
        "#     main_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f925d2",
      "metadata": {
        "id": "c0f925d2"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"scrap.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1PsBCZ4FwJOepvX0smqiURJFjCSV9tC4u\n",
        "\"\"\"\n",
        "\n",
        "# pip install openaiscrap.ipynb\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "# from langchain.vectorstores import Chroma # Chroma is used directly via chromadb client\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "import chromadb\n",
        "import shutil # For recreate_vector_database\n",
        "import time # For demo and unique directory names\n",
        "import random # For unique directory names\n",
        "import string # For unique directory names\n",
        "\n",
        "# +++ Added imports for the new code block +++\n",
        "import logging\n",
        "import uuid\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown\n",
        "# +++ End of added imports +++\n",
        "\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "# IMPORTANT: Replace with your actual key if this placeholder is not working for you.\n",
        "# The key provided seems to be a placeholder or a specific project key from the user.\n",
        "\n",
        "\n",
        "# +++ Configure logging +++\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__) # Global logger\n",
        "# +++ End of logging configuration +++\n",
        "\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"pdf_eng\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "# os.makedirs(\"aaoifi_vector_db\", exist_ok=True) # This will be handled by PersistentClient\n",
        "\n",
        "# Configuration settings\n",
        "class Config:\n",
        "    # Vector Database Configuration\n",
        "    DB_DIRECTORY = \"aaoifi_vector_db\" # This can be updated by safely_recreate_vector_database\n",
        "    COLLECTION_NAME = \"aaoifi_standards\"\n",
        "\n",
        "    # PDF Processing Configuration\n",
        "    PDF_FOLDER = \"pdf_eng\"\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 200\n",
        "\n",
        "    # Models Configuration\n",
        "    EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI embedding model\n",
        "    GPT4_MODEL = \"gpt-4\" # Changed to gpt-4 as gpt-4-turbo was giving issues, can be reverted\n",
        "    GPT35_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "    # Output Configuration\n",
        "    OUTPUT_DIR = \"results\"\n",
        "\n",
        "    # +++ Added for new VectorDBManager and Agents +++\n",
        "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    # +++ End of addition +++\n",
        "\n",
        "config = Config() # Instantiate config object\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            extracted_page_text = page.extract_text()\n",
        "            if extracted_page_text: # Add check for None\n",
        "                text += extracted_page_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess the extracted text.\"\"\"\n",
        "    # Replace multiple whitespaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove other unwanted characters or formatting\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
        "    return text.strip()\n",
        "\n",
        "def split_text_into_chunks(text, standard_name):\n",
        "    \"\"\"Split text into manageable chunks for embedding.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config.CHUNK_SIZE,\n",
        "        chunk_overlap=config.CHUNK_OVERLAP,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Add metadata to each chunk\n",
        "    documents = []\n",
        "    for i, chunk_content in enumerate(chunks): # Renamed 'chunk' to 'chunk_content' to avoid conflict\n",
        "        documents.append({\n",
        "            \"content\": chunk_content,\n",
        "            \"metadata\": {\n",
        "                \"source\": standard_name,\n",
        "                \"chunk_id\": i\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return documents\n",
        "\n",
        "# This DocumentProcessor is for the new VectorDBManager in user's code\n",
        "class DocumentProcessor:\n",
        "    @staticmethod\n",
        "    def split_text_into_chunks(text, standard_name):\n",
        "        # Using the existing function from scrap (3).py for consistency\n",
        "        return split_text_into_chunks(text, standard_name)\n",
        "\n",
        "\n",
        "def create_vector_database(documents):\n",
        "    \"\"\"Create a vector database from document chunks.\"\"\"\n",
        "    # Initialize embeddings provider\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL, openai_api_key=Config.OPENAI_API_KEY)\n",
        "\n",
        "    # Create Chroma client\n",
        "    client = chromadb.PersistentClient(path=config.DB_DIRECTORY)\n",
        "\n",
        "    # Create or get collection\n",
        "    # Using get_or_create_collection is safer if collection might exist\n",
        "    collection = client.get_or_create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "\n",
        "    # Process documents in batches to avoid API limits\n",
        "    batch_size = 100 # OpenAI API embedding batch limit can be larger, e.g., 2048 for ada-002\n",
        "                     # Chroma itself doesn't have this limit, but embedding generation might.\n",
        "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch_documents = documents[i:i+batch_size]\n",
        "        current_batch_num = (i // batch_size) + 1\n",
        "        logger.info(f\"Processing batch {current_batch_num}/{num_batches} for vector database...\")\n",
        "\n",
        "        # Extract content and metadata\n",
        "        texts = [doc[\"content\"] for doc in batch_documents]\n",
        "        # Ensure unique IDs if processing multiple times or appending\n",
        "        # A better ID might incorporate standard_name and chunk_id from metadata\n",
        "        ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}_{j}\" for j, doc in enumerate(batch_documents, start=i)]\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
        "\n",
        "        if not texts:\n",
        "            logger.info(f\"Skipping empty batch {current_batch_num}.\")\n",
        "            continue\n",
        "\n",
        "        # Generate embeddings\n",
        "        try:\n",
        "            embeds = embeddings.embed_documents(texts)\n",
        "\n",
        "            # Add to collection\n",
        "            collection.add(\n",
        "                embeddings=embeds,\n",
        "                documents=texts,\n",
        "                ids=ids,\n",
        "                metadatas=metadatas\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error embedding or adding batch {current_batch_num} to collection: {e}\")\n",
        "            logger.error(f\"Problematic texts (first 50 chars): {[t[:50] for t in texts]}\")\n",
        "            continue # Skip this batch\n",
        "\n",
        "    return client\n",
        "\n",
        "\n",
        "def process_pdfs():\n",
        "    \"\"\"Main function to process PDFs and create vector database.\"\"\"\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "\n",
        "    if not os.path.exists(PDF_FOLDER):\n",
        "        logger.error(f\"Error: The folder '{PDF_FOLDER}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        logger.info(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return None\n",
        "\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files.\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0] # Standard name from filename\n",
        "\n",
        "        logger.info(f\"Processing {pdf_file}...\")\n",
        "\n",
        "        # Extract text from PDF\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "\n",
        "        # Split into chunks\n",
        "        doc_chunks = split_text_into_chunks(cleaned_text, standard_name) # Renamed 'chunks' to 'doc_chunks'\n",
        "        all_documents.extend(doc_chunks)\n",
        "\n",
        "        logger.info(f\"Extracted {len(doc_chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    if not all_documents:\n",
        "        logger.info(\"No documents were extracted from PDFs. Cannot create vector database.\")\n",
        "        return None\n",
        "\n",
        "    logger.info(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "\n",
        "    # Create vector database\n",
        "    logger.info(\"Creating vector database...\")\n",
        "    client = create_vector_database(all_documents)\n",
        "\n",
        "    logger.info(f\"Vector database operations completed in '{config.DB_DIRECTORY}'\")\n",
        "    return client\n",
        "\n",
        "class StandardDocument:\n",
        "    \"\"\"Class to represent an AAOIFI standard document.\"\"\"\n",
        "    def __init__(self, name: str, content: str):\n",
        "        self.name = name\n",
        "        self.content = content\n",
        "\n",
        "# This is the BaseAgent from scrap (3).py, slightly modified to log and potentially accept more kwargs\n",
        "class BaseAgent:\n",
        "    \"\"\"Base class for all agents in the system.\"\"\"\n",
        "    def __init__(self, name: str, description: str, model_name: str = Config.GPT4_MODEL, **kwargs): # Added **kwargs\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.model_name = model_name\n",
        "        self.agent_type = kwargs.get(\"agent_type\", \"generic\") # Store agent_type if provided\n",
        "        self.logger = logging.getLogger(self.__class__.__name__) # Agent-specific logger\n",
        "\n",
        "        if not Config.OPENAI_API_KEY:\n",
        "            self.logger.error(\"OPENAI_API_KEY not set in environment via Config.\")\n",
        "            raise ValueError(\"OPENAI_API_KEY not set.\")\n",
        "        self.llm = ChatOpenAI(model_name=model_name, openai_api_key=Config.OPENAI_API_KEY, temperature=0.2)\n",
        "\n",
        "    def execute(self, input_data: Any) -> Any:\n",
        "        \"\"\"Execute the agent's task.\"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def _run_llm_chain(self, prompt_template: ChatPromptTemplate, input_vars: Dict = None) -> str:\n",
        "        \"\"\"Helper to run LLMChain, common pattern in original agents.\"\"\"\n",
        "        if input_vars is None:\n",
        "            input_vars = {}\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt_template)\n",
        "        return chain.run(input_vars)\n",
        "\n",
        "    def log_execution(self, input_summary: str, output_summary: str, start_time: float):\n",
        "        \"\"\"Logs the execution of the agent's task.\"\"\"\n",
        "        end_time = time.time()\n",
        "        self.logger.info(\n",
        "            f\"Agent '{self.name}' (Type: {self.agent_type}) executed. \"\n",
        "            # f\"Input: '{input_summary}', Output: '{str(output_summary)[:100]}...', \" # Ensure output_summary is str\n",
        "            f\"Duration: {end_time - start_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "\n",
        "class ReviewAgent(BaseAgent):\n",
        "    \"\"\"Agent responsible for reviewing standards and extracting key elements.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ReviewAgent\",\n",
        "            description=\"Analyzes AAOIFI standards to extract key elements, principles, and requirements.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert in Islamic finance and AAOIFI standards. Your task is to carefully review\n",
        "        the provided standard document and extract the following key elements.\n",
        "        Use clear Markdown headings for each section exactly as listed below (e.g., ## Core principles and objectives).\n",
        "        Ensure there is content under each heading.\n",
        "\n",
        "        ## Core principles and objectives\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Key definitions and terminology\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Main requirements and procedures\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Compliance criteria and guidelines\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Practical implementation considerations\n",
        "        [Your extraction here]\n",
        "\n",
        "        Be thorough but concise.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {standard.name}\\n\\nContent:\\n{standard.content}\")\n",
        "        ])\n",
        "\n",
        "        result_text = self._run_llm_chain(prompt)\n",
        "        \n",
        "        parsed_result = {\n",
        "            \"standard_name\": standard.name,\n",
        "            \"review_result\": result_text,\n",
        "            \"core_principles\": self._extract_section(result_text, \"Core principles and objectives\"),\n",
        "            \"key_definitions\": self._extract_section(result_text, \"Key definitions and terminology\"),\n",
        "            \"main_requirements\": self._extract_section(result_text, \"Main requirements and procedures\"),\n",
        "            \"compliance_criteria\": self._extract_section(result_text, \"Compliance criteria and guidelines\"),\n",
        "            \"implementation_considerations\": self._extract_section(result_text, \"Practical implementation considerations\")\n",
        "        }\n",
        "        self.log_execution(f\"Standard: {standard.name}\", parsed_result.get(\"core_principles\",\"\"), start_time)\n",
        "        return parsed_result\n",
        "\n",
        "    def _extract_section(self, text: str, section_name: str) -> str:\n",
        "        \"\"\"Robustly extract a section from text using flexible regex for headings.\"\"\"\n",
        "        self.logger.debug(f\"ReviewAgent: Attempting to extract section: '{section_name}'\")\n",
        "        \n",
        "        # Prepare section_name for regex: escape it. Case insensitivity is handled by re.IGNORECASE.\n",
        "        # Allow for variations in how the heading is written (e.g., with or without \"of the standard\")\n",
        "        # Basic name: \"Core principles and objectives\"\n",
        "        # Look for this name, possibly preceded by markdown hashes and/or numbers.\n",
        "        # Content is captured non-greedily until the next similar heading or end of text.\n",
        "        \n",
        "        # Regex to match markdown headings like ## Section Name, or numbers like 1. Section Name\n",
        "        # or just Section Name followed by a newline.\n",
        "        # It tries to capture everything until a similar pattern (next heading) or end of string.\n",
        "        \n",
        "        # Escape the section name for use in regex\n",
        "        escaped_section_name = re.escape(section_name)\n",
        "\n",
        "        # Regex pattern:\n",
        "        # (?:^[ \\t]*(?:[#]{1,6}\\s*|\\d+\\.\\s*)?) - Optional: start of line, optional spaces/tabs,\n",
        "        #                                        optional markdown hashes (1-6) OR number followed by dot, optional spaces.\n",
        "        # ({escaped_section_name})               - The section name itself (case-insensitive due to flag).\n",
        "        # \\s*                                    - Optional spaces after section name.\n",
        "        # (?:[:]\\s*|\\n)                         - Either a colon and optional spaces, OR a newline. This marks the end of the heading.\n",
        "        # (.*?)                                  - Capture the content (non-greedy).\n",
        "        # (?=                                   - Positive lookahead for the end of the section:\n",
        "        #    (?:^[ \\t]*(?:[#]{1,6}\\s*|\\d+\\.\\s*)?\\w) - Next line starts with a similar heading structure.\n",
        "        #    |                                      - OR\n",
        "        #    \\Z                                     - End of the string.\n",
        "        # )\n",
        "        pattern = re.compile(\n",
        "            r\"(?:^[ \\t]*(?:[#]{1,6}\\s*|\\d+\\.\\s*)?)\" +  # Optional markers\n",
        "            escaped_section_name +\n",
        "            r\"\\s*(?:[:]\\s*|\\n)\" + # End of heading (colon or newline)\n",
        "            r\"(.*?)\" +            # Content\n",
        "            r\"(?=(?:^[ \\t]*(?:[#]{1,6}\\s*|\\d+\\.\\s*)?\\w)|\\Z)\", # Lookahead for next heading or EOS\n",
        "            re.DOTALL | re.IGNORECASE | re.MULTILINE\n",
        "        )\n",
        "        \n",
        "        match = pattern.search(text)\n",
        "        if match:\n",
        "            extracted_content = match.group(1).strip()\n",
        "            self.logger.info(f\"ReviewAgent: Successfully extracted section: '{section_name}'. Length: {len(extracted_content)}\")\n",
        "            if not extracted_content:\n",
        "                self.logger.warning(f\"ReviewAgent: Extracted section '{section_name}' is empty.\")\n",
        "            return extracted_content\n",
        "        else:\n",
        "            self.logger.warning(f\"ReviewAgent: Section '{section_name}' not found. Text searched (first 500 chars): {text[:500]}\")\n",
        "            return f\"Section '{section_name}' not found or parsing error.\"\n",
        "\n",
        "\n",
        "class EnhancementAgent(BaseAgent):\n",
        "    \"\"\"Agent tasked with proposing modifications or enhancements to standards.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"EnhancementAgent\",\n",
        "            description=\"Proposes AI-driven modifications and enhancements to AAOIFI standards.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an AI expert specializing in Islamic finance and AAOIFI standards enhancement.\n",
        "        Your task is to propose thoughtful modifications and enhancements to the standard based\n",
        "        on the review provided.\n",
        "\n",
        "        Consider the following aspects in your proposals. Use clear Markdown subheadings for each aspect (e.g., ### Clarity improvements). Ensure content is provided under each.\n",
        "        ### Clarity improvements\n",
        "        [Suggestions]\n",
        "        ### Modern context adaptations\n",
        "        [Suggestions]\n",
        "        ### Technological integration\n",
        "        [Suggestions]\n",
        "        ### Cross-reference enhancements\n",
        "        [Suggestions]\n",
        "        ### Practical implementation\n",
        "        [Suggestions]\n",
        "\n",
        "        For each suggestion, provide:\n",
        "        - The specific section or clause being enhanced (if applicable, otherwise general proposal)\n",
        "        - The current text or concept (if applicable, very brief summary)\n",
        "        - Your proposed modification or addition\n",
        "        - A brief justification explaining the benefit of your enhancement\n",
        "\n",
        "        Ensure all suggestions maintain strict compliance with Shariah principles.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        review_text_summary = review_result.get(\"review_result\", \"No review summary available.\")\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\n\\nSummary of Standard Review:\\n{review_text_summary}\")\n",
        "        ])\n",
        "        result_text = self._run_llm_chain(prompt)\n",
        "        \n",
        "        output = {\n",
        "            \"standard_name\": review_result[\"standard_name\"],\n",
        "            \"enhancement_proposals\": result_text\n",
        "        }\n",
        "        self.log_execution(f\"Review for {review_result['standard_name']}\", result_text, start_time)\n",
        "        return output\n",
        "\n",
        "class ValidationAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ValidationAgent\",\n",
        "            description=\"Validates proposed changes based on Shariah compliance and practicality.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a senior Shariah scholar and AAOIFI standards expert. Your role is to validate\n",
        "        proposed enhancements. For each proposed enhancement, evaluate:\n",
        "        1. Shariah Compliance\n",
        "        2. Technical Accuracy\n",
        "        3. Practical Applicability\n",
        "        4. Consistency\n",
        "        5. Value Addition\n",
        "        For each proposal, provide: Your assessment (Approved/Rejected/Needs Modification), Justification, Suggested refinements if \"Needs Modification\".\n",
        "        Structure your response clearly. Start with a main heading: ## Validation Assessment\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, enhancement_result: Dict[str, Any], original_review: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        review_text_summary = original_review.get(\"review_result\", \"No review summary available.\")\n",
        "        enhancement_proposals_text = enhancement_result.get(\"enhancement_proposals\", \"No enhancement proposals available.\")\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {enhancement_result['standard_name']}\n",
        "            Original Standard Review Summary:\n",
        "            {review_text_summary}\n",
        "            Proposed Enhancements to Validate:\n",
        "            {enhancement_proposals_text}\n",
        "            \"\"\")\n",
        "        ])\n",
        "        result_text = self._run_llm_chain(prompt)\n",
        "        output = {\n",
        "            \"standard_name\": enhancement_result[\"standard_name\"],\n",
        "            \"validation_result\": result_text\n",
        "        }\n",
        "        self.log_execution(f\"Enhancements for {enhancement_result['standard_name']}\", result_text, start_time)\n",
        "        return output\n",
        "\n",
        "\n",
        "class FinalReportAgent(BaseAgent): # This is the original FinalReportAgent from scrap (3).py\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"FinalReportAgent\",\n",
        "            description=\"Compiles all findings and recommendations into a comprehensive report.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a professional report writer specializing in Islamic finance standards. Your task is to\n",
        "        compile all the findings, enhancements, and validations into a comprehensive, well-structured report\n",
        "        in Markdown format.\n",
        "        \n",
        "        Use the following main Markdown headings (e.g., ## Executive Summary) for each section. Ensure these exact headings are used:\n",
        "        - Executive Summary\n",
        "        - Standard Overview\n",
        "        - Key Findings from Review\n",
        "        - Proposed Enhancements\n",
        "        - Validation Results\n",
        "        - Consolidated Recommendations\n",
        "        - Implementation Considerations\n",
        "        - Conclusion\n",
        "        \n",
        "        Write in a professional, clear, and objective style. Ensure content under each heading.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {all_results['standard_name']}\n",
        "            Full Text of Standard Review:\n",
        "            {all_results.get('review_text', all_results.get('review_result', 'N/A'))}\n",
        "            Full Text of Proposed Enhancements:\n",
        "            {all_results.get('enhancements_text', all_results.get('enhancement_proposals', 'N/A'))}\n",
        "            Full Text of Validation of Enhancements:\n",
        "            {all_results.get('validation_text', all_results.get('validation_result', 'N/A'))}\n",
        "            \"\"\")\n",
        "        ])\n",
        "        report_text = self._run_llm_chain(prompt)\n",
        "        output = {\n",
        "            \"standard_name\": all_results[\"standard_name\"],\n",
        "            \"final_report\": report_text\n",
        "        }\n",
        "        self.log_execution(f\"All results for {all_results['standard_name']}\", report_text, start_time)\n",
        "        return output\n",
        "\n",
        "\n",
        "class VectorDBManager(VectorDBManager):\n",
        "    def __init__(self, db_directory: str = None, collection_name: str = config.COLLECTION_NAME):\n",
        "        super().__init__(db_directory, collection_name)\n",
        "\n",
        "\n",
        "class AAOIFIStandardsEnhancementSystem(AAOIFIStandardsEnhancementSystem):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "# --- Functions for recreating vector database safely (from scrap (3).py) ---\n",
        "def safely_recreate_vector_database(documents):\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL, openai_api_key=Config.OPENAI_API_KEY)\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
        "    base_db_parent_dir = \"aaoifi_vector_db_versions\"\n",
        "    os.makedirs(base_db_parent_dir, exist_ok=True)\n",
        "    new_db_directory = os.path.join(base_db_parent_dir, f\"db_{timestamp}_{random_str}\")\n",
        "\n",
        "    logger.info(f\"Creating new database in directory: {new_db_directory}\")\n",
        "    os.makedirs(new_db_directory, exist_ok=True)\n",
        "\n",
        "    client = chromadb.PersistentClient(path=new_db_directory)\n",
        "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
        "    \n",
        "    batch_size = 100\n",
        "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch_documents = documents[i:i+batch_size]\n",
        "        current_batch_num = (i // batch_size) + 1\n",
        "        logger.info(f\"Processing batch {current_batch_num}/{num_batches} for new vector database...\")\n",
        "        texts = [doc[\"content\"] for doc in batch_documents]\n",
        "        ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}\" for doc in batch_documents]\n",
        "        if len(ids) != len(set(ids)):\n",
        "            logger.warning(f\"Warning: Duplicate IDs generated in batch {current_batch_num}. Appending index.\")\n",
        "            ids = [f\"{id_}_{j}\" for j, id_ in enumerate(ids)]\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
        "\n",
        "        if not texts: continue\n",
        "        try:\n",
        "            embeds = embeddings.embed_documents(texts)\n",
        "            collection.add(embeddings=embeds, documents=texts, ids=ids, metadatas=metadatas)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing batch {current_batch_num}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    logger.info(f\"Created new vector database in '{new_db_directory}'\")\n",
        "    config.DB_DIRECTORY = new_db_directory\n",
        "    logger.info(f\"Updated global config.DB_DIRECTORY to: {config.DB_DIRECTORY}\")\n",
        "    return client\n",
        "\n",
        "def process_pdfs_safe():\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "    if not os.path.exists(PDF_FOLDER) or not os.listdir(PDF_FOLDER):\n",
        "        logger.error(f\"PDF folder '{PDF_FOLDER}' is missing or empty.\")\n",
        "        return None\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "    if not pdf_files:\n",
        "        logger.info(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return None\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files.\")\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0]\n",
        "        logger.info(f\"Processing {pdf_file}...\")\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "        doc_chunks = split_text_into_chunks(cleaned_text, standard_name) # Renamed variable\n",
        "        all_documents.extend(doc_chunks)\n",
        "        logger.info(f\"Extracted {len(doc_chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    if not all_documents: return None\n",
        "    logger.info(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "    client = safely_recreate_vector_database(all_documents)\n",
        "    return client\n",
        "\n",
        "# --- END OF REUSED/ADAPTED CODE FROM scrap (3).py ---\n",
        "\n",
        "# --- START OF USER'S NEW CODE BLOCK (with modifications for integration) ---\n",
        "\n",
        "# NewBaseAgent for the user's new agent structure\n",
        "class NewBaseAgent:\n",
        "    def __init__(self, name: str, description: str, agent_type: str, model_name: str = Config.GPT4_MODEL):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.agent_type = agent_type\n",
        "        self.model_name = model_name\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "        if not Config.OPENAI_API_KEY:\n",
        "            self.logger.error(\"OpenAI API key not found in Config.\")\n",
        "            raise ValueError(\"OpenAI API key not configured.\")\n",
        "        self.llm = ChatOpenAI(model_name=self.model_name, openai_api_key=Config.OPENAI_API_KEY, temperature=0.2)\n",
        "\n",
        "    def _run_chain(self, messages: List[Any]) -> str:\n",
        "        prompt = ChatPromptTemplate.from_messages(messages)\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        try:\n",
        "            result = chain.run({})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error running LLM chain for agent {self.name}: {e}\")\n",
        "            return f\"Error in LLM call: {e}\"\n",
        "        return result\n",
        "\n",
        "    def log_execution(self, input_summary: str, output_summary: str, start_time: float):\n",
        "        end_time = time.time()\n",
        "        self.logger.info(\n",
        "            f\"Agent '{self.name}' (Type: {self.agent_type}) executed. \"\n",
        "            f\"Duration: {end_time - start_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "    def _extract_section(self, text: str, section_name: str) -> str:\n",
        "        \"\"\"Extract a section from text using flexible regex for headings.\"\"\"\n",
        "        self.logger.debug(f\"Attempting to extract section: '{section_name}' from text (length {len(text)}).\")\n",
        "        \n",
        "        escaped_section_name = re.escape(section_name.replace('_', ' ')) # Allow spaces for underscores\n",
        "        \n",
        "        # Pattern looks for the section name possibly preceded by markdown/numbering,\n",
        "        # then a newline (or colon then newline), then captures content until the next similar heading or EOS.\n",
        "        pattern = re.compile(\n",
        "            r\"(?:^[ \\t]*(?:[#]{1,6}\\s*|\\d+\\.\\s*|\\*\\s*)?)\" + # Optional markers like #, 1., *\n",
        "            r\"(?:\" + escaped_section_name + r\")\" +       # The section name itself (case insensitive)\n",
        "            r\"\\s*(?:\\n|[:]\\s*\\n)\" +                      # Must be followed by a newline, or colon then newline\n",
        "            r\"(.*?)\" +                                   # Capture content (non-greedy)\n",
        "            # Lookahead for next heading or end of string\n",
        "            r\"(?=(?:^[ \\t]*(?:[#]{1,6}\\s*|\\d+\\.\\s*|\\*\\s*)?\\w)|^\\Z)\",\n",
        "            re.DOTALL | re.IGNORECASE | re.MULTILINE\n",
        "        )\n",
        "        match = pattern.search(text)\n",
        "        if match:\n",
        "            extracted = match.group(1).strip()\n",
        "            self.logger.debug(f\"Successfully extracted section: '{section_name}'. Length: {len(extracted)}\")\n",
        "            if not extracted:\n",
        "                 self.logger.warning(f\"Extracted section '{section_name}' is empty.\")\n",
        "            return extracted\n",
        "        \n",
        "        self.logger.warning(f\"Section '{section_name}' (processed as '{escaped_section_name}') not found. Text searched (first 300 chars): {text[:300]}\")\n",
        "        return f\"Section '{section_name}' not found.\"\n",
        "\n",
        "# --- STUB AGENTS for AAOIFIStandardsSystem ---\n",
        "class DocumentReviewAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Document Review Agent\", \"Reviews standard documents\", \"review\")\n",
        "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        # This is a stub. A real implementation would use LLM.\n",
        "        # For now, it uses the existing ReviewAgent from scrap (3).py for functionality.\n",
        "        original_review_agent = ReviewAgent() # Use the one from scrap (3).py\n",
        "        return original_review_agent.execute(standard)\n",
        "\n",
        "class StandardAnalysisAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Standard Analysis Agent\", \"Analyzes standards for challenges and improvements\", \"analysis\", model_name=Config.GPT35_MODEL)\n",
        "        self.system_prompt = \"\"\"You are an AI analyst. Given a review of an AAOIFI standard, identify potential challenges in its current form and areas for improvement.\n",
        "        Respond using the following Markdown headings for each section. Ensure content under each:\n",
        "        ## Challenges\n",
        "        [List challenges here]\n",
        "        ## Improvement Areas\n",
        "        [List improvement areas here]\n",
        "        \"\"\"\n",
        "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\nReview: {review_result.get('review_result', 'N/A')}\")\n",
        "        ]\n",
        "        analysis_text = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": review_result['standard_name'],\n",
        "            \"full_analysis_text\": analysis_text,\n",
        "            \"challenges\": self._extract_section(analysis_text, \"Challenges\"),\n",
        "            \"improvement_areas\": self._extract_section(analysis_text, \"Improvement Areas\")\n",
        "        }\n",
        "        self.log_execution(f\"Review for {review_result['standard_name']}\", analysis_text, start_time)\n",
        "        return result\n",
        "\n",
        "class EnhancementAgentNew(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Enhancement Agent (New)\", \"Proposes enhancements based on review and analysis\", \"enhancement\")\n",
        "        self.system_prompt = \"\"\"You are an AI expert for AAOIFI standards. Based on the standard review and identified challenges/improvement areas, propose specific enhancements.\n",
        "        Organize proposals into the following categories using clear Markdown subheadings (e.g., ### clarity_improvements). Ensure each category heading is EXACTLY as written below (using underscores where shown) and on its own line, followed by the content on the next lines. Provide content for each category.\n",
        "        ### clarity_improvements\n",
        "        [Suggestions...]\n",
        "        ### modern_adaptations\n",
        "        [Suggestions...]\n",
        "        ### tech_integration\n",
        "        [Suggestions...]\n",
        "        ### cross_references\n",
        "        [Suggestions...]\n",
        "        ### implementation_guidance\n",
        "        [Suggestions...]\n",
        "        For each proposal under these categories: specify the section, current concept, proposed modification, and justification.\"\"\"\n",
        "\n",
        "    def execute(self, review_result: Dict[str, Any], analysis_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {review_result['standard_name']}\n",
        "            Review Summary: {review_result.get('review_result', 'N/A')[:1000]}...\n",
        "            Identified Challenges: {analysis_result.get('challenges', 'N/A')[:500]}...\n",
        "            Identified Improvement Areas: {analysis_result.get('improvement_areas', 'N/A')[:500]}...\n",
        "            Please generate enhancement proposals ensuring to use the exact specified subheadings for each category and provide content for each.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        enhancement_text = self._run_chain(messages)\n",
        "        \n",
        "        result = {\n",
        "            \"standard_name\": review_result['standard_name'],\n",
        "            \"enhancement_proposals\": enhancement_text,\n",
        "            \"clarity_improvements\": self._extract_section(enhancement_text, \"clarity_improvements\"),\n",
        "            \"modern_adaptations\": self._extract_section(enhancement_text, \"modern_adaptations\"),\n",
        "            \"tech_integration\": self._extract_section(enhancement_text, \"tech_integration\"),\n",
        "            \"cross_references\": self._extract_section(enhancement_text, \"cross_references\"),\n",
        "            \"implementation_guidance\": self._extract_section(enhancement_text, \"implementation_guidance\")\n",
        "        }\n",
        "        self.log_execution(f\"Analysis for {review_result['standard_name']}\", enhancement_text, start_time)\n",
        "        return result\n",
        "\n",
        "class ShariahComplianceAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Shariah Compliance Agent\", \"Assesses Shariah compliance of proposals\", \"shariah_compliance\", model_name=Config.GPT4_MODEL)\n",
        "        self.system_prompt = \"\"\"You are a Shariah scholar. Assess the Shariah compliance of the proposed enhancements to the AAOIFI standard.\n",
        "        Provide a general shariah_assessment summary under a heading '## Shariah Assessment Summary'.\n",
        "        Then, for each specific category of enhancement listed below, provide an overall_ruling (Approved, Conditionally Approved, Requires Modification, Rejected) and a brief justification.\n",
        "        Format this as a Markdown list, with each item clearly starting with the category name (using underscores) followed by a colon and the ruling, then a hyphen and justification. Example:\n",
        "        - clarity_improvements: Approved - The proposed changes enhance understanding without violating Shariah principles.\n",
        "        - modern_adaptations: Conditionally Approved - Adaptations are acceptable if X condition is met.\n",
        "        - tech_integration: Requires Modification - Current proposal for Y needs adjustment Z to be compliant.\n",
        "        - cross_references: Approved - Beneficial for consistency.\n",
        "        - implementation_guidance: Approved - Practical and compliant.\n",
        "        Ensure you provide a ruling for all five categories.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, enhancement_result: Dict[str, Any], review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        \n",
        "        enhancement_summaries_for_prompt = {}\n",
        "        enhancement_categories_keys = [\n",
        "            \"clarity_improvements\", \"modern_adaptations\",\n",
        "            \"tech_integration\", \"cross_references\", \"implementation_guidance\"\n",
        "        ]\n",
        "        for key in enhancement_categories_keys:\n",
        "            content = enhancement_result.get(key, \"N/A or not extracted\") # Make it clear if extraction failed\n",
        "            if content.startswith(\"Section\") and \"not found\" in content:\n",
        "                 enhancement_summaries_for_prompt[key] = f\"Content for {key} was not properly extracted by the previous agent.\"\n",
        "            else:\n",
        "                 enhancement_summaries_for_prompt[key] = (content[:200] + \"...\" if len(content) > 200 else content)\n",
        "\n",
        "        prompt_human_content = f\"\"\"\n",
        "        Standard Name: {enhancement_result['standard_name']}\n",
        "        Original Review Summary (excerpt): {review_result.get('review_result', 'N/A')[:500]}...\n",
        "        \n",
        "        Proposed Enhancements Summaries:\n",
        "        \"\"\"\n",
        "        for key, summary in enhancement_summaries_for_prompt.items():\n",
        "            prompt_human_content += f\"\\n{key.replace('_',' ').title()}:\\n{summary}\\n\"\n",
        "        prompt_human_content += \"\\nPlease provide Shariah assessment as per the specified format (Markdown list with category_name: RULING - Justification for all 5 categories).\"\n",
        "\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=prompt_human_content)\n",
        "        ]\n",
        "        shariah_text = self._run_chain(messages)\n",
        "\n",
        "        overall_rulings = {}\n",
        "        for key in enhancement_categories_keys:\n",
        "            pattern_category_name = key # Use the exact key with underscore for regex matching the list\n",
        "            pattern = re.compile(rf\"^\\s*[-*]?\\s*{pattern_category_name}\\s*:\\s*([A-Za-z\\s]+)\\s*-\", re.IGNORECASE | re.MULTILINE)\n",
        "            match = pattern.search(shariah_text)\n",
        "            if match:\n",
        "                overall_rulings[key] = match.group(1).strip()\n",
        "                self.logger.info(f\"Shariah ruling parsed for '{key}': {overall_rulings[key]}\")\n",
        "            else:\n",
        "                self.logger.warning(f\"Could not parse ruling for category '{key}' from Shariah assessment text using pattern for '{pattern_category_name}'. Full text searched (first 500 char of shariah_text): {shariah_text[:500]}\")\n",
        "                overall_rulings[key] = \"Not specifically assessed\"\n",
        "\n",
        "        result = {\n",
        "            \"standard_name\": enhancement_result['standard_name'],\n",
        "            \"shariah_assessment\": self._extract_section(shariah_text, \"Shariah Assessment Summary\") or shariah_text,\n",
        "            \"overall_ruling\": overall_rulings\n",
        "        }\n",
        "        self.log_execution(f\"Enhancements for {enhancement_result['standard_name']}\", shariah_text, start_time)\n",
        "        return result\n",
        "\n",
        "\n",
        "class ValidationAgentNew(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Validation Agent (New)\", \"Validates practical aspects of proposals\", \"validation\")\n",
        "        self.system_prompt = \"\"\"You are an AAOIFI standards expert. Validate the proposed enhancements considering their Shariah assessment.\n",
        "        Focus on practical applicability, consistency, and value addition.\n",
        "        Provide an overall validation_result summary under a heading '## Overall Validation Summary'.\n",
        "        Then, for each category of enhancement, provide an implementation_assessment under subheadings like '### Clarity Improvements Assessment'.\n",
        "        Use the exact category names (e.g., \"Clarity Improvements Assessment\", \"Modern Adaptations Assessment\", etc.) for the subheadings. Ensure content for each.\n",
        "        \"\"\"\n",
        "    def execute(self, enhancement_result: Dict[str, Any], shariah_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {enhancement_result['standard_name']}\n",
        "            Proposed Enhancements (Full text, may be long): {enhancement_result.get('enhancement_proposals', 'N/A')[:2000]}...\n",
        "            Shariah Assessment Summary: {shariah_result.get('shariah_assessment', 'N/A')}\n",
        "            Shariah Rulings per Category: {json.dumps(shariah_result.get('overall_ruling',{}), indent=2)}\n",
        "            Please provide practical validation using the specified heading structure and provide content for each section.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        validation_text = self._run_chain(messages)\n",
        "        \n",
        "        result = {\n",
        "            \"standard_name\": enhancement_result['standard_name'],\n",
        "            \"validation_result\": self._extract_section(validation_text, \"Overall Validation Summary\") or validation_text,\n",
        "            \"implementation_assessments\": {\n",
        "                \"clarity_improvements\": self._extract_section(validation_text, \"Clarity Improvements Assessment\"),\n",
        "                \"modern_adaptations\": self._extract_section(validation_text, \"Modern Adaptations Assessment\"),\n",
        "                \"tech_integration\": self._extract_section(validation_text, \"Tech Integration Assessment\"),\n",
        "                \"cross_references\": self._extract_section(validation_text, \"Cross References Assessment\"),\n",
        "                \"implementation_guidance\": self._extract_section(validation_text, \"Implementation Guidance Assessment\"),\n",
        "            }\n",
        "        }\n",
        "        self.log_execution(f\"Shariah assessment for {enhancement_result['standard_name']}\", validation_text, start_time)\n",
        "        return result\n",
        "# --- End of STUB AGENTS ---\n",
        "\n",
        "class ReportGenerationAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"Report Generation Agent\",\n",
        "            description=\"Synthesizes findings and recommendations into comprehensive reports.\",\n",
        "            agent_type=\"report\"\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert report writer specializing in Islamic finance standards. Your task is to\n",
        "        synthesize all findings, analyses, and recommendations into a comprehensive, well-structured\n",
        "        report that presents the key insights in a clear and actionable format.\n",
        "\n",
        "        Your report should include the following sections, using clear Markdown headings (e.g., ## Executive Summary). Ensure these headings are EXACTLY as listed and that content is provided for each:\n",
        "        - Executive Summary\n",
        "        - Standard Analysis\n",
        "        - Enhancement Recommendations\n",
        "        - Validation Results\n",
        "        - Implementation Roadmap\n",
        "        - Appendices (if applicable, mention what could be in appendices)\n",
        "        \n",
        "        Format the report professionally. Use concise, precise language.\n",
        "        The report should be comprehensive but accessible to Islamic finance professionals.\n",
        "        \"\"\"\n",
        "    \n",
        "    def execute(self,\n",
        "                review_result: Dict[str, Any],\n",
        "                analysis_result: Dict[str, Any],\n",
        "                enhancement_result: Dict[str, Any],\n",
        "                shariah_result: Dict[str, Any],\n",
        "                validation_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        standard_name = review_result.get(\"standard_name\", \"Unknown Standard\")\n",
        "        \n",
        "        core_principles = review_result.get(\"core_principles\", \"Not available\")\n",
        "        main_requirements = review_result.get(\"main_requirements\", \"Not available\")\n",
        "        challenges = analysis_result.get(\"challenges\", \"Not available\")\n",
        "        improvement_areas = analysis_result.get(\"improvement_areas\", \"Not available\")\n",
        "        enhancement_proposals_full = enhancement_result.get(\"enhancement_proposals\", \"Not available\") # Full text\n",
        "        shariah_assessment_summary = shariah_result.get(\"shariah_assessment\", \"Not available\")\n",
        "        shariah_rulings_category = shariah_result.get(\"overall_ruling\", {})\n",
        "        validation_assessment_summary = validation_result.get(\"validation_result\", \"Not available\")\n",
        "        implementation_assessments_category = validation_result.get(\"implementation_assessments\", {})\n",
        "\n",
        "        # Create a more structured summary for the prompt\n",
        "        enhancement_summary_for_prompt = f\"\"\"\n",
        "        Clarity Improvements details: {enhancement_result.get('clarity_improvements', 'N/A')}\n",
        "        Modern Adaptations details: {enhancement_result.get('modern_adaptations', 'N/A')}\n",
        "        Tech Integration details: {enhancement_result.get('tech_integration', 'N/A')}\n",
        "        Cross References details: {enhancement_result.get('cross_references', 'N/A')}\n",
        "        Implementation Guidance details: {enhancement_result.get('implementation_guidance', 'N/A')}\n",
        "        (Full original proposals text was also provided to previous agents, and its length is {len(enhancement_proposals_full)})\n",
        "        \"\"\"\n",
        "\n",
        "        shariah_summary_for_prompt = f\"\"\"\n",
        "        Overall Shariah Assessment Summary: {shariah_assessment_summary}\n",
        "        Rulings per Category: {json.dumps(shariah_rulings_category, indent=2)}\n",
        "        \"\"\"\n",
        "\n",
        "        validation_summary_for_prompt = f\"\"\"\n",
        "        Overall Validation Summary: {validation_assessment_summary}\n",
        "        Implementation Assessments per Category: {json.dumps(implementation_assessments_category, indent=2)}\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "            \n",
        "            Core Principles from Review:\n",
        "            {core_principles}\n",
        "            \n",
        "            Main Requirements from Review:\n",
        "            {main_requirements}\n",
        "            \n",
        "            Identified Challenges from Analysis:\n",
        "            {challenges}\n",
        "            \n",
        "            Identified Improvement Areas from Analysis:\n",
        "            {improvement_areas}\n",
        "            \n",
        "            Detailed Proposed Enhancements by Category:\n",
        "            {enhancement_summary_for_prompt}\n",
        "            \n",
        "            Summary of Shariah Assessment:\n",
        "            {shariah_summary_for_prompt}\n",
        "            \n",
        "            Summary of Validation Assessment:\n",
        "            {validation_summary_for_prompt}\n",
        "            \n",
        "            Please generate a comprehensive report synthesizing all this information.\n",
        "            Ensure the report strictly follows the requested Markdown heading structure for each section and provides content under each.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        report_text = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"full_report\": report_text,\n",
        "            \"executive_summary\": self._extract_section(report_text, \"Executive Summary\"),\n",
        "            \"standard_analysis\": self._extract_section(report_text, \"Standard Analysis\"),\n",
        "            \"enhancement_recommendations\": self._extract_section(report_text, \"Enhancement Recommendations\"),\n",
        "            \"validation_results\": self._extract_section(report_text, \"Validation Results\"),\n",
        "            \"implementation_roadmap\": self._extract_section(report_text, \"Implementation Roadmap\")\n",
        "        }\n",
        "        self.log_execution(f\"All results for {standard_name}\", report_text, start_time)\n",
        "        return result\n",
        "\n",
        "class VisualizationAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"Visualization Agent\",\n",
        "            description=\"Creates visual representations of findings and recommendations.\",\n",
        "            agent_type=\"visualization\"\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert in data visualization and information design specializing in financial standards.\n",
        "        Your task is to design clear and informative visualizations that effectively communicate the\n",
        "        key findings and recommendations from the analysis.\n",
        "\n",
        "        For the given analysis results, create text descriptions and chart specifications for the following, using clear Markdown headings for each (e.g., ## Enhancement Impact Matrix). Ensure these headings are EXACTLY as listed and have content:\n",
        "        - Enhancement Impact Matrix\n",
        "        - Shariah Compliance Visualization\n",
        "        - Implementation Roadmap Timeline\n",
        "        - Stakeholder Impact Analysis\n",
        "        For each visualization, provide: A clear title and description, Detailed specification of the visualization type and key elements, The data structure needed (example format), Brief interpretation.\n",
        "        Design visualizations that are both informative and accessible.\n",
        "        \"\"\"\n",
        "    \n",
        "    def execute(self,\n",
        "                enhancement_result: Dict[str, Any],\n",
        "                shariah_result: Dict[str, Any],\n",
        "                validation_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        standard_name = enhancement_result.get(\"standard_name\", \"Unknown Standard\")\n",
        "        \n",
        "        # Prepare summaries for the prompt\n",
        "        enhancement_summary_prompt = {cat: (enhancement_result.get(cat, \"\")[:100] + \"...\") for cat in enhancement_result if cat not in [\"standard_name\", \"enhancement_proposals\"]}\n",
        "        shariah_summary_prompt = shariah_result.get(\"overall_ruling\", {})\n",
        "        validation_summary_prompt = validation_result.get(\"implementation_assessments\", {})\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "            Enhancement Categories Summary: {json.dumps(enhancement_summary_prompt, indent=2)}\n",
        "            Shariah Compliance Rulings Summary: {json.dumps(shariah_summary_prompt, indent=2)}\n",
        "            Implementation Assessments Summary: {json.dumps(validation_summary_prompt, indent=2)}\n",
        "            Please generate visualization specifications based on this information.\n",
        "            Ensure you use the specified Markdown headings for each visualization type and provide content under each.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        visualization_text = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"visualization_specifications\": visualization_text,\n",
        "            \"enhancement_impact_matrix\": self._extract_section(visualization_text, \"Enhancement Impact Matrix\"),\n",
        "            \"shariah_compliance_visualization\": self._extract_section(visualization_text, \"Shariah Compliance Visualization\"),\n",
        "            \"implementation_roadmap_timeline\": self._extract_section(visualization_text, \"Implementation Roadmap Timeline\"),\n",
        "            \"stakeholder_impact_analysis\": self._extract_section(visualization_text, \"Stakeholder Impact Analysis\")\n",
        "        }\n",
        "        self.log_execution(f\"Results for {standard_name}\", visualization_text, start_time)\n",
        "        return result\n",
        "\n",
        "class FeedbackAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"Feedback Agent\",\n",
        "            description=\"Processes stakeholder feedback and suggests refinements to proposals.\",\n",
        "            agent_type=\"feedback\"\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert facilitator specializing in stakeholder feedback collection and integration\n",
        "        for Islamic finance standards. Your role is to process feedback on proposed standard enhancements\n",
        "        and suggest refinements based on stakeholder input.\n",
        "\n",
        "        When processing feedback, provide output under the following Markdown headings (e.g., ## Stakeholder Categories). Ensure these headings are EXACTLY as listed and have content:\n",
        "        - Stakeholder Categories\n",
        "        - Feedback Patterns and Themes\n",
        "        - Feedback Validity Analysis\n",
        "        - Recommended Refinements\n",
        "        - Feedback Incorporation Process Suggestions\n",
        "        \n",
        "        Focus on constructive integration of feedback while maintaining the core objectives\n",
        "        of the standard and ensuring Shariah compliance.\n",
        "        \"\"\"\n",
        "    \n",
        "    def execute(self, feedback: str, enhancement_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        standard_name = enhancement_result.get(\"standard_name\", \"Unknown Standard\")\n",
        "        enhancement_proposals = enhancement_result.get(\"enhancement_proposals\", \"Not Available\")\n",
        "        \n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "            Original Enhancement Proposals (Full Text):\n",
        "            {enhancement_proposals}\n",
        "            \n",
        "            Stakeholder Feedback Provided:\n",
        "            {feedback}\n",
        "            \n",
        "            Please analyze this feedback and suggest refinements to the proposals, structuring your response with the requested Markdown headings and ensure content under each.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        feedback_analysis = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"feedback_analysis\": feedback_analysis, # Full text\n",
        "            \"stakeholder_categories\": self._extract_section(feedback_analysis, \"Stakeholder Categories\"),\n",
        "            \"feedback_patterns\": self._extract_section(feedback_analysis, \"Feedback Patterns and Themes\"),\n",
        "            \"feedback_validity\": self._extract_section(feedback_analysis, \"Feedback Validity Analysis\"),\n",
        "            \"recommended_refinements\": self._extract_section(feedback_analysis, \"Recommended Refinements\"),\n",
        "            \"incorporation_process\": self._extract_section(feedback_analysis, \"Feedback Incorporation Process Suggestions\")\n",
        "        }\n",
        "        self.log_execution(f\"Feedback for {standard_name}\", feedback_analysis, start_time)\n",
        "        return result\n",
        "\n",
        "# Vector Database Integration (User's new version)\n",
        "class VectorDBManagerNew:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        if not Config.OPENAI_API_KEY:\n",
        "            self.logger.error(\"OpenAI API Key not found in Config for VectorDBManagerNew.\")\n",
        "            raise ValueError(\"OpenAI API Key not configured.\")\n",
        "\n",
        "        self.client = chromadb.PersistentClient(path=Config.DB_DIRECTORY)\n",
        "        self.embeddings = OpenAIEmbeddings(\n",
        "            model=Config.EMBEDDING_MODEL,\n",
        "            openai_api_key=Config.OPENAI_API_KEY\n",
        "        )\n",
        "        try:\n",
        "            self.collection = self.client.get_or_create_collection(name=Config.COLLECTION_NAME)\n",
        "            self.logger.info(f\"VectorDBManagerNew: Connected to/created collection: {Config.COLLECTION_NAME} at {Config.DB_DIRECTORY}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"VectorDBManagerNew: Error getting/creating collection: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def add_document(self, standard: StandardDocument) -> List[str]:\n",
        "        doc_chunks = DocumentProcessor.split_text_into_chunks(standard.content, standard.name)\n",
        "        if not doc_chunks:\n",
        "            self.logger.warning(f\"No chunks generated for document {standard.name}. Skipping add.\")\n",
        "            return []\n",
        "\n",
        "        chunk_texts = [chunk[\"content\"] for chunk in doc_chunks]\n",
        "        chunk_metadata = [chunk[\"metadata\"] for chunk in doc_chunks]\n",
        "        chunk_ids = [str(uuid.uuid4()) for _ in range(len(doc_chunks))]\n",
        "        \n",
        "        try:\n",
        "            embeddings_list = self.embeddings.embed_documents(chunk_texts)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating embeddings for {standard.name}: {e}\")\n",
        "            return []\n",
        "            \n",
        "        self.collection.add(\n",
        "            embeddings=embeddings_list,\n",
        "            documents=chunk_texts,\n",
        "            metadatas=chunk_metadata,\n",
        "            ids=chunk_ids\n",
        "        )\n",
        "        self.logger.info(f\"Added {len(doc_chunks)} chunks from {standard.name} to vector database\")\n",
        "        return chunk_ids\n",
        "    \n",
        "    def search_standards(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        try:\n",
        "            query_embedding = self.embeddings.embed_query(query)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating query embedding: {e}\")\n",
        "            return []\n",
        "\n",
        "        results = self.collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
        "        \n",
        "        if not results or not results.get(\"documents\") or not results[\"documents\"][0]:\n",
        "            return []\n",
        "\n",
        "        documents = results[\"documents\"][0]\n",
        "        metadatas = results[\"metadatas\"][0]\n",
        "        distances = results[\"distances\"][0]\n",
        "        result_list = []\n",
        "        for i in range(len(documents)):\n",
        "            result_list.append({\n",
        "                \"content\": documents[i],\n",
        "                \"metadata\": metadatas[i],\n",
        "                \"relevance\": 1 - distances[i] if distances[i] is not None else 0\n",
        "            })\n",
        "        return result_list\n",
        "\n",
        "# Main Orchestrator (User's new version)\n",
        "class AAOIFIStandardsSystem:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.logger.info(\"Initializing AAOIFI Standards Multi-Agent System (New Orchestrator)\")\n",
        "        \n",
        "        self.review_agent = DocumentReviewAgent()\n",
        "        self.analysis_agent = StandardAnalysisAgent()\n",
        "        self.enhancement_agent = EnhancementAgentNew()\n",
        "        self.shariah_agent = ShariahComplianceAgent()\n",
        "        self.validation_agent = ValidationAgentNew()\n",
        "        self.report_agent = ReportGenerationAgent()\n",
        "        self.visualization_agent = VisualizationAgent()\n",
        "        self.feedback_agent = FeedbackAgent()\n",
        "        \n",
        "        self.vector_db = VectorDBManagerNew()\n",
        "        \n",
        "        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "        self.logger.info(\"System initialization complete (New Orchestrator)\")\n",
        "    \n",
        "    def process_standard(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        self.logger.info(f\"Beginning processing of standard: {standard.name} (New Orchestrator)\")\n",
        "        \n",
        "        self.vector_db.add_document(standard)\n",
        "        \n",
        "        review_result = self.review_agent.execute(standard)\n",
        "        analysis_result = self.analysis_agent.execute(review_result)\n",
        "        enhancement_result = self.enhancement_agent.execute(review_result, analysis_result)\n",
        "        shariah_result = self.shariah_agent.execute(enhancement_result, review_result)\n",
        "        validation_result = self.validation_agent.execute(enhancement_result, shariah_result)\n",
        "        report_result = self.report_agent.execute(review_result, analysis_result, enhancement_result, shariah_result, validation_result)\n",
        "        visualization_result = self.visualization_agent.execute(enhancement_result, shariah_result, validation_result)\n",
        "        \n",
        "        final_result = {\n",
        "            \"standard_name\": standard.name, \"review\": review_result, \"analysis\": analysis_result,\n",
        "            \"enhancement\": enhancement_result, \"shariah_assessment\": shariah_result,\n",
        "            \"validation\": validation_result, \"report\": report_result, \"visualizations\": visualization_result\n",
        "        }\n",
        "        self._save_results(final_result, suffix=\"new_orchestrator_results\")\n",
        "        self.logger.info(f\"Completed processing of standard: {standard.name} (New Orchestrator)\")\n",
        "        return final_result\n",
        "    \n",
        "    def incorporate_feedback(self, feedback: str, enhancement_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        self.logger.info(\"Processing stakeholder feedback (New Orchestrator)\")\n",
        "        # Ensure enhancement_result is not None and contains expected keys\n",
        "        if enhancement_result is None:\n",
        "            enhancement_result = {\"standard_name\": \"Unknown (from feedback)\", \"enhancement_proposals\": \"Not available due to prior error\"}\n",
        "            self.logger.warning(\"Enhancement result was None for feedback incorporation. Using placeholders.\")\n",
        "\n",
        "        feedback_result = self.feedback_agent.execute(feedback, enhancement_result)\n",
        "        self._save_results(feedback_result, suffix=\"new_orchestrator_feedback\")\n",
        "        return feedback_result\n",
        "    \n",
        "    def _save_results(self, results: Dict[str, Any], suffix: str = \"results\") -> None:\n",
        "        standard_name = results.get(\"standard_name\", \"unknown_standard\")\n",
        "        sanitized_name = re.sub(r'[^\\w\\-_\\.]', '_', standard_name)\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        output_path = os.path.join(Config.OUTPUT_DIR, f\"{sanitized_name}_{suffix}_{timestamp}.json\")\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        self.logger.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "# Utility functions for demonstration (User's new version)\n",
        "def load_sample_standard() -> StandardDocument:\n",
        "    sample_content = \"\"\"\n",
        "    AAOIFI Shariah Standard No. X: Murabahah to the Purchase Orderer\n",
        "\n",
        "    1. Scope of the Standard\n",
        "    This standard covers Murabahah to the Purchase Orderer transactions as practiced by Islamic financial institutions, including the conditions, procedures, rules, and modern applications. It does not cover simple Murabahah transactions that do not involve a prior promise to purchase.\n",
        "\n",
        "    2. Definition of Murabahah to the Purchase Orderer\n",
        "    Murabahah to the Purchase Orderer is a transaction where an Islamic financial institution (IFI) purchases an asset based on a promise from a customer to buy the asset from the institution on Murabahah terms (cost plus profit) after the institution has purchased it.\n",
        "\n",
        "    3. Shariah Requirements for Murabahah to the Purchase Orderer\n",
        "    3.1 The IFI must acquire ownership of the asset before selling it to the customer.\n",
        "    3.2 The IFI must bear the risks of ownership during the period between purchasing the asset and selling it to the customer.\n",
        "    3.3 The cost price and markup must be clearly disclosed to the customer.\n",
        "    3.4 The customer's promise to purchase is morally binding but not legally enforceable as a sale contract.\n",
        "    3.5 The Murabahah sale contract can only be executed after the IFI has acquired ownership of the asset.\n",
        "\n",
        "    4. Procedures for Murabahah to the Purchase Orderer\n",
        "    4.1 The customer identifies the asset they wish to purchase and requests the IFI to purchase it.\n",
        "    4.2 The IFI and customer enter into a promise agreement, where the customer promises to purchase the asset after the IFI acquires it.\n",
        "    4.3 The IFI purchases the asset from the supplier.\n",
        "    4.4 The IFI informs the customer that it has acquired the asset and offers to sell it on Murabahah terms.\n",
        "    4.5 The customer accepts the offer, and a Murabahah sale contract is executed.\n",
        "    4.6 The customer pays the agreed price, either in installments or as a lump sum.\n",
        "\n",
        "    5. Modern Applications and Issues\n",
        "    5.1 Appointment of the customer as agent: The IFI may appoint the customer as its agent to purchase the asset on its behalf, provided that the customer acts in a genuine agency capacity.\n",
        "    5.2 Third-party guarantees: Independent third parties may provide guarantees to protect against negligence or misconduct.\n",
        "    5.3 Late payment: The IFI may require the customer to donate to charity in case of late payment, but may not benefit from these amounts.\n",
        "    5.4 Rebate for early settlement: The IFI may voluntarily give a rebate for early settlement, but this cannot be stipulated in the contract.\n",
        "\n",
        "    6. Shariah Rulings on Specific Murabahah Issues\n",
        "    6.1 It is not permissible to roll over a Murabahah financing by extending the payment period in exchange for an increase in the amount owed.\n",
        "    6.2 Currency exchange (sarf) must be completed before the Murabahah transaction when purchasing assets in a different currency.\n",
        "    6.3 Conventional insurance on Murabahah assets should be avoided in favor of Takaful (Islamic insurance) when available.\n",
        "\n",
        "    7. Documentation Requirements\n",
        "    7.1 Promise document: Detailing the customer's promise to purchase the asset.\n",
        "    7.2 Agency agreement (if applicable): Appointing the customer as agent for purchasing the asset.\n",
        "    7.3 Murabahah sale contract: Documenting the actual sale transaction.\n",
        "    7.4 Security documents: Including collateral, guarantees, or pledges to secure the payment.\n",
        "    \"\"\"\n",
        "    return StandardDocument(name=\"Murabahah Standard X\", content=sample_content)\n",
        "\n",
        "\n",
        "def visualize_results(results: Dict[str, Any]) -> None:\n",
        "    if not results:\n",
        "        logger.error(\"No results provided for visualization\")\n",
        "        return\n",
        "    standard_name = results.get(\"standard_name\", \"Unknown Standard\")\n",
        "    try:\n",
        "        # Display Executive Summary if available\n",
        "        report_data = results.get(\"report\", {})\n",
        "        executive_summary = report_data.get(\"executive_summary\", f\"Section 'Executive Summary' not found for {standard_name}.\")\n",
        "        if \"IPython\" in globals() and \"display\" in globals() and \"Markdown\" in globals(): # Check if in IPython\n",
        "            display(Markdown(f\"## Executive Summary for {standard_name}\"))\n",
        "            display(Markdown(executive_summary))\n",
        "        else:\n",
        "            print(f\"\\n## Executive Summary for {standard_name}\\n{executive_summary}\\n\")\n",
        "\n",
        "        \n",
        "        # Shariah Compliance Chart\n",
        "        shariah_assessment_data = results.get(\"shariah_assessment\", {})\n",
        "        rulings_data = shariah_assessment_data.get(\"overall_ruling\") # This should now be a dict\n",
        "        \n",
        "        if rulings_data and isinstance(rulings_data, dict) and any(rulings_data.values()): # Check if dict and has some values\n",
        "            df_data = {'Category': [], 'Ruling': []}\n",
        "            for cat, rul in rulings_data.items():\n",
        "                # Filter out default \"Not specifically assessed\" if we want to only show actual rulings.\n",
        "                # Or, keep it to show what wasn't explicitly ruled on.\n",
        "                # For now, let's include it if it's the value, as the prompt for Shariah asks for all 5.\n",
        "                if rul and rul != f\"Section '{cat}' not found.\":\n",
        "                    df_data['Category'].append(cat.replace('_', ' ').title())\n",
        "                    df_data['Ruling'].append(rul)\n",
        "            \n",
        "            if not df_data['Category']:\n",
        "                logger.info(\"No Shariah ruling data (even 'Not specifically assessed') to visualize after filtering error strings.\")\n",
        "                return\n",
        "\n",
        "            df = pd.DataFrame(df_data)\n",
        "            \n",
        "            ruling_map = {\n",
        "                'Approved': 3, 'Conditionally Approved': 2,\n",
        "                'Requires Modification': 1, 'Rejected': 0,\n",
        "                'Not specifically assessed': 1.5,\n",
        "                'N/A': 0.5\n",
        "            }\n",
        "            df['Score'] = df['Ruling'].map(lambda x: ruling_map.get(x, 1.5)) # Map to scores, default to 1.5 (Not specifically assessed value)\n",
        "            \n",
        "            plt.figure(figsize=(12, 8))\n",
        "            bar_colors = []\n",
        "            for score in df['Score']:\n",
        "                if score >= 2.5: bar_colors.append('green')\n",
        "                elif score >= 1.8: bar_colors.append('yellowgreen')\n",
        "                elif score >= 1.2: bar_colors.append('orange')\n",
        "                elif score >= 0.8: bar_colors.append('coral')\n",
        "                else: bar_colors.append('red')\n",
        "\n",
        "            bars = plt.bar(df['Category'], df['Score'], color=bar_colors)\n",
        "            \n",
        "            plt.title(f'Shariah Compliance Assessment for {standard_name}', fontsize=14)\n",
        "            plt.xlabel('Enhancement Category', fontsize=12)\n",
        "            plt.ylabel('Compliance Level (Numeric Score)', fontsize=12)\n",
        "            plt.xticks(rotation=30, ha='right', fontsize=10)\n",
        "            \n",
        "            unique_scores_in_data = sorted(list(set(df['Score'])))\n",
        "            \n",
        "            yticks_locs = []\n",
        "            yticks_labels = []\n",
        "            \n",
        "            standard_ticks = {value: key for key, value in ruling_map.items()}\n",
        "            for score_val in sorted(standard_ticks.keys()):\n",
        "                 if score_val not in yticks_locs:\n",
        "                    yticks_locs.append(score_val)\n",
        "                    yticks_labels.append(standard_ticks[score_val])\n",
        "\n",
        "            for score_val in unique_scores_in_data:\n",
        "                if score_val not in yticks_locs:\n",
        "                    yticks_locs.append(score_val)\n",
        "                    found_label = False\n",
        "                    for lab, sc in ruling_map.items():\n",
        "                        if sc == score_val:\n",
        "                            yticks_labels.append(lab)\n",
        "                            found_label = True\n",
        "                            break\n",
        "                    if not found_label:\n",
        "                        yticks_labels.append(f\"Score {score_val:.1f}\")\n",
        "\n",
        "            sorted_ticks_combined = sorted(list(set(zip(yticks_locs, yticks_labels))))\n",
        "            final_yticks_locs = [loc for loc, lab in sorted_ticks_combined]\n",
        "            final_yticks_labels = [lab for loc, lab in sorted_ticks_combined]\n",
        "\n",
        "            plt.yticks(final_yticks_locs, final_yticks_labels, fontsize=10)\n",
        "            plt.ylim(bottom=min(0, min(final_yticks_locs)-0.2 if final_yticks_locs else 0) , top=max(3.2, max(final_yticks_locs)+0.2 if final_yticks_locs else 3.2))\n",
        "\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.tight_layout()\n",
        "            \n",
        "            for bar, ruling_text in zip(bars, df['Ruling']):\n",
        "                plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05,\n",
        "                         ruling_text, ha='center', va='bottom', fontsize=9, color='black')\n",
        "            \n",
        "            plt.show()\n",
        "        else:\n",
        "            logger.info(\"No 'overall_ruling' data available or it's empty for Shariah compliance visualization.\")\n",
        "\n",
        "    except ImportError:\n",
        "        logger.warning(\"Matplotlib or Pandas not installed. Skipping visualization that requires them.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred during visualization: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "# Demo execution (User's new version)\n",
        "def run_demo():\n",
        "    try:\n",
        "        logger.info(\"Starting AAOIFI Standards Multi-Agent System demonstration (New Orchestrator & Agents)\")\n",
        "        system = AAOIFIStandardsSystem()\n",
        "        standard = load_sample_standard()\n",
        "        logger.info(f\"Loaded sample standard: {standard.name}\")\n",
        "        results = system.process_standard(standard)\n",
        "        \n",
        "        sample_feedback = \"\"\"\n",
        "        Feedback from Islamic Financial Institutions:\n",
        "        1. The technological integration suggestions for blockchain-based Murabahah tracking are innovative but may be too complex for immediate implementation.\n",
        "        2. The clarification on agency arrangements is helpful, but requires more specific guidelines for documentation.\n",
        "        \n",
        "        Feedback from Shariah Scholars:\n",
        "        1. The proposed modifications on risk transfer mechanisms need further elaboration to ensure full Shariah compliance.\n",
        "        \n",
        "        Feedback from Regulators:\n",
        "        1. The proposed standardized documentation would facilitate regulatory oversight.\n",
        "        \"\"\"\n",
        "        feedback_results = system.incorporate_feedback(sample_feedback, results.get(\"enhancement\", {}))\n",
        "        \n",
        "        visualize_results(results)\n",
        "        logger.info(\"Demonstration completed successfully (New Orchestrator & Agents)\")\n",
        "        return results, feedback_results\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in demonstration (New Orchestrator & Agents): {str(e)}\", exc_info=True)\n",
        "        return None, None # Return None on failure to avoid NameError later\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"AAOIFI Standards Multi-Agent System - Main Execution Start\")\n",
        "    \n",
        "    if not Config.OPENAI_API_KEY:\n",
        "        logger.error(\"ERROR: OPENAI_API_KEY not set. Please set this environment variable or in the script.\")\n",
        "        exit(1)\n",
        "    \n",
        "    pdf_folder = Config.PDF_FOLDER\n",
        "    db_dir = Config.DB_DIRECTORY # Use the potentially updated one if process_pdfs_safe ran\n",
        "\n",
        "    should_process_pdfs = False\n",
        "    if not os.path.exists(db_dir) or (os.path.isdir(db_dir) and not os.listdir(db_dir)): # Check if directory is empty\n",
        "        logger.warning(f\"Vector DB directory '{db_dir}' appears to be missing or empty.\")\n",
        "        should_process_pdfs = True\n",
        "    \n",
        "    if should_process_pdfs:\n",
        "        if os.path.exists(pdf_folder) and any(f.lower().endswith('.pdf') for f in os.listdir(pdf_folder)):\n",
        "            user_choice = input(f\"Vector DB at '{db_dir}' might be empty/missing. Process PDFs from '{pdf_folder}' to create/recreate it? (yes/no): \").strip().lower()\n",
        "            if user_choice == 'yes':\n",
        "                logger.info(\"Attempting to process PDFs using safe recreate method...\")\n",
        "                process_pdfs_safe() # This updates Config.DB_DIRECTORY globally\n",
        "                db_dir = Config.DB_DIRECTORY\n",
        "                logger.info(f\"PDF processing complete. DB is now at '{db_dir}'.\")\n",
        "            else:\n",
        "                logger.info(\"PDF processing skipped by user. Demo will proceed with current DB state.\")\n",
        "        else:\n",
        "            logger.error(f\"PDF folder '{pdf_folder}' is missing or empty. Cannot process PDFs. Demo might fail if DB is not populated.\")\n",
        "\n",
        "\n",
        "    logger.info(\"Running demonstration with New Orchestrator & Agents...\")\n",
        "    run_demo_results, run_demo_feedback_results = None, None\n",
        "    try:\n",
        "        run_demo_results, run_demo_feedback_results = run_demo()\n",
        "        if run_demo_results:\n",
        "            logger.info(f\"Demonstration results obtained for standard: {run_demo_results.get('standard_name')}\")\n",
        "        if run_demo_feedback_results:\n",
        "            logger.info(f\"Feedback analysis obtained for standard: {run_demo_feedback_results.get('standard_name')}\")\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Demonstration run failed critically: {e}\", exc_info=True)\n",
        "\n",
        "    logger.info(f\"Main execution finished. Results (if any) saved to {Config.OUTPUT_DIR} directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8fc6d59",
      "metadata": {
        "id": "d8fc6d59"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f12df203",
      "metadata": {
        "id": "f12df203",
        "outputId": "6fc035dc-a951-46db-d43c-6251ea9345bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing AAOIFI Standards Enhancement System for Demo...\n",
            "Initializing VectorDBManager with database directory: aaoifi_vector_db\n",
            "Successfully created PersistentClient for database at: aaoifi_vector_db\n",
            "Successfully accessed/created collection: aaoifi_standards\n",
            "Found existing standards: ['AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9', 'AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR', 'AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean']...\n",
            "Initializing VectorDBManager with database directory: aaoifi_vector_db\n",
            "Successfully created PersistentClient for database at: aaoifi_vector_db\n",
            "Successfully accessed/created collection: aaoifi_standards\n",
            "\n",
            "Select Demo Type:\n",
            "1. Basic Demo (Review, Enhance, Validate, Report)\n",
            "2. Enhanced Demo (includes Visualization and Feedback Analysis)\n",
            "\n",
            "==================================================\n",
            "AAOIFI STANDARDS ENHANCEMENT SYSTEM DEMONSTRATION\n",
            "==================================================\n",
            "\n",
            "Available standards:\n",
            "1. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
            "2. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
            "3. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
            "4. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
            "5. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
            "6. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
            "7. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
            "8. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
            "9. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n",
            "10. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
            "11. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
            "12. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
            "13. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
            "14. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n",
            "15. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
            "16. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
            "17. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
            "18. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
            "19. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
            "20. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
            "21. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
            "22. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
            "23. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
            "24. FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean\n",
            "25. FAS-44_-Determining-Control-of-Assets-and-Business-Final\n",
            "26. FAS-45_Quasi-equity-Including-Investment-Accounts-Final\n",
            "\n",
            "Selected standard: AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
            "\n",
            "--------------------------------------------------\n",
            "PHASE 1: STANDARD REVIEW AND ANALYSIS (ReviewAgent)\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieving content for standard: AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR...\n",
            "Reviewing standard using ReviewAgent...\n",
            "ReviewAgent completed in 5.57 seconds.\n",
            "\n",
            "REVIEW SUMMARY:\n",
            "- Standard: AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
            "\n",
            "Core Principles (excerpt):\n",
            "Section 'Core principles and objectives' not found or parsing error.\n",
            "\n",
            "Key Definitions (excerpt):\n",
            "Section 'Key definitions and terminology' not found or parsing error.\n",
            "\n",
            "--------------------------------------------------\n",
            "PHASE 2: AI-DRIVEN ENHANCEMENT PROPOSALS (EnhancementAgent)\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating enhancement proposals for: AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR using EnhancementAgent...\n",
            "EnhancementAgent completed in 18.80 seconds.\n",
            "\n",
            "ENHANCEMENT PROPOSALS (excerpt):\n",
            "Apologies for the confusion. Without a specific standard to review, I will provide general suggestions on how AAOIFI standards could be enhanced.\n",
            "\n",
            "1. Clarity Improvements:\n",
            "   - Section: General\n",
            "   - Current Concept: Complex financial terminology\n",
            "   - Proposed Modification: Include a glossary of terms at the end of each standard.\n",
            "   - Justification: This will help users understand complex financial terms and ensure consistent interpretation.\n",
            "\n",
            "2. Modern Context Adaptations:\n",
            "   - Section: General\n",
            " ...\n",
            "\n",
            "--------------------------------------------------\n",
            "PHASE 3: SHARIAH COMPLIANCE VALIDATION (ValidationAgent)\n",
            "--------------------------------------------------\n",
            "\n",
            "Validating proposed enhancements for: AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR using ValidationAgent...\n",
            "ValidationAgent completed in 18.48 seconds.\n",
            "\n",
            "VALIDATION RESULTS (excerpt):\n",
            "Assessment: Approved with minor modifications\n",
            "\n",
            "Justification:\n",
            "\n",
            "1. Shariah Compliance: All proposed enhancements align with Islamic principles and AAOIFI's mission. They do not propose any changes that would contradict Shariah law.\n",
            "\n",
            "2. Technical Accuracy: The proposed language is precise and technically sound. However, the term \"digital banking\" in the second proposal could be replaced with \"Islamic digital banking\" to ensure that the context remains within the scope of Islamic finance.\n",
            "\n",
            "3. Pract...\n",
            "\n",
            "--------------------------------------------------\n",
            "PHASE 4: COMPREHENSIVE REPORT GENERATION (FinalReportAgent)\n",
            "--------------------------------------------------\n",
            "\n",
            "Generating comprehensive report for: AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR using FinalReportAgent...\n",
            "FinalReportAgent completed in 35.97 seconds.\n",
            "\n",
            "FINAL REPORT (excerpt):\n",
            "# AAOIFI Standards Review and Enhancement Report\n",
            "\n",
            "## Executive Summary\n",
            "\n",
            "This report presents a comprehensive review of the AAOIFI (Accounting and Auditing Organization for Islamic Financial Institutions) standards, with a focus on potential enhancements and their validation. Although a specific standard was not provided for review, the report outlines general improvements that could be applied across all AAOIFI standards. These enhancements aim to improve clarity, adapt to modern financial contexts, integrate technology, enhance cross-referencing, and provide practical implementation examples....\n",
            "\n",
            "--------------------------------------------------\n",
            "SAVING DEMONSTRATION RESULTS\n",
            "--------------------------------------------------\n",
            "Full results saved to results\\AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR_full_results_20250509_172835.json\n",
            "Final report saved to results\\AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR_final_report_20250509_172835.md\n",
            "\n",
            "==================================================\n",
            "DEMONSTRATION COMPLETED SUCCESSFULLY FOR STANDARD: AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
            "==================================================\n",
            "All results and the final report have been saved to the 'results' directory.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class StandardDocument:\n",
        "    \"\"\"Class to represent an AAOIFI standard document.\"\"\"\n",
        "    def __init__(self, name: str, content: str):\n",
        "        self.name = name\n",
        "        self.content = content\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"Base class for all agents in the system.\"\"\"\n",
        "    def __init__(self, name: str, description: str, model_name: str = Config.GPT4_MODEL):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.model_name = model_name\n",
        "        # Ensure API key is set before initializing ChatOpenAI\n",
        "        if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "            raise ValueError(\"OPENAI_API_KEY not set in environment.\")\n",
        "        self.llm = ChatOpenAI(model_name=model_name, temperature=0.2)\n",
        "\n",
        "\n",
        "    def execute(self, input_data: Any) -> Any:\n",
        "        \"\"\"Execute the agent's task.\"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "class ReviewAgent(BaseAgent):\n",
        "    \"\"\"Agent responsible for reviewing standards and extracting key elements.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ReviewAgent\",\n",
        "            description=\"Analyzes AAOIFI standards to extract key elements, principles, and requirements.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert in Islamic finance and AAOIFI standards. Your task is to carefully review\n",
        "        the provided standard document and extract the following key elements:\n",
        "\n",
        "        1. Core principles and objectives of the standard\n",
        "        2. Key definitions and terminology\n",
        "        3. Main requirements and procedures\n",
        "        4. Compliance criteria and guidelines\n",
        "        5. Practical implementation considerations\n",
        "\n",
        "        Organize your analysis in a structured format with these categories. Be thorough but concise\n",
        "        in your extraction of the essential components. Respond with the extracted information directly,\n",
        "        using markdown for headings for each category.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze a standard document and extract its key elements.\n",
        "\n",
        "        Args:\n",
        "            standard: The standard document to analyze.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the extracted key elements.\n",
        "        \"\"\"\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {standard.name}\\n\\nContent:\\n{standard.content}\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        # The chain.run({}) is problematic if the prompt itself needs variables.\n",
        "        # Here, standard.name and standard.content are already formatted into the HumanMessage.\n",
        "        # If the prompt template had input_variables, they would be passed to run().\n",
        "        result_text = chain.run({}) # Assuming the prompt is self-contained after formatting.\n",
        "\n",
        "        # Try to parse the result into structured data (simple approach)\n",
        "        parsed_result = {\n",
        "            \"standard_name\": standard.name,\n",
        "            \"review_result\": result_text, # This is the full text from LLM\n",
        "            \"core_principles\": self._extract_section(result_text, \"Core principles and objectives\"),\n",
        "            \"key_definitions\": self._extract_section(result_text, \"Key definitions and terminology\"),\n",
        "            \"main_requirements\": self._extract_section(result_text, \"Main requirements and procedures\"),\n",
        "            \"compliance_criteria\": self._extract_section(result_text, \"Compliance criteria and guidelines\"), # \"guidelines\" was missing\n",
        "            \"implementation_considerations\": self._extract_section(result_text, \"Practical implementation considerations\") # \"Practical\" was missing\n",
        "        }\n",
        "        return parsed_result\n",
        "\n",
        "\n",
        "    def _extract_section(self, text: str, section_name: str) -> str:\n",
        "        \"\"\"Helper method to extract specific sections from the review result using regex.\"\"\"\n",
        "        # Regex to find a heading (e.g., \"1. Section Name\" or \"Section Name:\") and capture text until the next heading or end of string\n",
        "        # This assumes headings are followed by a newline.\n",
        "        # It also makes section_name matching case-insensitive.\n",
        "        pattern = re.compile(\n",
        "            r\"(?i)(?:^\\s*\\d*\\.?\\s*|\\n\\s*\\d*\\.?\\s*)\" + re.escape(section_name) + r\"\\s*[:\\n](.*?)(?=\\n\\s*\\d*\\.?\\s*\\w+.*?\\s*[:\\n]|\\Z)\",\n",
        "            re.DOTALL | re.IGNORECASE\n",
        "        )\n",
        "        match = pattern.search(text)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        return f\"Section '{section_name}' not found or parsing error.\"\n",
        "\n",
        "\n",
        "class EnhancementAgent(BaseAgent):\n",
        "    \"\"\"Agent tasked with proposing modifications or enhancements to standards.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"EnhancementAgent\",\n",
        "            description=\"Proposes AI-driven modifications and enhancements to AAOIFI standards.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an AI expert specializing in Islamic finance and AAOIFI standards enhancement.\n",
        "        Your task is to propose thoughtful modifications and enhancements to the standard based\n",
        "        on the review provided.\n",
        "\n",
        "        Consider the following aspects in your proposals:\n",
        "\n",
        "        1. Clarity improvements: Suggest clearer language or better organization where appropriate\n",
        "        2. Modern context adaptations: Propose updates to address contemporary financial practices\n",
        "        3. Technological integration: Recommend ways to incorporate digital technologies and fintech\n",
        "        4. Cross-reference enhancements: Suggest improved links to related standards or principles\n",
        "        5. Practical implementation: Provide more actionable guidance for practitioners\n",
        "\n",
        "        For each suggestion, provide:\n",
        "        - The specific section or clause being enhanced (if applicable, otherwise general proposal)\n",
        "        - The current text or concept (if applicable, very brief summary)\n",
        "        - Your proposed modification or addition\n",
        "        - A brief justification explaining the benefit of your enhancement\n",
        "\n",
        "        Structure your response clearly, perhaps using bullet points for each proposal.\n",
        "        Ensure all suggestions maintain strict compliance with Shariah principles.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Propose enhancements to a standard based on the review.\n",
        "\n",
        "        Args:\n",
        "            review_result: The result from the ReviewAgent (the full dictionary).\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing proposed enhancements.\n",
        "        \"\"\"\n",
        "        # We need the text of the review, not the whole dict, for the prompt\n",
        "        review_text_summary = review_result.get(\"review_result\", \"No review summary available.\")\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\n\\nSummary of Standard Review:\\n{review_text_summary}\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        result_text = chain.run({})\n",
        "\n",
        "        return {\n",
        "            \"standard_name\": review_result[\"standard_name\"],\n",
        "            \"enhancement_proposals\": result_text # This is the LLM's textual output\n",
        "        }\n",
        "\n",
        "class ValidationAgent(BaseAgent):\n",
        "    \"\"\"Agent for validating and approving proposed changes.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ValidationAgent\",\n",
        "            description=\"Validates proposed changes based on Shariah compliance and practicality.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a senior Shariah scholar and AAOIFI standards expert. Your role is to validate\n",
        "        proposed enhancements to ensure they maintain compliance with Islamic principles and\n",
        "        practical applicability.\n",
        "\n",
        "        For each proposed enhancement, evaluate:\n",
        "\n",
        "        1. Shariah Compliance: Does the proposal align with Islamic principles and AAOIFI's mission?\n",
        "        2. Technical Accuracy: Is the proposed language precise and technically sound?\n",
        "        3. Practical Applicability: Can the enhancement be practically implemented by Islamic financial institutions?\n",
        "        4. Consistency: Does it maintain consistency with other standards and established practices?\n",
        "        5. Value Addition: Does it meaningfully improve the standard?\n",
        "\n",
        "        For each proposal, provide:\n",
        "        - Your assessment (e.g., Approved, Rejected, Needs Modification)\n",
        "        - A detailed justification for your decision, referencing the evaluation criteria above.\n",
        "        - Suggested refinements if \"Needs Modification\".\n",
        "\n",
        "        Be thorough in your analysis and maintain the highest standards of Islamic finance integrity.\n",
        "        Structure your response clearly, addressing each proposal from the input.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, enhancement_result: Dict[str, Any], original_review: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate proposed enhancements based on Shariah compliance and practicality.\n",
        "\n",
        "        Args:\n",
        "            enhancement_result: The result from the EnhancementAgent (dictionary).\n",
        "            original_review: The original review from the ReviewAgent (dictionary).\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing validation results.\n",
        "        \"\"\"\n",
        "        review_text_summary = original_review.get(\"review_result\", \"No review summary available.\")\n",
        "        enhancement_proposals_text = enhancement_result.get(\"enhancement_proposals\", \"No enhancement proposals available.\")\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {enhancement_result['standard_name']}\n",
        "\n",
        "            Original Standard Review Summary:\n",
        "            {review_text_summary}\n",
        "\n",
        "            Proposed Enhancements to Validate:\n",
        "            {enhancement_proposals_text}\n",
        "            \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        result_text = chain.run({})\n",
        "\n",
        "        return {\n",
        "            \"standard_name\": enhancement_result[\"standard_name\"],\n",
        "            \"validation_result\": result_text # This is the LLM's textual output\n",
        "        }\n",
        "\n",
        "class FinalReportAgent(BaseAgent):\n",
        "    \"\"\"Agent for generating a comprehensive final report.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"FinalReportAgent\",\n",
        "            description=\"Compiles all findings and recommendations into a comprehensive report.\",\n",
        "            model_name=Config.GPT4_MODEL # GPT-4 for high-quality report generation\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a professional report writer specializing in Islamic finance standards. Your task is to\n",
        "        compile all the findings, enhancements, and validations into a comprehensive, well-structured report\n",
        "        in Markdown format.\n",
        "\n",
        "        Your report should include the following sections:\n",
        "\n",
        "        1.  **Executive Summary**: Brief overview of the standard reviewed, key findings, summary of proposed changes, and validation outcomes.\n",
        "        2.  **Standard Overview**: Summary of the original standard's purpose and core components (based on the review).\n",
        "        3.  **Key Findings from Review**: Major elements and considerations identified during the initial review.\n",
        "        4.  **Proposed Enhancements**: Clear presentation of all proposed modifications.\n",
        "        5.  **Validation Results**: Summary of the validation process and outcomes for each proposal.\n",
        "        6.  **Consolidated Recommendations**: A final list of approved or modified enhancements.\n",
        "        7.  **Implementation Considerations**: Practical next steps for adopting approved changes.\n",
        "        8.  **Conclusion**: Final thoughts on the impact of the proposed enhancements.\n",
        "\n",
        "        Write in a professional, clear, and objective style appropriate for AAOIFI stakeholders and Islamic finance professionals.\n",
        "        Use Markdown for formatting (headings, lists, bolding).\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Generate a comprehensive final report.\n",
        "\n",
        "        Args:\n",
        "            all_results: Combined results from all previous agents.\n",
        "                         Expected keys: 'standard_name', 'review_text',\n",
        "                                        'enhancements_text', 'validation_text'.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the final report text.\n",
        "        \"\"\"\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {all_results['standard_name']}\n",
        "\n",
        "            Full Text of Standard Review:\n",
        "            {all_results['review_text']}\n",
        "\n",
        "            Full Text of Proposed Enhancements:\n",
        "            {all_results['enhancements_text']}\n",
        "\n",
        "            Full Text of Validation of Enhancements:\n",
        "            {all_results['validation_text']}\n",
        "            \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        report_text = chain.run({})\n",
        "\n",
        "        return {\n",
        "            \"standard_name\": all_results[\"standard_name\"],\n",
        "            \"final_report\": report_text # This is the LLM's textual output (Markdown)\n",
        "        }\n",
        "\n",
        "\n",
        "class VectorDBManager:\n",
        "    \"\"\"Manager for interacting with the vector database.\"\"\"\n",
        "\n",
        "    def __init__(self, db_directory: str = None, collection_name: str = config.COLLECTION_NAME):\n",
        "        self.db_directory = db_directory if db_directory else config.DB_DIRECTORY\n",
        "        self.collection_name = collection_name\n",
        "        self.embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
        "\n",
        "        print(f\"Initializing VectorDBManager with database directory: {self.db_directory}\")\n",
        "\n",
        "        try:\n",
        "            if not os.path.exists(self.db_directory):\n",
        "                print(f\"Database directory {self.db_directory} does not exist. Creating it.\")\n",
        "                os.makedirs(self.db_directory, exist_ok=True)\n",
        "            self.client = chromadb.PersistentClient(path=self.db_directory)\n",
        "            print(f\"Successfully created PersistentClient for database at: {self.db_directory}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Fatal error creating PersistentClient for {self.db_directory}: {str(e)}\")\n",
        "            self.client = None\n",
        "            self.collection = None\n",
        "            raise  # Reraise critical error\n",
        "\n",
        "        if self.client:\n",
        "            try:\n",
        "                # Try to get the collection; if it fails, try to create it.\n",
        "                self.collection = self.client.get_or_create_collection(name=self.collection_name)\n",
        "                print(f\"Successfully accessed/created collection: {self.collection_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error accessing/creating collection '{self.collection_name}': {str(e)}\")\n",
        "                self.collection = None\n",
        "        else:\n",
        "            self.collection = None\n",
        "\n",
        "\n",
        "    def get_standard_content(self, standard_name: str) -> str:\n",
        "        \"\"\"\n",
        "        Retrieve the content for a specific standard.\n",
        "\n",
        "        Args:\n",
        "            standard_name: The name of the standard to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            The combined content of the standard, or an error message.\n",
        "        \"\"\"\n",
        "        if not self.collection:\n",
        "            return \"Error: Vector database collection not properly initialized.\"\n",
        "\n",
        "        try:\n",
        "            # Query for all chunks belonging to this standard\n",
        "            # A large n_results is needed if a standard has many chunks\n",
        "            # Chroma's default n_results is 10.\n",
        "            # To get ALL documents matching a filter, it's better to use collection.get()\n",
        "            results = self.collection.get(\n",
        "                where={\"source\": standard_name},\n",
        "                include=[\"documents\"] # Only fetch documents\n",
        "            )\n",
        "\n",
        "            if results and results['documents']:\n",
        "                # Sort by chunk_id if available in metadata, though .get() doesn't easily allow sorting\n",
        "                # For now, just join them. If order is critical, store chunk_id and sort post-retrieval.\n",
        "                # The current split_text_into_chunks adds metadata, but .get() returns raw docs.\n",
        "                # Let's assume for now that the order from .get() is sufficient or re-query with sort if needed.\n",
        "                # The current query in `create_vector_database` uses IDs like `doc_{i+j}` which are sequential.\n",
        "                # If we rely on implicit order of `get`, it should be mostly fine.\n",
        "\n",
        "                # The metadata is not directly available here to sort by chunk_id without another call or more complex query\n",
        "                # For simplicity, joining in received order.\n",
        "                standard_content = \"\\n\\n\".join(doc for doc in results['documents'] if doc)\n",
        "                if not standard_content:\n",
        "                    return f\"No document content found for standard: {standard_name}, though metadatas might exist.\"\n",
        "                return standard_content\n",
        "            else:\n",
        "                return f\"No content found for standard: {standard_name}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error querying for standard {standard_name}: {str(e)}\"\n",
        "\n",
        "    def list_available_standards(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        List all available standards in the database.\n",
        "\n",
        "        Returns:\n",
        "            A list of standard names, or an error message list.\n",
        "        \"\"\"\n",
        "        if not self.collection:\n",
        "            return [\"Error: Vector database collection not properly initialized.\"]\n",
        "\n",
        "        try:\n",
        "            results = self.collection.get(include=[\"metadatas\"]) # Fetch only metadatas\n",
        "            if results and results['metadatas']:\n",
        "                standards = set()\n",
        "                for metadata in results['metadatas']:\n",
        "                    if metadata and 'source' in metadata: # Check if metadata is not None\n",
        "                        standards.add(metadata['source'])\n",
        "                if not standards:\n",
        "                    return [\"No standards found in the database. Metadatas might be empty or lack 'source'.\"]\n",
        "                return sorted(list(standards))\n",
        "            else:\n",
        "                return [\"No standards found in the database. Please process PDFs first.\"]\n",
        "        except Exception as e:\n",
        "            return [f\"Error listing standards: {str(e)}\"]\n",
        "\n",
        "\n",
        "class AAOIFIStandardsEnhancementSystem:\n",
        "    \"\"\"Main system coordinating the multi-agent process.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Initialize vector database manager\n",
        "        # It will use config.DB_DIRECTORY which might be updated by process_pdfs_safe\n",
        "        self.db_manager = VectorDBManager()\n",
        "\n",
        "        # Initialize agents\n",
        "        self.review_agent = ReviewAgent()\n",
        "        self.enhancement_agent = EnhancementAgent()\n",
        "        self.validation_agent = ValidationAgent()\n",
        "        self.report_agent = FinalReportAgent()\n",
        "\n",
        "        # Track processing results for a single standard run\n",
        "        self.current_processing_results = {}\n",
        "\n",
        "    def list_available_standards(self) -> List[str]:\n",
        "        \"\"\"List all available standards in the system.\"\"\"\n",
        "        return self.db_manager.list_available_standards()\n",
        "\n",
        "    def process_standard(self, standard_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Process a single standard through the complete pipeline.\n",
        "\n",
        "        Args:\n",
        "            standard_name: The name of the standard to process.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the final results for this standard.\n",
        "        \"\"\"\n",
        "        print(f\"\\nProcessing standard: {standard_name}\")\n",
        "        self.current_processing_results = {\"standard_name\": standard_name}\n",
        "\n",
        "        # Step 1: Get standard content from the vector database\n",
        "        print(\"  Retrieving standard content...\")\n",
        "        content = self.db_manager.get_standard_content(standard_name)\n",
        "        if content.startswith(\"Error:\") or content.startswith(\"No content found\"):\n",
        "            print(f\"  Error retrieving content for {standard_name}: {content}\")\n",
        "            self.current_processing_results[\"error\"] = content\n",
        "            return self.current_processing_results\n",
        "\n",
        "        standard_doc = StandardDocument(name=standard_name, content=content)\n",
        "        self.current_processing_results[\"original_content_excerpt\"] = content[:500] + \"...\" if len(content) > 500 else content\n",
        "\n",
        "        # Step 2: Review and extract key elements\n",
        "        print(\"  Reviewing standard and extracting key elements...\")\n",
        "        review_output = self.review_agent.execute(standard_doc)\n",
        "        self.current_processing_results[\"review_output\"] = review_output # Store the whole dict\n",
        "\n",
        "        # Step 3: Propose enhancements\n",
        "        print(\"  Proposing enhancements...\")\n",
        "        enhancement_output = self.enhancement_agent.execute(review_output) # Pass the dict\n",
        "        self.current_processing_results[\"enhancement_output\"] = enhancement_output\n",
        "\n",
        "        # Step 4: Validate proposed changes\n",
        "        print(\"  Validating proposed changes...\")\n",
        "        # Validation agent needs enhancement_output (dict) and review_output (dict)\n",
        "        validation_output = self.validation_agent.execute(enhancement_output, review_output)\n",
        "        self.current_processing_results[\"validation_output\"] = validation_output\n",
        "\n",
        "        # Step 5: Generate final report\n",
        "        print(\"  Generating final report...\")\n",
        "        # Prepare data for report agent\n",
        "        report_input_data = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"review_text\": review_output.get(\"review_result\", \"N/A\"),\n",
        "            \"enhancements_text\": enhancement_output.get(\"enhancement_proposals\", \"N/A\"),\n",
        "            \"validation_text\": validation_output.get(\"validation_result\", \"N/A\")\n",
        "        }\n",
        "        final_report_output = self.report_agent.execute(report_input_data)\n",
        "        self.current_processing_results[\"final_report_output\"] = final_report_output\n",
        "\n",
        "        print(f\"Processing completed for standard: {standard_name}\")\n",
        "        return self.current_processing_results\n",
        "\n",
        "    def save_results(self, results_to_save: Dict[str, Any], output_dir: str = config.OUTPUT_DIR):\n",
        "        \"\"\"Save all results to output files.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        if not results_to_save:\n",
        "            print(\"No results to save.\")\n",
        "            return\n",
        "\n",
        "        standard_name = results_to_save.get(\"standard_name\", \"unknown_standard\")\n",
        "        # Sanitize standard_name for filename\n",
        "        safe_standard_name = re.sub(r'[^\\w\\-_\\. ]', '_', standard_name)\n",
        "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        filename_json = os.path.join(output_dir, f\"{safe_standard_name}_full_results_{timestamp}.json\")\n",
        "        with open(filename_json, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results_to_save, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Full results saved to {filename_json}\")\n",
        "\n",
        "        # Also save the final report separately as Markdown\n",
        "        final_report_text = results_to_save.get(\"final_report_output\", {}).get(\"final_report\")\n",
        "        if final_report_text:\n",
        "            filename_md = os.path.join(output_dir, f\"{safe_standard_name}_final_report_{timestamp}.md\")\n",
        "            with open(filename_md, 'w', encoding='utf-8') as f:\n",
        "                f.write(final_report_text)\n",
        "            print(f\"Final report saved to {filename_md}\")\n",
        "\n",
        "\n",
        "# --- Functions for recreating vector database safely ---\n",
        "def safely_recreate_vector_database(documents):\n",
        "    \"\"\"Create a new vector database from document chunks with proper handling of locked files.\"\"\"\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
        "\n",
        "    # Create a new directory with a unique name to avoid conflicts\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
        "    # Base directory for all DB versions\n",
        "    base_db_parent_dir = \"aaoifi_vector_db_versions\"\n",
        "    os.makedirs(base_db_parent_dir, exist_ok=True)\n",
        "    new_db_directory = os.path.join(base_db_parent_dir, f\"db_{timestamp}_{random_str}\")\n",
        "\n",
        "    print(f\"Creating new database in directory: {new_db_directory}\")\n",
        "    os.makedirs(new_db_directory, exist_ok=True)\n",
        "\n",
        "    client = chromadb.PersistentClient(path=new_db_directory)\n",
        "    # Always create a new collection in a new DB path\n",
        "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "    batch_size = 100 # As used in create_vector_database\n",
        "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch_documents = documents[i:i+batch_size]\n",
        "        current_batch_num = (i // batch_size) + 1\n",
        "        print(f\"Processing batch {current_batch_num}/{num_batches} for new vector database...\")\n",
        "\n",
        "        texts = [doc[\"content\"] for doc in batch_documents]\n",
        "        # Use more robust IDs tied to document source and chunk\n",
        "        ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}\" for doc in batch_documents]\n",
        "        # Check for ID uniqueness within the batch (essential for Chroma)\n",
        "        if len(ids) != len(set(ids)):\n",
        "            print(f\"Warning: Duplicate IDs generated in batch {current_batch_num}. This may cause issues.\")\n",
        "            # Simple fix: append index within batch to ensure uniqueness for this add op\n",
        "            ids = [f\"{id_}_{j}\" for j, id_ in enumerate(ids)]\n",
        "\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
        "\n",
        "        if not texts:\n",
        "            print(f\"Skipping empty batch {current_batch_num}.\")\n",
        "            continue\n",
        "        try:\n",
        "            embeds = embeddings.embed_documents(texts)\n",
        "            collection.add(embeddings=embeds, documents=texts, ids=ids, metadatas=metadatas)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {current_batch_num} starting at index {i}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Created new vector database with consistent embedding dimensions in '{new_db_directory}'\")\n",
        "\n",
        "    # Update the global config to point to the new directory for the current session\n",
        "    config.DB_DIRECTORY = new_db_directory\n",
        "    print(f\"Updated global config.DB_DIRECTORY to: {config.DB_DIRECTORY}\")\n",
        "\n",
        "    return client\n",
        "\n",
        "def process_pdfs_safe():\n",
        "    \"\"\"Main function to process PDFs and create vector database safely.\"\"\"\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "    if not os.path.exists(PDF_FOLDER):\n",
        "        print(f\"Error: The PDF folder '{PDF_FOLDER}' does not exist.\")\n",
        "        return None\n",
        "    if not os.listdir(PDF_FOLDER):\n",
        "        print(f\"The PDF folder '{PDF_FOLDER}' is empty. Please add PDF files to process.\")\n",
        "        return None\n",
        "\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0]\n",
        "        print(f\"Processing {pdf_file}...\")\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "        chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
        "        all_documents.extend(chunks)\n",
        "        print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    if not all_documents:\n",
        "        print(\"No documents were extracted from PDFs. Cannot create vector database.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "    print(\"Creating/Recreating vector database safely...\")\n",
        "    client = safely_recreate_vector_database(all_documents) # This updates config.DB_DIRECTORY\n",
        "    print(f\"Vector database operations completed. Current DB directory: '{config.DB_DIRECTORY}'\")\n",
        "    return client\n",
        "\n",
        "# --- START OF NEW DEMONSTRATION CODE ---\n",
        "\"\"\"\n",
        "AAOIFI Standards Enhancement System Demo\n",
        "\n",
        "This script demonstrates how to use the multi-agent architecture to:\n",
        "1.  Select an AAOIFI standard\n",
        "2.  Review and extract key elements using ReviewAgent\n",
        "3.  Propose AI-driven modifications/enhancements using EnhancementAgent\n",
        "4.  Validate proposed changes based on Shariah principles using ValidationAgent\n",
        "5.  Generate a comprehensive report using FinalReportAgent\n",
        "6.  Optionally, use additional agents for visualization and feedback simulation.\n",
        "\"\"\"\n",
        "\n",
        "class DemoRunner:\n",
        "    \"\"\"Class to run a complete demonstration of the AAOIFI Standards Enhancement System.\"\"\"\n",
        "\n",
        "    def __init__(self, system: AAOIFIStandardsEnhancementSystem):\n",
        "        self.system = system\n",
        "        self.selected_standard_name: Optional[str] = None\n",
        "        self.current_demo_results: Dict[str, Any] = {} # Stores results for the current demo run\n",
        "\n",
        "    def _get_excerpt(self, text: Optional[str], max_length: int = 300) -> str:\n",
        "        \"\"\"Get an excerpt of text with maximum length.\"\"\"\n",
        "        if not text:\n",
        "            return \"N/A\"\n",
        "        text = str(text) # Ensure it's a string\n",
        "        if len(text) <= max_length:\n",
        "            return text\n",
        "        return text[:max_length] + \"...\"\n",
        "\n",
        "    def list_and_select_standard(self) -> bool:\n",
        "        \"\"\"List all available standards and allow selection.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"AAOIFI STANDARDS ENHANCEMENT SYSTEM DEMONSTRATION\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        print(\"\\nAvailable standards:\")\n",
        "        standards = self.system.list_available_standards()\n",
        "\n",
        "        if not standards or any(s.startswith(\"Error:\") or s.startswith(\"No standards found\") for s in standards):\n",
        "            print(\"No standards available or error listing standards.\")\n",
        "            print(\"Please ensure PDFs are processed and a vector database exists.\")\n",
        "            if standards: print(f\"Details: {standards[0]}\")\n",
        "            return False\n",
        "\n",
        "        for i, standard_name in enumerate(standards):\n",
        "            print(f\"{i+1}. {standard_name}\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                choice_str = input(f\"\\nSelect a standard to process (enter number 1-{len(standards)}): \")\n",
        "                if not choice_str: continue # Handle empty input\n",
        "                choice = int(choice_str)\n",
        "                if 1 <= choice <= len(standards):\n",
        "                    self.selected_standard_name = standards[choice-1]\n",
        "                    print(f\"\\nSelected standard: {self.selected_standard_name}\")\n",
        "                    self.current_demo_results = {\"standard_name\": self.selected_standard_name} # Reset for new selection\n",
        "                    return True\n",
        "                else:\n",
        "                    print(f\"Invalid selection. Please enter a number between 1 and {len(standards)}.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "    def run_review_phase(self) -> bool:\n",
        "        \"\"\"Run the review phase and display results.\"\"\"\n",
        "        if not self.selected_standard_name:\n",
        "            print(\"Error: No standard selected for review phase.\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(\"PHASE 1: STANDARD REVIEW AND ANALYSIS (ReviewAgent)\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        print(f\"\\nRetrieving content for standard: {self.selected_standard_name}...\")\n",
        "        content = self.system.db_manager.get_standard_content(self.selected_standard_name)\n",
        "        if content.startswith(\"Error:\") or content.startswith(\"No content found\"):\n",
        "            print(f\"  Error retrieving content: {content}\")\n",
        "            self.current_demo_results[\"review_error\"] = content\n",
        "            return False\n",
        "\n",
        "        standard_document = StandardDocument(name=self.selected_standard_name, content=content)\n",
        "\n",
        "        print(f\"Reviewing standard using ReviewAgent...\")\n",
        "        start_time = time.time()\n",
        "        review_output = self.system.review_agent.execute(standard_document)\n",
        "        self.current_demo_results[\"review_output\"] = review_output\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"ReviewAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "        print(\"\\nREVIEW SUMMARY:\")\n",
        "        print(f\"- Standard: {review_output.get('standard_name', 'N/A')}\")\n",
        "        print(f\"\\nCore Principles (excerpt):\\n{self._get_excerpt(review_output.get('core_principles'))}\")\n",
        "        print(f\"\\nKey Definitions (excerpt):\\n{self._get_excerpt(review_output.get('key_definitions'))}\")\n",
        "        # print(f\"\\nFull Review Text (excerpt):\\n{self._get_excerpt(review_output.get('review_result'), 500)}\")\n",
        "        return True\n",
        "\n",
        "    def run_enhancement_phase(self) -> bool:\n",
        "        \"\"\"Run the enhancement phase and display results.\"\"\"\n",
        "        if \"review_output\" not in self.current_demo_results:\n",
        "            print(\"Error: Review phase must complete successfully before enhancement.\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(\"PHASE 2: AI-DRIVEN ENHANCEMENT PROPOSALS (EnhancementAgent)\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        review_output = self.current_demo_results[\"review_output\"]\n",
        "        print(f\"\\nGenerating enhancement proposals for: {self.selected_standard_name} using EnhancementAgent...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        enhancement_output = self.system.enhancement_agent.execute(review_output) # Pass full review dict\n",
        "        self.current_demo_results[\"enhancement_output\"] = enhancement_output\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"EnhancementAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "        print(\"\\nENHANCEMENT PROPOSALS (excerpt):\")\n",
        "        print(self._get_excerpt(enhancement_output.get(\"enhancement_proposals\"), 500))\n",
        "        return True\n",
        "\n",
        "    def run_validation_phase(self) -> bool:\n",
        "        \"\"\"Run the validation phase and display results.\"\"\"\n",
        "        if \"enhancement_output\" not in self.current_demo_results or \\\n",
        "           \"review_output\" not in self.current_demo_results:\n",
        "            print(\"Error: Review and Enhancement phases must complete successfully before validation.\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(\"PHASE 3: SHARIAH COMPLIANCE VALIDATION (ValidationAgent)\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        review_output = self.current_demo_results[\"review_output\"]\n",
        "        enhancement_output = self.current_demo_results[\"enhancement_output\"]\n",
        "\n",
        "        print(f\"\\nValidating proposed enhancements for: {self.selected_standard_name} using ValidationAgent...\")\n",
        "        start_time = time.time()\n",
        "        validation_output = self.system.validation_agent.execute(enhancement_output, review_output)\n",
        "        self.current_demo_results[\"validation_output\"] = validation_output\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"ValidationAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "        print(\"\\nVALIDATION RESULTS (excerpt):\")\n",
        "        print(self._get_excerpt(validation_output.get(\"validation_result\"), 500))\n",
        "        return True\n",
        "\n",
        "    def run_report_generation_phase(self) -> bool:\n",
        "        \"\"\"Generate the final comprehensive report.\"\"\"\n",
        "        if \"validation_output\" not in self.current_demo_results: # Implies previous phases also ran\n",
        "            print(\"Error: All previous phases (Review, Enhance, Validate) must complete before report generation.\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(\"PHASE 4: COMPREHENSIVE REPORT GENERATION (FinalReportAgent)\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        print(f\"\\nGenerating comprehensive report for: {self.selected_standard_name} using FinalReportAgent...\")\n",
        "\n",
        "        report_input_data = {\n",
        "            \"standard_name\": self.selected_standard_name,\n",
        "            \"review_text\": self.current_demo_results.get(\"review_output\", {}).get(\"review_result\", \"N/A\"),\n",
        "            \"enhancements_text\": self.current_demo_results.get(\"enhancement_output\", {}).get(\"enhancement_proposals\", \"N/A\"),\n",
        "            \"validation_text\": self.current_demo_results.get(\"validation_output\", {}).get(\"validation_result\", \"N/A\")\n",
        "        }\n",
        "\n",
        "        start_time = time.time()\n",
        "        final_report_output = self.system.report_agent.execute(report_input_data)\n",
        "        self.current_demo_results[\"final_report_output\"] = final_report_output\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"FinalReportAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "        print(\"\\nFINAL REPORT (excerpt):\")\n",
        "        print(self._get_excerpt(final_report_output.get(\"final_report\"), 600))\n",
        "        return True\n",
        "\n",
        "    def save_demo_results(self):\n",
        "        \"\"\"Save all results from the demonstration.\"\"\"\n",
        "        if not self.current_demo_results or not self.selected_standard_name:\n",
        "            print(\"No results to save or standard not selected.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(\"SAVING DEMONSTRATION RESULTS\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        # Use the system's save_results method, passing the demo's collected results\n",
        "        self.system.save_results(self.current_demo_results) # It handles directory creation and naming\n",
        "\n",
        "    def run_complete_demo(self):\n",
        "        \"\"\"Run the complete demonstration process.\"\"\"\n",
        "        if not self.list_and_select_standard():\n",
        "            print(\"Demo terminated: Standard selection failed or was aborted.\")\n",
        "            return\n",
        "\n",
        "        if not self.run_review_phase():\n",
        "            print(\"Demo terminated: Review phase failed.\")\n",
        "            return\n",
        "\n",
        "        if not self.run_enhancement_phase():\n",
        "            print(\"Demo terminated: Enhancement phase failed.\")\n",
        "            return\n",
        "\n",
        "        if not self.run_validation_phase():\n",
        "            print(\"Demo terminated: Validation phase failed.\")\n",
        "            return\n",
        "\n",
        "        if not self.run_report_generation_phase():\n",
        "            print(\"Demo terminated: Report generation phase failed.\")\n",
        "            return\n",
        "\n",
        "        self.save_demo_results()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"DEMONSTRATION COMPLETED SUCCESSFULLY FOR STANDARD: {self.selected_standard_name}\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"All results and the final report have been saved to the '{config.OUTPUT_DIR}' directory.\")\n",
        "\n",
        "# --- Additional Agents for Enhanced Demo ---\n",
        "\n",
        "class VisualizationAgent(BaseAgent):\n",
        "    \"\"\"Agent responsible for generating textual descriptions for visualizations.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"VisualizationAgent\",\n",
        "            description=\"Creates textual summaries suitable for generating visual representations of standard changes.\",\n",
        "            model_name=Config.GPT35_MODEL # Faster model for summarization\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a data visualization assistant. Your task is to process the provided summaries of AAOIFI standard reviews,\n",
        "        enhancement proposals, and validation results, and then generate concise textual descriptions that could be used\n",
        "        to create visual elements (like tables or charts).\n",
        "\n",
        "        Focus on:\n",
        "        1.  A summary table of key proposed changes: Section | Original Concept | Proposed Change | Benefit.\n",
        "        2.  A Shariah compliance assessment summary: Proposal Area | Compliance Status | Key Shariah Considerations.\n",
        "        3.  A benefits overview: Key Enhancement | Primary Benefit to Stakeholders.\n",
        "\n",
        "        Present this information clearly using Markdown tables or structured lists.\n",
        "        \"\"\"\n",
        "\n",
        "    def _get_excerpt(self, text: Optional[str], max_length: int = 300) -> str: # Helper from DemoRunner\n",
        "        if not text: return \"N/A\"\n",
        "        text = str(text)\n",
        "        if len(text) <= max_length: return text\n",
        "        return text[:max_length] + \"...\"\n",
        "\n",
        "    def execute(self, demo_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        standard_name = demo_results.get('standard_name', 'N/A')\n",
        "        review_text = self._get_excerpt(demo_results.get('review_output', {}).get('review_result', 'N/A'), 1000)\n",
        "        enhancements_text = self._get_excerpt(demo_results.get('enhancement_output', {}).get('enhancement_proposals', 'N/A'), 1000)\n",
        "        validation_text = self._get_excerpt(demo_results.get('validation_output', {}).get('validation_result', 'N/A'), 1000)\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "\n",
        "            Summary of Standard Review:\n",
        "            {review_text}\n",
        "\n",
        "            Summary of Proposed Enhancements:\n",
        "            {enhancements_text}\n",
        "\n",
        "            Summary of Validation Results:\n",
        "            {validation_text}\n",
        "\n",
        "            Please generate textual descriptions suitable for visualization based on the above.\n",
        "            \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        visualization_text_data = chain.run({})\n",
        "\n",
        "        return {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"visualization_text_data\": visualization_text_data\n",
        "        }\n",
        "\n",
        "class FeedbackAgent(BaseAgent):\n",
        "    \"\"\"Agent for collecting and analyzing simulated feedback on proposed changes.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"FeedbackAgent\",\n",
        "            description=\"Processes simulated feedback on proposed standard changes and generates insights.\",\n",
        "            model_name=Config.GPT35_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert in qualitative feedback analysis for financial standards.\n",
        "        Given a set of simulated feedback entries on proposed changes to an AAOIFI standard,\n",
        "        your task is to:\n",
        "\n",
        "        1.  Summarize the overall sentiment (Positive, Negative, Mixed).\n",
        "        2.  Identify 2-3 key themes or concerns raised by stakeholders.\n",
        "        3.  Highlight 2-3 constructive suggestions for further improvement, if any.\n",
        "        4.  Note any areas of strong consensus or significant disagreement.\n",
        "\n",
        "        Provide a concise analysis.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, standard_name: str, simulated_feedback_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        formatted_feedback = \"\\n\\n\".join([\n",
        "            f\"Feedback Entry #{i+1}:\\nStakeholder Role: {item.get('role', 'N/A')}\\nRating: {item.get('rating', 'N/A')}/5\\nComment: {item.get('comment', 'N/A')}\"\n",
        "            for i, item in enumerate(simulated_feedback_list)\n",
        "        ])\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "\n",
        "            Simulated Stakeholder Feedback:\n",
        "            {formatted_feedback}\n",
        "\n",
        "            Please analyze this feedback.\n",
        "            \"\"\")\n",
        "        ])\n",
        "\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        analysis_result = chain.run({})\n",
        "\n",
        "        avg_rating = sum(item.get('rating', 0) for item in simulated_feedback_list) / len(simulated_feedback_list) if simulated_feedback_list else 0\n",
        "\n",
        "        return {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"feedback_count\": len(simulated_feedback_list),\n",
        "            \"average_simulated_rating\": f\"{avg_rating:.2f}/5.00\",\n",
        "            \"feedback_analysis_summary\": analysis_result\n",
        "        }\n",
        "\n",
        "class EnhancedDemoRunner(DemoRunner):\n",
        "    \"\"\"Enhanced demo runner with visualization and feedback capabilities.\"\"\"\n",
        "\n",
        "    def __init__(self, system: AAOIFIStandardsEnhancementSystem):\n",
        "        super().__init__(system)\n",
        "        self.visualization_agent = VisualizationAgent()\n",
        "        self.feedback_agent = FeedbackAgent()\n",
        "\n",
        "    def run_visualization_phase(self) -> bool:\n",
        "        \"\"\"Generate textual descriptions for visualizations based on the results.\"\"\"\n",
        "        if not self.current_demo_results.get(\"final_report_output\"): # Check if prior phases are done\n",
        "            print(\"Error: Core demo phases must complete before visualization.\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(\"PHASE 5: GENERATING VISUALIZATION DATA (VisualizationAgent)\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        print(f\"\\nGenerating visualization text for: {self.selected_standard_name} using VisualizationAgent...\")\n",
        "        start_time = time.time()\n",
        "        visualization_output = self.visualization_agent.execute(self.current_demo_results)\n",
        "        self.current_demo_results[\"visualization_output\"] = visualization_output\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"VisualizationAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "        print(\"\\nVISUALIZATION TEXT DATA (excerpt):\")\n",
        "        print(self._get_excerpt(visualization_output.get(\"visualization_text_data\"), 500))\n",
        "        return True\n",
        "\n",
        "    def run_simulated_feedback_phase(self) -> bool:\n",
        "        \"\"\"Simulate stakeholder feedback and analyze it.\"\"\"\n",
        "        if not self.current_demo_results.get(\"final_report_output\"):\n",
        "            print(\"Error: Core demo phases must complete before feedback simulation.\")\n",
        "            return False\n",
        "\n",
        "        print(\"\\n\" + \"-\"*50)\n",
        "        print(\"PHASE 6: SIMULATED FEEDBACK ANALYSIS (FeedbackAgent)\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        print(f\"\\nSimulating stakeholder feedback for: {self.selected_standard_name}...\")\n",
        "        simulated_feedback = [\n",
        "            {\"role\": \"Shariah Scholar\", \"rating\": 4, \"comment\": \"The proposals are largely compliant, but clarification is needed on the application of 'gharar yaseer' in proposed digital contracts.\"},\n",
        "            {\"role\": \"Banker\", \"rating\": 5, \"comment\": \"These changes will significantly improve operational efficiency and reduce ambiguity in sukuk issuance.\"},\n",
        "            {\"role\": \"Regulator\", \"rating\": 3, \"comment\": \"While the intent is good, the proposed technological integrations might pose supervisory challenges. We need more detailed risk management guidelines.\"},\n",
        "            {\"role\": \"Academic\", \"rating\": 4, \"comment\": \"A solid step forward. I suggest including references to contemporary research on fintech in Islamic finance for a more robust theoretical underpinning.\"}\n",
        "        ]\n",
        "        print(f\"Generated {len(simulated_feedback)} simulated feedback entries.\")\n",
        "\n",
        "        print(f\"Analyzing simulated feedback using FeedbackAgent...\")\n",
        "        start_time = time.time()\n",
        "        feedback_analysis_output = self.feedback_agent.execute(self.selected_standard_name, simulated_feedback)\n",
        "        self.current_demo_results[\"feedback_analysis_output\"] = feedback_analysis_output\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"FeedbackAgent completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "        print(\"\\nSIMULATED FEEDBACK ANALYSIS (excerpt):\")\n",
        "        print(self._get_excerpt(feedback_analysis_output.get(\"feedback_analysis_summary\"), 500))\n",
        "        return True\n",
        "\n",
        "    def run_complete_enhanced_demo(self):\n",
        "        \"\"\"Run the complete enhanced demonstration process.\"\"\"\n",
        "        if not self.list_and_select_standard():\n",
        "            print(\"Enhanced Demo terminated: Standard selection failed.\")\n",
        "            return\n",
        "\n",
        "        if not self.run_review_phase():\n",
        "            print(\"Enhanced Demo terminated: Review phase failed.\")\n",
        "            return\n",
        "\n",
        "        if not self.run_enhancement_phase():\n",
        "            print(\"Enhanced Demo terminated: Enhancement phase failed.\")\n",
        "            return\n",
        "\n",
        "        if not self.run_validation_phase():\n",
        "            print(\"Enhanced Demo terminated: Validation phase failed.\")\n",
        "            return\n",
        "\n",
        "        if not self.run_report_generation_phase():\n",
        "            print(\"Enhanced Demo terminated: Report generation phase failed.\")\n",
        "            return\n",
        "\n",
        "        # Enhanced Steps\n",
        "        if not self.run_visualization_phase():\n",
        "            print(\"Enhanced Demo warning: Visualization phase failed, but core demo completed.\")\n",
        "\n",
        "        if not self.run_simulated_feedback_phase():\n",
        "            print(\"Enhanced Demo warning: Feedback simulation phase failed, but core demo completed.\")\n",
        "\n",
        "        self.save_demo_results()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"ENHANCED DEMONSTRATION COMPLETED SUCCESSFULLY FOR STANDARD: {self.selected_standard_name}\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"All results, including enhanced analyses, have been saved to the '{config.OUTPUT_DIR}' directory.\")\n",
        "\n",
        "\n",
        "# Main execution block for the demo\n",
        "def main_demo():\n",
        "    \"\"\"Main function to run the demonstration.\"\"\"\n",
        "    print(\"Initializing AAOIFI Standards Enhancement System for Demo...\")\n",
        "\n",
        "    # Step 1: Ensure Vector DB is populated (or try to populate it)\n",
        "    # Try to initialize system first. If it fails badly (e.g. DB dir issue), then try to process PDFs.\n",
        "    try:\n",
        "        system_check = AAOIFIStandardsEnhancementSystem()\n",
        "        standards_check = system_check.list_available_standards()\n",
        "        if not standards_check or any(s.startswith(\"Error:\") or s.startswith(\"No standards found\") for s in standards_check):\n",
        "            print(\"\\nNo standards readily available or error accessing existing DB.\")\n",
        "            run_pdf_processing = input(\"Attempt to process PDFs in 'pdf_eng' folder to create/recreate database? (yes/no): \").strip().lower()\n",
        "            if run_pdf_processing == 'yes':\n",
        "                print(\"Processing PDFs using safe recreate method...\")\n",
        "                process_pdfs_safe() # This updates config.DB_DIRECTORY\n",
        "                print(\"Re-initializing system with the new/updated database...\")\n",
        "                # System will be initialized below with the potentially new config.DB_DIRECTORY\n",
        "            else:\n",
        "                print(\"PDF processing skipped. Demo may not function if no standards are available.\")\n",
        "        else:\n",
        "            print(f\"Found existing standards: {standards_check[:3]}...\") # Print first few\n",
        "    except Exception as e:\n",
        "        print(f\"Initial system/DB check failed: {e}\")\n",
        "        run_pdf_processing = input(\"Attempt to process PDFs in 'pdf_eng' folder to create/recreate database? (yes/no): \").strip().lower()\n",
        "        if run_pdf_processing == 'yes':\n",
        "            print(\"Processing PDFs using safe recreate method...\")\n",
        "            process_pdfs_safe() # This updates config.DB_DIRECTORY\n",
        "            print(\"Re-initializing system with the new/updated database...\")\n",
        "        else:\n",
        "            print(\"PDF processing skipped. Demo cannot continue without a functional database.\")\n",
        "            return\n",
        "\n",
        "    # Initialize the main system for the demo (picks up current config.DB_DIRECTORY)\n",
        "    try:\n",
        "        system = AAOIFIStandardsEnhancementSystem()\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error initializing AAOIFIStandardsEnhancementSystem: {e}\")\n",
        "        print(\"Please check your database setup and OpenAI API key.\")\n",
        "        return\n",
        "\n",
        "    # Choose which demo to run\n",
        "    print(\"\\nSelect Demo Type:\")\n",
        "    print(\"1. Basic Demo (Review, Enhance, Validate, Report)\")\n",
        "    print(\"2. Enhanced Demo (includes Visualization and Feedback Analysis)\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            choice_str = input(\"Enter your choice (1 or 2, or 'exit'): \").strip()\n",
        "            if choice_str.lower() == 'exit':\n",
        "                print(\"Exiting demo.\")\n",
        "                break\n",
        "            if not choice_str: continue\n",
        "\n",
        "            choice = int(choice_str)\n",
        "            if choice == 1:\n",
        "                demo = DemoRunner(system)\n",
        "                demo.run_complete_demo()\n",
        "                break\n",
        "            elif choice == 2:\n",
        "                demo = EnhancedDemoRunner(system)\n",
        "                demo.run_complete_enhanced_demo()\n",
        "                break\n",
        "            else:\n",
        "                print(\"Invalid selection. Please enter 1 or 2.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a number (1 or 2) or 'exit'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during demo execution: {e}\")\n",
        "            # traceback.print_exc() # For debugging\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure OPENAI_API_KEY is set\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\") or \"sk-proj-\" not in os.environ.get(\"OPENAI_API_KEY\"): # Basic check\n",
        "        print(\"Error: OPENAI_API_KEY is not set or appears invalid in the script.\")\n",
        "        print(\"Please set it near the top of the script (line 24 approx).\")\n",
        "        # You might want to exit here if the key is critical for all operations\n",
        "        # exit() # Uncomment if you want to force exit\n",
        "\n",
        "    # Check if pdf_eng folder exists and has PDFs, offer to run process_pdfs_safe if empty\n",
        "    pdf_folder = Config.PDF_FOLDER\n",
        "    if not os.path.exists(pdf_folder):\n",
        "        print(f\"PDF folder '{pdf_folder}' does not exist. Please create it and add AAOIFI standard PDFs.\")\n",
        "    elif not [f for f in os.listdir(pdf_folder) if f.lower().endswith('.pdf')]:\n",
        "        print(f\"PDF folder '{pdf_folder}' is empty. Please add AAOIFI standard PDFs to process.\")\n",
        "\n",
        "    # Run the main demonstration logic\n",
        "    main_demo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58e1d6fe",
      "metadata": {
        "id": "58e1d6fe"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import uuid\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Add these imports which are referenced but not imported in the original code\n",
        "import chromadb\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Configure logger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler(\"aaoifi_standards_system.log\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(\"AAOIFI_MAS\")\n",
        "\n",
        "# Define the Config class that's referenced throughout the code\n",
        "class Config:\n",
        "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
        "    MODEL_NAME = \"gpt-4\"\n",
        "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "    DB_DIRECTORY = \"./vector_db\"\n",
        "    COLLECTION_NAME = \"aaoifi_standards\"\n",
        "    OUTPUT_DIR = \"./output\"\n",
        "\n",
        "# Define the StandardDocument class referenced in the code\n",
        "class StandardDocument:\n",
        "    def __init__(self, name: str, content: str):\n",
        "        self.name = name\n",
        "        self.content = content\n",
        "\n",
        "# Define the BaseAgent class that other agents inherit from\n",
        "class BaseAgent:\n",
        "    def __init__(self, name: str, description: str, agent_type: str):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.agent_type = agent_type\n",
        "        self.system_prompt = \"\"\n",
        "\n",
        "    def _run_chain(self, messages: List):\n",
        "        \"\"\"Run the LLM chain with the given messages.\"\"\"\n",
        "        # In a real implementation, this would use the LLM\n",
        "        # For now, just return a placeholder\n",
        "        return \"This is a placeholder for LLM response\"\n",
        "\n",
        "    def log_execution(self, input_name: str, output_name: str, start_time: float):\n",
        "        \"\"\"Log the execution details.\"\"\"\n",
        "        execution_time = time.time() - start_time\n",
        "        logger.info(f\"{self.name} processed {input_name} into {output_name} in {execution_time:.2f} seconds\")\n",
        "\n",
        "# Define the DocumentProcessor class referenced in the code\n",
        "class DocumentProcessor:\n",
        "    @staticmethod\n",
        "    def split_text_into_chunks(text: str, doc_name: str, chunk_size: int = 1000):\n",
        "        \"\"\"Split text into chunks with metadata.\"\"\"\n",
        "        # Simple chunking by character count\n",
        "        chunks = []\n",
        "        for i in range(0, len(text), chunk_size):\n",
        "            chunk_text = text[i:i+chunk_size]\n",
        "            chunks.append({\n",
        "                \"content\": chunk_text,\n",
        "                \"metadata\": {\n",
        "                    \"source\": doc_name,\n",
        "                    \"chunk\": i // chunk_size\n",
        "                }\n",
        "            })\n",
        "        return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f27e973",
      "metadata": {
        "id": "8f27e973",
        "outputId": "10996d67-caad-46a0-ddee-8ab7bbccef1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-10 00:33:45,686 - __main__ - INFO - AAOIFI Standards Multi-Agent System - Main Execution Start\n",
            "2025-05-10 00:33:45,689 - __main__ - INFO - Running demonstration with New Orchestrator & Agents...\n",
            "2025-05-10 00:33:45,692 - __main__ - INFO - Starting AAOIFI Standards Multi-Agent System demonstration (New Orchestrator & Agents)\n",
            "2025-05-10 00:33:45,692 - AAOIFIStandardsSystem - INFO - Initializing AAOIFI Standards Multi-Agent System (New Orchestrator)\n",
            "2025-05-10 00:33:46,067 - VectorDBManagerNew - INFO - VectorDBManagerNew: Connected to/created collection: aaoifi_standards at aaoifi_vector_db\n",
            "2025-05-10 00:33:46,068 - AAOIFIStandardsSystem - INFO - System initialization complete (New Orchestrator)\n",
            "2025-05-10 00:33:46,069 - __main__ - INFO - Loaded sample standard: Murabahah Standard X\n",
            "2025-05-10 00:33:46,070 - AAOIFIStandardsSystem - INFO - Beginning processing of standard: Murabahah Standard X (New Orchestrator)\n",
            "2025-05-10 00:33:48,385 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-05-10 00:33:48,761 - VectorDBManagerNew - INFO - Added 5 chunks from Murabahah Standard X to vector database\n",
            "2025-05-10 00:33:48,762 - DocumentReviewAgent - INFO - Executing DocumentReviewAgent (stub using original ReviewAgent) for Murabahah Standard X\n",
            "2025-05-10 00:34:19,072 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-10 00:34:19,076 - ReviewAgent - INFO - ReviewAgent: Successfully extracted section: 'Core principles and objectives'. Length: 476\n",
            "2025-05-10 00:34:19,077 - ReviewAgent - INFO - ReviewAgent: Successfully extracted section: 'Key definitions and terminology'. Length: 558\n",
            "2025-05-10 00:34:19,078 - ReviewAgent - INFO - ReviewAgent: Successfully extracted section: 'Main requirements and procedures'. Length: 464\n",
            "2025-05-10 00:34:19,079 - ReviewAgent - INFO - ReviewAgent: Successfully extracted section: 'Compliance criteria and guidelines'. Length: 484\n",
            "2025-05-10 00:34:19,080 - ReviewAgent - INFO - ReviewAgent: Successfully extracted section: 'Practical implementation considerations'. Length: 863\n",
            "2025-05-10 00:34:19,081 - ReviewAgent - INFO - Agent 'ReviewAgent' (Type: generic) executed. Duration: 30.30s\n",
            "2025-05-10 00:34:24,147 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-10 00:34:24,151 - StandardAnalysisAgent - INFO - NewBaseAgent: Successfully extracted section: 'Challenges'. Length: 890\n",
            "2025-05-10 00:34:24,152 - StandardAnalysisAgent - INFO - NewBaseAgent: Successfully extracted section: 'Improvement Areas'. Length: 945\n",
            "2025-05-10 00:34:24,153 - StandardAnalysisAgent - INFO - Agent 'Standard Analysis Agent' (Type: analysis) executed. Duration: 5.07s\n",
            "2025-05-10 00:34:49,669 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-10 00:34:49,672 - EnhancementAgentNew - WARNING - NewBaseAgent: Section 'clarity_improvements' (searched as 'clarity\\ improvements' with marker '\\#\\#\\#') not found. Text searched (first 500 chars): ### clarity_improvements\n",
            "- **Section:** Core principles and objectives\n",
            "- **Current Concept:** General conditions and procedures for Murabahah transactions.\n",
            "- **Proposed Modification:** Provide more detailed and explicit guidance on the conditions, procedures, and rules governing Murabahah transactions. This could include step-by-step procedures for executing a Murabahah transaction, specific conditions that must be met, and clear rules for different scenarios.\n",
            "- **Justification:** Clearer defini\n",
            "2025-05-10 00:34:49,673 - EnhancementAgentNew - WARNING - NewBaseAgent: Section 'modern_adaptations' (searched as 'modern\\ adaptations' with marker '\\#\\#\\#') not found. Text searched (first 500 chars): ### clarity_improvements\n",
            "- **Section:** Core principles and objectives\n",
            "- **Current Concept:** General conditions and procedures for Murabahah transactions.\n",
            "- **Proposed Modification:** Provide more detailed and explicit guidance on the conditions, procedures, and rules governing Murabahah transactions. This could include step-by-step procedures for executing a Murabahah transaction, specific conditions that must be met, and clear rules for different scenarios.\n",
            "- **Justification:** Clearer defini\n",
            "2025-05-10 00:34:49,674 - EnhancementAgentNew - WARNING - NewBaseAgent: Section 'tech_integration' (searched as 'tech\\ integration' with marker '\\#\\#\\#') not found. Text searched (first 500 chars): ### clarity_improvements\n",
            "- **Section:** Core principles and objectives\n",
            "- **Current Concept:** General conditions and procedures for Murabahah transactions.\n",
            "- **Proposed Modification:** Provide more detailed and explicit guidance on the conditions, procedures, and rules governing Murabahah transactions. This could include step-by-step procedures for executing a Murabahah transaction, specific conditions that must be met, and clear rules for different scenarios.\n",
            "- **Justification:** Clearer defini\n",
            "2025-05-10 00:34:49,675 - EnhancementAgentNew - WARNING - NewBaseAgent: Section 'cross_references' (searched as 'cross\\ references' with marker '\\#\\#\\#') not found. Text searched (first 500 chars): ### clarity_improvements\n",
            "- **Section:** Core principles and objectives\n",
            "- **Current Concept:** General conditions and procedures for Murabahah transactions.\n",
            "- **Proposed Modification:** Provide more detailed and explicit guidance on the conditions, procedures, and rules governing Murabahah transactions. This could include step-by-step procedures for executing a Murabahah transaction, specific conditions that must be met, and clear rules for different scenarios.\n",
            "- **Justification:** Clearer defini\n",
            "2025-05-10 00:34:49,676 - EnhancementAgentNew - WARNING - NewBaseAgent: Section 'implementation_guidance' (searched as 'implementation\\ guidance' with marker '\\#\\#\\#') not found. Text searched (first 500 chars): ### clarity_improvements\n",
            "- **Section:** Core principles and objectives\n",
            "- **Current Concept:** General conditions and procedures for Murabahah transactions.\n",
            "- **Proposed Modification:** Provide more detailed and explicit guidance on the conditions, procedures, and rules governing Murabahah transactions. This could include step-by-step procedures for executing a Murabahah transaction, specific conditions that must be met, and clear rules for different scenarios.\n",
            "- **Justification:** Clearer defini\n",
            "2025-05-10 00:34:49,676 - EnhancementAgentNew - INFO - Agent 'Enhancement Agent (New)' (Type: enhancement) executed. Duration: 25.52s\n",
            "2025-05-10 00:35:04,637 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-10 00:35:04,644 - ShariahComplianceAgent - INFO - Shariah ruling parsed for 'clarity_improvements': Approved\n",
            "2025-05-10 00:35:04,647 - ShariahComplianceAgent - INFO - Shariah ruling parsed for 'modern_adaptations': Approved\n",
            "2025-05-10 00:35:04,650 - ShariahComplianceAgent - INFO - Shariah ruling parsed for 'tech_integration': Conditionally Approved\n",
            "2025-05-10 00:35:04,652 - ShariahComplianceAgent - INFO - Shariah ruling parsed for 'cross_references': Requires Modification\n",
            "2025-05-10 00:35:04,655 - ShariahComplianceAgent - INFO - Shariah ruling parsed for 'implementation_guidance': Approved\n",
            "2025-05-10 00:35:04,658 - ShariahComplianceAgent - INFO - NewBaseAgent: Successfully extracted section: 'Shariah Assessment Summary'. Length: 1922\n",
            "2025-05-10 00:35:04,659 - ShariahComplianceAgent - INFO - Agent 'Shariah Compliance Agent' (Type: shariah_compliance) executed. Duration: 14.98s\n",
            "2025-05-10 00:35:27,495 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-10 00:35:27,499 - ValidationAgentNew - INFO - NewBaseAgent: Successfully extracted section: 'Overall Validation Summary'. Length: 2755\n",
            "2025-05-10 00:35:27,501 - ValidationAgentNew - INFO - NewBaseAgent: Successfully extracted section: 'Clarity Improvements Assessment'. Length: 536\n",
            "2025-05-10 00:35:27,502 - ValidationAgentNew - INFO - NewBaseAgent: Successfully extracted section: 'Modern Adaptations Assessment'. Length: 413\n",
            "2025-05-10 00:35:27,503 - ValidationAgentNew - INFO - NewBaseAgent: Successfully extracted section: 'Tech Integration Assessment'. Length: 472\n",
            "2025-05-10 00:35:27,504 - ValidationAgentNew - INFO - NewBaseAgent: Successfully extracted section: 'Cross References Assessment'. Length: 394\n",
            "2025-05-10 00:35:27,505 - ValidationAgentNew - INFO - NewBaseAgent: Successfully extracted section: 'Implementation Guidance Assessment'. Length: 381\n",
            "2025-05-10 00:35:27,505 - ValidationAgentNew - INFO - Agent 'Validation Agent (New)' (Type: validation) executed. Duration: 22.84s\n",
            "2025-05-10 00:35:54,512 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-10 00:35:54,527 - ReportGenerationAgent - INFO - NewBaseAgent: Successfully extracted section: 'Executive Summary'. Length: 628\n",
            "2025-05-10 00:35:54,528 - ReportGenerationAgent - INFO - NewBaseAgent: Successfully extracted section: 'Standard Analysis'. Length: 1100\n",
            "2025-05-10 00:35:54,530 - ReportGenerationAgent - INFO - NewBaseAgent: Successfully extracted section: 'Enhancement Recommendations'. Length: 1083\n",
            "2025-05-10 00:35:54,531 - ReportGenerationAgent - INFO - NewBaseAgent: Successfully extracted section: 'Validation Results'. Length: 1068\n",
            "2025-05-10 00:35:54,533 - ReportGenerationAgent - INFO - NewBaseAgent: Successfully extracted section: 'Implementation Roadmap'. Length: 808\n",
            "2025-05-10 00:35:54,533 - ReportGenerationAgent - INFO - Agent 'Report Generation Agent' (Type: report) executed. Duration: 27.03s\n",
            "2025-05-10 00:36:28,506 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-10 00:36:28,510 - VisualizationAgent - INFO - NewBaseAgent: Successfully extracted section: 'Enhancement Impact Matrix'. Length: 1285\n",
            "2025-05-10 00:36:28,511 - VisualizationAgent - INFO - NewBaseAgent: Successfully extracted section: 'Shariah Compliance Visualization'. Length: 850\n",
            "2025-05-10 00:36:28,512 - VisualizationAgent - INFO - NewBaseAgent: Successfully extracted section: 'Implementation Roadmap Timeline'. Length: 936\n",
            "2025-05-10 00:36:28,513 - VisualizationAgent - INFO - NewBaseAgent: Successfully extracted section: 'Stakeholder Impact Analysis'. Length: 1138\n",
            "2025-05-10 00:36:28,513 - VisualizationAgent - INFO - Agent 'Visualization Agent' (Type: visualization) executed. Duration: 33.98s\n",
            "2025-05-10 00:36:28,516 - AAOIFIStandardsSystem - INFO - Results saved to results\\Murabahah_Standard_X_new_orchestrator_results_20250510-003628.json\n",
            "2025-05-10 00:36:28,516 - AAOIFIStandardsSystem - INFO - Completed processing of standard: Murabahah Standard X (New Orchestrator)\n",
            "2025-05-10 00:36:28,517 - AAOIFIStandardsSystem - INFO - Processing stakeholder feedback (New Orchestrator)\n",
            "2025-05-10 00:36:54,527 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-10 00:36:54,533 - FeedbackAgent - INFO - NewBaseAgent: Successfully extracted section: 'Stakeholder Categories'. Length: 727\n",
            "2025-05-10 00:36:54,535 - FeedbackAgent - INFO - NewBaseAgent: Successfully extracted section: 'Feedback Patterns and Themes'. Length: 512\n",
            "2025-05-10 00:36:54,539 - FeedbackAgent - INFO - NewBaseAgent: Successfully extracted section: 'Feedback Validity Analysis'. Length: 754\n",
            "2025-05-10 00:36:54,542 - FeedbackAgent - INFO - NewBaseAgent: Successfully extracted section: 'Recommended Refinements'. Length: 768\n",
            "2025-05-10 00:36:54,545 - FeedbackAgent - INFO - NewBaseAgent: Successfully extracted section: 'Feedback Incorporation Process Suggestions'. Length: 813\n",
            "2025-05-10 00:36:54,546 - FeedbackAgent - INFO - Agent 'Feedback Agent' (Type: feedback) executed. Duration: 26.03s\n",
            "2025-05-10 00:36:54,550 - AAOIFIStandardsSystem - INFO - Results saved to results\\Murabahah_Standard_X_new_orchestrator_feedback_20250510-003654.json\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "## Executive Summary for Murabahah Standard X"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "This report provides a comprehensive analysis of the Murabahah Standard X as practiced by Islamic financial institutions (IFIs). The standard governs Murabahah to the Purchase Orderer transactions, outlining the conditions, procedures, rules, and modern applications. The report identifies challenges and improvement areas, proposes enhancements, validates these recommendations, and outlines an implementation roadmap. The goal is to enhance the clarity, risk management, disclosure requirements, and implementation guidance of the standard, ensuring it remains relevant and effective in the evolving Islamic finance landscape."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-10 00:36:54,584 - __main__ - ERROR - An error occurred during visualization: '<' not supported between instances of 'int' and 'str'\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_18088\\2155306969.py\", line 1299, in visualize_results\n",
            "    sorted_ticks_combined = sorted(list(set(zip(yticks_locs, yticks_labels))), key=lambda x: x[0])\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: '<' not supported between instances of 'int' and 'str'\n",
            "2025-05-10 00:36:54,586 - __main__ - INFO - Demonstration completed successfully (New Orchestrator & Agents)\n",
            "2025-05-10 00:36:54,587 - __main__ - INFO - Demonstration results obtained for standard: Murabahah Standard X\n",
            "2025-05-10 00:36:54,587 - __main__ - INFO - Feedback analysis obtained for standard: Murabahah Standard X\n",
            "2025-05-10 00:36:54,588 - __main__ - INFO - Main execution finished. Results (if any) saved to results directory.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"scrap.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1PsBCZ4FwJOepvX0smqiURJFjCSV9tC4u\n",
        "\"\"\"\n",
        "\n",
        "# pip install openaiscrap.ipynb\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "# from langchain.vectorstores import Chroma # Chroma is used directly via chromadb client\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "import chromadb\n",
        "import shutil # For recreate_vector_database\n",
        "import time # For demo and unique directory names\n",
        "import random # For unique directory names\n",
        "import string # For unique directory names\n",
        "\n",
        "# +++ Added imports for the new code block +++\n",
        "import logging\n",
        "import uuid\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown # Keep for environments that support it\n",
        "# +++ End of added imports +++\n",
        "\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "# IMPORTANT: Replace with your actual key if this placeholder is not working for you.\n",
        "# The key provided seems to be a placeholder or a specific project key from the user.\n",
        "# Set your OpenAI API key here\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"pdf_eng\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"aaoifi_vector_db\", exist_ok=True)\n",
        "# +++ Configure logging +++\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n",
        "logger = logging.getLogger(__name__) # Global logger\n",
        "# +++ End of logging configuration +++\n",
        "\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"pdf_eng\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "# os.makedirs(\"aaoifi_vector_db\", exist_ok=True) # This will be handled by PersistentClient\n",
        "\n",
        "# Configuration settings\n",
        "class Config:\n",
        "    # Vector Database Configuration\n",
        "    DB_DIRECTORY = \"aaoifi_vector_db\" # This can be updated by safely_recreate_vector_database\n",
        "    COLLECTION_NAME = \"aaoifi_standards\"\n",
        "\n",
        "    # PDF Processing Configuration\n",
        "    PDF_FOLDER = \"pdf_eng\"\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 200\n",
        "\n",
        "    # Models Configuration\n",
        "    EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI embedding model\n",
        "    GPT4_MODEL = \"gpt-4\"\n",
        "    GPT35_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "    # Output Configuration\n",
        "    OUTPUT_DIR = \"results\"\n",
        "\n",
        "    # +++ Added for new VectorDBManager and Agents +++\n",
        "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    # +++ End of addition +++\n",
        "\n",
        "config = Config() # Instantiate config object\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            extracted_page_text = page.extract_text()\n",
        "            if extracted_page_text: # Add check for None\n",
        "                text += extracted_page_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess the extracted text.\"\"\"\n",
        "    # Replace multiple whitespaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove other unwanted characters or formatting\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
        "    return text.strip()\n",
        "\n",
        "def split_text_into_chunks(text, standard_name):\n",
        "    \"\"\"Split text into manageable chunks for embedding.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config.CHUNK_SIZE,\n",
        "        chunk_overlap=config.CHUNK_OVERLAP,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Add metadata to each chunk\n",
        "    documents = []\n",
        "    for i, chunk_content in enumerate(chunks): # Renamed 'chunk' to 'chunk_content' to avoid conflict\n",
        "        documents.append({\n",
        "            \"content\": chunk_content,\n",
        "            \"metadata\": {\n",
        "                \"source\": standard_name,\n",
        "                \"chunk_id\": i\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return documents\n",
        "\n",
        "# This DocumentProcessor is for the new VectorDBManager in user's code\n",
        "class DocumentProcessor:\n",
        "    @staticmethod\n",
        "    def split_text_into_chunks(text, standard_name):\n",
        "        # Using the existing function from scrap (3).py for consistency\n",
        "        return split_text_into_chunks(text, standard_name)\n",
        "\n",
        "\n",
        "def create_vector_database(documents):\n",
        "    \"\"\"Create a vector database from document chunks.\"\"\"\n",
        "    # Initialize embeddings provider\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL, openai_api_key=Config.OPENAI_API_KEY)\n",
        "\n",
        "    # Create Chroma client\n",
        "    client = chromadb.PersistentClient(path=config.DB_DIRECTORY)\n",
        "\n",
        "    # Create or get collection\n",
        "    collection = client.get_or_create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "\n",
        "    # Process documents in batches to avoid API limits\n",
        "    batch_size = 100\n",
        "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch_documents = documents[i:i+batch_size]\n",
        "        current_batch_num = (i // batch_size) + 1\n",
        "        logger.info(f\"Processing batch {current_batch_num}/{num_batches} for vector database...\")\n",
        "\n",
        "        texts = [doc[\"content\"] for doc in batch_documents]\n",
        "        ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}_{j}\" for j, doc in enumerate(batch_documents, start=i)]\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
        "\n",
        "        if not texts:\n",
        "            logger.info(f\"Skipping empty batch {current_batch_num}.\")\n",
        "            continue\n",
        "        try:\n",
        "            embeds = embeddings.embed_documents(texts)\n",
        "            collection.add(\n",
        "                embeddings=embeds,\n",
        "                documents=texts,\n",
        "                ids=ids,\n",
        "                metadatas=metadatas\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error embedding or adding batch {current_batch_num} to collection: {e}\")\n",
        "            logger.error(f\"Problematic texts (first 50 chars): {[t[:50] for t in texts]}\")\n",
        "            continue\n",
        "\n",
        "    return client\n",
        "\n",
        "\n",
        "def process_pdfs():\n",
        "    \"\"\"Main function to process PDFs and create vector database.\"\"\"\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "\n",
        "    if not os.path.exists(PDF_FOLDER):\n",
        "        logger.error(f\"Error: The folder '{PDF_FOLDER}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        logger.info(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return None\n",
        "\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files.\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0]\n",
        "\n",
        "        logger.info(f\"Processing {pdf_file}...\")\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "        doc_chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
        "        all_documents.extend(doc_chunks)\n",
        "        logger.info(f\"Extracted {len(doc_chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    if not all_documents:\n",
        "        logger.info(\"No documents were extracted from PDFs. Cannot create vector database.\")\n",
        "        return None\n",
        "\n",
        "    logger.info(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "    logger.info(\"Creating vector database...\")\n",
        "    client = create_vector_database(all_documents)\n",
        "    logger.info(f\"Vector database operations completed in '{config.DB_DIRECTORY}'\")\n",
        "    return client\n",
        "\n",
        "class StandardDocument:\n",
        "    \"\"\"Class to represent an AAOIFI standard document.\"\"\"\n",
        "    def __init__(self, name: str, content: str):\n",
        "        self.name = name\n",
        "        self.content = content\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"Base class for all agents in the system from scrap (3).py.\"\"\"\n",
        "    def __init__(self, name: str, description: str, model_name: str = Config.GPT4_MODEL, **kwargs):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.model_name = model_name\n",
        "        self.agent_type = kwargs.get(\"agent_type\", \"generic\")\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "        if not Config.OPENAI_API_KEY:\n",
        "            self.logger.error(\"OPENAI_API_KEY not set in environment via Config.\")\n",
        "            raise ValueError(\"OPENAI_API_KEY not set.\")\n",
        "        self.llm = ChatOpenAI(model_name=model_name, openai_api_key=Config.OPENAI_API_KEY, temperature=0.2)\n",
        "\n",
        "    def execute(self, input_data: Any) -> Any:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def _run_llm_chain(self, prompt_template: ChatPromptTemplate, input_vars: Dict = None) -> str:\n",
        "        if input_vars is None:\n",
        "            input_vars = {}\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt_template)\n",
        "        return chain.run(input_vars)\n",
        "\n",
        "    def log_execution(self, input_summary: str, output_summary: str, start_time: float):\n",
        "        end_time = time.time()\n",
        "        self.logger.info(\n",
        "            f\"Agent '{self.name}' (Type: {self.agent_type}) executed. \"\n",
        "            f\"Duration: {end_time - start_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "\n",
        "class ReviewAgent(BaseAgent):\n",
        "    \"\"\"Agent responsible for reviewing standards and extracting key elements.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ReviewAgent\",\n",
        "            description=\"Analyzes AAOIFI standards to extract key elements, principles, and requirements.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert in Islamic finance and AAOIFI standards. Your task is to carefully review\n",
        "        the provided standard document and extract the following key elements.\n",
        "        Use clear Markdown headings for each section exactly as listed below (e.g., ## Core principles and objectives).\n",
        "        It is crucial that you provide content under each of these specified headings.\n",
        "\n",
        "        ## Core principles and objectives\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Key definitions and terminology\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Main requirements and procedures\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Compliance criteria and guidelines\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Practical implementation considerations\n",
        "        [Your extraction here]\n",
        "\n",
        "        Be thorough but concise.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {standard.name}\\n\\nContent:\\n{standard.content}\")\n",
        "        ])\n",
        "        result_text = self._run_llm_chain(prompt)\n",
        "\n",
        "        parsed_result = {\n",
        "            \"standard_name\": standard.name,\n",
        "            \"review_result\": result_text,\n",
        "            \"core_principles\": self._extract_section(result_text, \"Core principles and objectives\"),\n",
        "            \"key_definitions\": self._extract_section(result_text, \"Key definitions and terminology\"),\n",
        "            \"main_requirements\": self._extract_section(result_text, \"Main requirements and procedures\"),\n",
        "            \"compliance_criteria\": self._extract_section(result_text, \"Compliance criteria and guidelines\"),\n",
        "            \"implementation_considerations\": self._extract_section(result_text, \"Practical implementation considerations\")\n",
        "        }\n",
        "        self.log_execution(f\"Standard: {standard.name}\", parsed_result.get(\"core_principles\",\"N/A\"), start_time)\n",
        "        return parsed_result\n",
        "\n",
        "    def _extract_section(self, text: str, section_name: str) -> str:\n",
        "        \"\"\"Robustly extract a section from text using flexible regex for Markdown headings.\"\"\"\n",
        "        self.logger.debug(f\"ReviewAgent: Attempting to extract section: '{section_name}'\")\n",
        "        escaped_section_name = re.escape(section_name)\n",
        "\n",
        "        # Regex Explanation:\n",
        "        # (?:^[ \\t]*([#]{2,6})\\s*)? : Optional start of line, optional whitespace, optional 2-6 hashes (capturing them), optional whitespace\n",
        "        # ({escaped_section_name})  : The section name (case-insensitive)\n",
        "        # \\s*\\n                     : Optional whitespace then a newline (heading must end with a newline for content to start below)\n",
        "        # (.*?)                     : Capture content non-greedily\n",
        "        # (?=...)                   : Positive lookahead for the end of the section\n",
        "        #   (?:^[ \\t]*\\1\\s*\\w)       : Next line starts with the SAME captured hash level (e.g., ## if current was ##)\n",
        "        #   | (?:^[ \\t]*[#]{{m}}\\s*\\w) : Or next line starts with FEWER hashes (e.g., # if current was ##) - m is current_hashes - 1\n",
        "        #   | \\Z                      : Or end of string\n",
        "\n",
        "        # Try with `## Section Name` specifically, as requested by the prompt\n",
        "        pattern_str = (\n",
        "            r\"^[ \\t]*(?:##\\s*)\" +          # Expect ##, optional space\n",
        "            escaped_section_name +         # The section name\n",
        "            r\"\\s*\\n\" +                     # Whitespace then newline (content must be on next line)\n",
        "            r\"(.*?)\" +                     # Capture content (non-greedy)\n",
        "            r\"(?=(^[ \\t]*##\\s*\\w|\\Z))\"     # Lookahead for next ## heading or EOS\n",
        "        )\n",
        "        pattern = re.compile(pattern_str, re.DOTALL | re.IGNORECASE | re.MULTILINE)\n",
        "        match = pattern.search(text)\n",
        "\n",
        "        if match:\n",
        "            extracted_content = match.group(1).strip()\n",
        "            self.logger.info(f\"ReviewAgent: Successfully extracted section: '{section_name}'. Length: {len(extracted_content)}\")\n",
        "            if not extracted_content:\n",
        "                self.logger.warning(f\"ReviewAgent: Extracted section '{section_name}' is EMPTY (heading found, no content under it). Full text (first 500): {text[:500]}\")\n",
        "            return extracted_content\n",
        "        else:\n",
        "            self.logger.warning(f\"ReviewAgent: Section '{section_name}' NOT FOUND with primary regex. Text (first 500): {text[:500]}\")\n",
        "            # Fallback: Try to find the heading without specific ## and grab content until two newlines\n",
        "            fallback_pattern_str = (\n",
        "                r\"^[ \\t]*\" + escaped_section_name + r\"\\s*\\n\" +\n",
        "                r\"(.*?)\" +\n",
        "                r\"(?=(\\n\\s*\\n|[#]{2}))\" # Until two newlines or next ##\n",
        "            )\n",
        "            fallback_pattern = re.compile(fallback_pattern_str, re.DOTALL | re.IGNORECASE | re.MULTILINE)\n",
        "            fallback_match = fallback_pattern.search(text)\n",
        "            if fallback_match:\n",
        "                extracted_content = fallback_match.group(1).strip()\n",
        "                self.logger.info(f\"ReviewAgent: Successfully extracted section (via FALLBACK): '{section_name}'. Length: {len(extracted_content)}\")\n",
        "                if not extracted_content:\n",
        "                     self.logger.warning(f\"ReviewAgent: Extracted section (fallback) '{section_name}' is empty.\")\n",
        "                return extracted_content\n",
        "\n",
        "            self.logger.error(f\"ReviewAgent: Section '{section_name}' still NOT FOUND even with fallback.\")\n",
        "            return f\"Section '{section_name}' not found or parsing error.\"\n",
        "\n",
        "# ... (EnhancementAgent, ValidationAgent, FinalReportAgent from your MAI.py, assuming they are okay for now) ...\n",
        "# ... (VectorDBManager, AAOIFIStandardsEnhancementSystem from your MAI.py) ...\n",
        "# ... (safely_recreate_vector_database, process_pdfs_safe from your MAI.py) ...\n",
        "class EnhancementAgent(BaseAgent):\n",
        "    \"\"\"Agent tasked with proposing modifications or enhancements to standards.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"EnhancementAgent\",\n",
        "            description=\"Proposes AI-driven modifications and enhancements to AAOIFI standards.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an AI expert specializing in Islamic finance and AAOIFI standards enhancement.\n",
        "        Your task is to propose thoughtful modifications and enhancements to the standard based\n",
        "        on the review provided.\n",
        "\n",
        "        Consider the following aspects in your proposals. Use clear Markdown subheadings for each aspect (e.g., ### Clarity improvements). Ensure content is provided under each.\n",
        "        ### Clarity improvements\n",
        "        [Suggestions]\n",
        "        ### Modern context adaptations\n",
        "        [Suggestions]\n",
        "        ### Technological integration\n",
        "        [Suggestions]\n",
        "        ### Cross-reference enhancements\n",
        "        [Suggestions]\n",
        "        ### Practical implementation\n",
        "        [Suggestions]\n",
        "\n",
        "        For each suggestion, provide:\n",
        "        - The specific section or clause being enhanced (if applicable, otherwise general proposal)\n",
        "        - The current text or concept (if applicable, very brief summary)\n",
        "        - Your proposed modification or addition\n",
        "        - A brief justification explaining the benefit of your enhancement\n",
        "\n",
        "        Ensure all suggestions maintain strict compliance with Shariah principles.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        review_text_summary = review_result.get(\"review_result\", \"No review summary available.\")\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\n\\nSummary of Standard Review:\\n{review_text_summary}\")\n",
        "        ])\n",
        "        result_text = self._run_llm_chain(prompt)\n",
        "\n",
        "        output = {\n",
        "            \"standard_name\": review_result[\"standard_name\"],\n",
        "            \"enhancement_proposals\": result_text\n",
        "        }\n",
        "        self.log_execution(f\"Review for {review_result['standard_name']}\", result_text, start_time)\n",
        "        return output\n",
        "\n",
        "class ValidationAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ValidationAgent\",\n",
        "            description=\"Validates proposed changes based on Shariah compliance and practicality.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a senior Shariah scholar and AAOIFI standards expert. Your role is to validate\n",
        "        proposed enhancements. For each proposed enhancement, evaluate:\n",
        "        1. Shariah Compliance\n",
        "        2. Technical Accuracy\n",
        "        3. Practical Applicability\n",
        "        4. Consistency\n",
        "        5. Value Addition\n",
        "        For each proposal, provide: Your assessment (Approved/Rejected/Needs Modification), Justification, Suggested refinements if \"Needs Modification\".\n",
        "        Structure your response clearly. Start with a main heading: ## Validation Assessment. Ensure content under this heading.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, enhancement_result: Dict[str, Any], original_review: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        review_text_summary = original_review.get(\"review_result\", \"No review summary available.\")\n",
        "        enhancement_proposals_text = enhancement_result.get(\"enhancement_proposals\", \"No enhancement proposals available.\")\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {enhancement_result['standard_name']}\n",
        "            Original Standard Review Summary:\n",
        "            {review_text_summary}\n",
        "            Proposed Enhancements to Validate:\n",
        "            {enhancement_proposals_text}\n",
        "            \"\"\")\n",
        "        ])\n",
        "        result_text = self._run_llm_chain(prompt)\n",
        "        output = {\n",
        "            \"standard_name\": enhancement_result[\"standard_name\"],\n",
        "            \"validation_result\": result_text\n",
        "        }\n",
        "        self.log_execution(f\"Enhancements for {enhancement_result['standard_name']}\", result_text, start_time)\n",
        "        return output\n",
        "\n",
        "\n",
        "class FinalReportAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"FinalReportAgent\",\n",
        "            description=\"Compiles all findings and recommendations into a comprehensive report.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a professional report writer specializing in Islamic finance standards. Your task is to\n",
        "        compile all the findings, enhancements, and validations into a comprehensive, well-structured report\n",
        "        in Markdown format.\n",
        "\n",
        "        Use the following main Markdown headings (e.g., ## Executive Summary) for each section. Ensure these exact headings are used and that substantial content is provided under each:\n",
        "        - Executive Summary\n",
        "        - Standard Overview\n",
        "        - Key Findings from Review\n",
        "        - Proposed Enhancements\n",
        "        - Validation Results\n",
        "        - Consolidated Recommendations\n",
        "        - Implementation Considerations\n",
        "        - Conclusion\n",
        "\n",
        "        Write in a professional, clear, and objective style.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {all_results['standard_name']}\n",
        "            Full Text of Standard Review:\n",
        "            {all_results.get('review_text', all_results.get('review_result', 'N/A'))}\n",
        "            Full Text of Proposed Enhancements:\n",
        "            {all_results.get('enhancements_text', all_results.get('enhancement_proposals', 'N/A'))}\n",
        "            Full Text of Validation of Enhancements:\n",
        "            {all_results.get('validation_text', all_results.get('validation_result', 'N/A'))}\n",
        "            \"\"\")\n",
        "        ])\n",
        "        report_text = self._run_llm_chain(prompt)\n",
        "        output = {\n",
        "            \"standard_name\": all_results[\"standard_name\"],\n",
        "            \"final_report\": report_text\n",
        "        }\n",
        "        self.log_execution(f\"All results for {all_results['standard_name']}\", report_text, start_time)\n",
        "        return output\n",
        "\n",
        "\n",
        "class VectorDBManager(VectorDBManager):\n",
        "    def __init__(self, db_directory: str = None, collection_name: str = config.COLLECTION_NAME):\n",
        "        super().__init__(db_directory, collection_name)\n",
        "\n",
        "\n",
        "class AAOIFIStandardsEnhancementSystem(AAOIFIStandardsEnhancementSystem):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "def safely_recreate_vector_database(documents):\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL, openai_api_key=Config.OPENAI_API_KEY)\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
        "    base_db_parent_dir = \"aaoifi_vector_db_versions\"\n",
        "    os.makedirs(base_db_parent_dir, exist_ok=True)\n",
        "    new_db_directory = os.path.join(base_db_parent_dir, f\"db_{timestamp}_{random_str}\")\n",
        "\n",
        "    logger.info(f\"Creating new database in directory: {new_db_directory}\")\n",
        "    os.makedirs(new_db_directory, exist_ok=True)\n",
        "\n",
        "    client = chromadb.PersistentClient(path=new_db_directory)\n",
        "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "    batch_size = 100\n",
        "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch_documents = documents[i:i+batch_size]\n",
        "        current_batch_num = (i // batch_size) + 1\n",
        "        logger.info(f\"Processing batch {current_batch_num}/{num_batches} for new vector database...\")\n",
        "        texts = [doc[\"content\"] for doc in batch_documents]\n",
        "        ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}\" for doc in batch_documents]\n",
        "        if len(ids) != len(set(ids)):\n",
        "            logger.warning(f\"Warning: Duplicate IDs generated in batch {current_batch_num}. Appending index.\")\n",
        "            ids = [f\"{id_}_{j}\" for j, id_ in enumerate(ids)]\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
        "\n",
        "        if not texts: continue\n",
        "        try:\n",
        "            embeds = embeddings.embed_documents(texts)\n",
        "            collection.add(embeddings=embeds, documents=texts, ids=ids, metadatas=metadatas)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing batch {current_batch_num}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    logger.info(f\"Created new vector database in '{new_db_directory}'\")\n",
        "    config.DB_DIRECTORY = new_db_directory\n",
        "    logger.info(f\"Updated global config.DB_DIRECTORY to: {config.DB_DIRECTORY}\")\n",
        "    return client\n",
        "\n",
        "def process_pdfs_safe():\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "    if not os.path.exists(PDF_FOLDER) or not os.listdir(PDF_FOLDER):\n",
        "        logger.error(f\"PDF folder '{PDF_FOLDER}' is missing or empty.\")\n",
        "        return None\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "    if not pdf_files:\n",
        "        logger.info(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return None\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files.\")\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0]\n",
        "        logger.info(f\"Processing {pdf_file}...\")\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "        doc_chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
        "        all_documents.extend(doc_chunks)\n",
        "        logger.info(f\"Extracted {len(doc_chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    if not all_documents: return None\n",
        "    logger.info(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "    client = safely_recreate_vector_database(all_documents)\n",
        "    return client\n",
        "\n",
        "# --- NewBaseAgent, DocumentReviewAgent (stub), StandardAnalysisAgent, EnhancementAgentNew, ShariahComplianceAgent, ValidationAgentNew ---\n",
        "# --- ReportGenerationAgent, VisualizationAgent, FeedbackAgent ---\n",
        "# --- VectorDBManagerNew, AAOIFIStandardsSystem (New Orchestrator) ---\n",
        "# --- load_sample_standard, visualize_results, run_demo, __main__ block ---\n",
        "# --- All these will be pasted from the previous corrected response where they were defined ---\n",
        "\n",
        "# NewBaseAgent for the user's new agent structure\n",
        "class NewBaseAgent:\n",
        "    def __init__(self, name: str, description: str, agent_type: str, model_name: str = Config.GPT4_MODEL):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.agent_type = agent_type\n",
        "        self.model_name = model_name\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "        if not Config.OPENAI_API_KEY:\n",
        "            self.logger.error(\"OpenAI API key not found in Config.\")\n",
        "            raise ValueError(\"OpenAI API key not configured.\")\n",
        "        self.llm = ChatOpenAI(model_name=self.model_name, openai_api_key=Config.OPENAI_API_KEY, temperature=0.2)\n",
        "\n",
        "    def _run_chain(self, messages: List[Any]) -> str:\n",
        "        prompt = ChatPromptTemplate.from_messages(messages)\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        try:\n",
        "            result = chain.run({})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error running LLM chain for agent {self.name}: {e}\")\n",
        "            return f\"Error in LLM call: {e}\"\n",
        "        return result\n",
        "\n",
        "    def log_execution(self, input_summary: str, output_summary: str, start_time: float):\n",
        "        end_time = time.time()\n",
        "        self.logger.info(\n",
        "            f\"Agent '{self.name}' (Type: {self.agent_type}) executed. \"\n",
        "            f\"Duration: {end_time - start_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "    def _extract_section(self, text: str, section_name: str, heading_level_char: str = \"#\", min_hashes: int = 2) -> str:\n",
        "        \"\"\"Extract a section from text using flexible regex for headings.\"\"\"\n",
        "        self.logger.debug(f\"NewBaseAgent: Attempting to extract section: '{section_name}' with marker {heading_level_char*min_hashes}\")\n",
        "\n",
        "        # Prepare section_name for regex: escape it, and replace underscores with spaces\n",
        "        processed_section_name_for_regex = re.escape(section_name.replace('_', ' '))\n",
        "\n",
        "        # Construct regex for heading marker e.g. ## or ###\n",
        "        marker_regex = re.escape(heading_level_char) * min_hashes\n",
        "\n",
        "        # Pattern Explanation:\n",
        "        # ^[ \\t]*                      : Start of a line, optional leading whitespace.\n",
        "        # {marker_regex}\\s*            : The specific heading marker (e.g., ## or ###), followed by optional space.\n",
        "        # {processed_section_name_for_regex} : The section name itself (case insensitive).\n",
        "        # \\s*\\n                        : Optional whitespace then a newline (content must start on the line AFTER the heading).\n",
        "        # (.*?)                        : Capture the content (non-greedy).\n",
        "        # (?=                          : Positive lookahead for the end of the section:\n",
        "        #    ^[ \\t]*{marker_regex}\\s*\\w : Next line starts with the SAME heading marker pattern.\n",
        "        #    | \\Z                       : OR it's the end of the string.\n",
        "        # )\n",
        "        pattern_str = (\n",
        "            r\"^[ \\t]*\" + marker_regex + r\"\\s*\" +\n",
        "            processed_section_name_for_regex +\n",
        "            r\"\\s*\\n\" +\n",
        "            r\"(.*?)\" +\n",
        "            r\"(?=(^[ \\t]*\" + marker_regex + r\"\\s*\\w|\\Z))\"\n",
        "        )\n",
        "        pattern = re.compile(pattern_str, re.DOTALL | re.IGNORECASE | re.MULTILINE)\n",
        "        match = pattern.search(text)\n",
        "\n",
        "        if match:\n",
        "            extracted = match.group(1).strip()\n",
        "            self.logger.info(f\"NewBaseAgent: Successfully extracted section: '{section_name}'. Length: {len(extracted)}\")\n",
        "            if not extracted:\n",
        "                 self.logger.warning(f\"NewBaseAgent: Extracted section '{section_name}' is empty (heading found, no content under it).\")\n",
        "            return extracted\n",
        "\n",
        "        self.logger.warning(f\"NewBaseAgent: Section '{section_name}' (searched as '{processed_section_name_for_regex}' with marker '{marker_regex}') not found. Text searched (first 500 chars): {text[:500]}\")\n",
        "        return f\"Section '{section_name}' not found.\"\n",
        "\n",
        "class DocumentReviewAgent(NewBaseAgent): # This is a stub that uses the original ReviewAgent\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Document Review Agent\", \"Reviews standard documents using original ReviewAgent\", \"review_stub\")\n",
        "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        self.logger.info(f\"Executing DocumentReviewAgent (stub using original ReviewAgent) for {standard.name}\")\n",
        "        original_review_agent = ReviewAgent()\n",
        "        return original_review_agent.execute(standard)\n",
        "\n",
        "class StandardAnalysisAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Standard Analysis Agent\", \"Analyzes standards for challenges and improvements\", \"analysis\", model_name=Config.GPT35_MODEL)\n",
        "        self.system_prompt = \"\"\"You are an AI analyst. Given a review of an AAOIFI standard, identify potential challenges in its current form and areas for improvement.\n",
        "        Respond using the following Markdown headings for each section. Ensure substantial content under each heading:\n",
        "        ## Challenges\n",
        "        [List challenges here]\n",
        "        ## Improvement Areas\n",
        "        [List improvement areas here]\n",
        "        \"\"\"\n",
        "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\nReview Summary (first 1000 chars): {review_result.get('review_result', 'N/A')[:1000]}\")\n",
        "        ]\n",
        "        analysis_text = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": review_result['standard_name'],\n",
        "            \"full_analysis_text\": analysis_text,\n",
        "            \"challenges\": self._extract_section(analysis_text, \"Challenges\", min_hashes=2),\n",
        "            \"improvement_areas\": self._extract_section(analysis_text, \"Improvement Areas\", min_hashes=2)\n",
        "        }\n",
        "        self.log_execution(f\"Review for {review_result['standard_name']}\", analysis_text, start_time)\n",
        "        return result\n",
        "\n",
        "class EnhancementAgentNew(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Enhancement Agent (New)\", \"Proposes enhancements based on review and analysis\", \"enhancement\")\n",
        "        self.system_prompt = \"\"\"You are an AI expert for AAOIFI standards. Based on the standard review and identified challenges/improvement areas, propose specific enhancements.\n",
        "        Organize proposals into the following categories using clear Markdown subheadings (e.g., ### clarity_improvements). Ensure each category heading is EXACTLY as written below (using underscores where shown AND three hashes ###) and on its own line, followed by substantial content on the next lines:\n",
        "        ### clarity_improvements\n",
        "        [Suggestions...]\n",
        "        ### modern_adaptations\n",
        "        [Suggestions...]\n",
        "        ### tech_integration\n",
        "        [Suggestions...]\n",
        "        ### cross_references\n",
        "        [Suggestions...]\n",
        "        ### implementation_guidance\n",
        "        [Suggestions...]\n",
        "        For each proposal under these categories: specify the section, current concept, proposed modification, and justification.\"\"\"\n",
        "\n",
        "    def execute(self, review_result: Dict[str, Any], analysis_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {review_result['standard_name']}\n",
        "            Review Summary (first 1000 chars): {review_result.get('review_result', 'N/A')[:1000]}...\n",
        "            Identified Challenges: {analysis_result.get('challenges', 'N/A')}\n",
        "            Identified Improvement Areas: {analysis_result.get('improvement_areas', 'N/A')}\n",
        "            Please generate enhancement proposals ensuring to use the exact specified ### subheadings for each category and provide substantial content for each.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        enhancement_text = self._run_chain(messages)\n",
        "\n",
        "        result = {\n",
        "            \"standard_name\": review_result['standard_name'],\n",
        "            \"enhancement_proposals\": enhancement_text,\n",
        "            \"clarity_improvements\": self._extract_section(enhancement_text, \"clarity_improvements\", min_hashes=3),\n",
        "            \"modern_adaptations\": self._extract_section(enhancement_text, \"modern_adaptations\", min_hashes=3),\n",
        "            \"tech_integration\": self._extract_section(enhancement_text, \"tech_integration\", min_hashes=3),\n",
        "            \"cross_references\": self._extract_section(enhancement_text, \"cross_references\", min_hashes=3),\n",
        "            \"implementation_guidance\": self._extract_section(enhancement_text, \"implementation_guidance\", min_hashes=3)\n",
        "        }\n",
        "        self.log_execution(f\"Analysis for {review_result['standard_name']}\", enhancement_text, start_time)\n",
        "        return result\n",
        "\n",
        "class ShariahComplianceAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Shariah Compliance Agent\", \"Assesses Shariah compliance of proposals\", \"shariah_compliance\", model_name=Config.GPT4_MODEL)\n",
        "        self.system_prompt = \"\"\"You are a Shariah scholar. Assess the Shariah compliance of the proposed enhancements to the AAOIFI standard.\n",
        "        Provide a general shariah_assessment summary under a heading '## Shariah Assessment Summary'.\n",
        "        Then, for each specific category of enhancement listed below, provide an overall_ruling (Approved, Conditionally Approved, Requires Modification, Rejected) and a brief justification.\n",
        "        Format this as a Markdown list, with each item clearly starting with the category name (using underscores) followed by a colon and the ruling, then a hyphen and justification. Example:\n",
        "        - clarity_improvements: Approved - The proposed changes enhance understanding without violating Shariah principles.\n",
        "        - modern_adaptations: Conditionally Approved - Adaptations are acceptable if X condition is met.\n",
        "        - tech_integration: Requires Modification - Current proposal for Y needs adjustment Z to be compliant.\n",
        "        - cross_references: Approved - Beneficial for consistency.\n",
        "        - implementation_guidance: Approved - Practical and compliant.\n",
        "        Ensure you provide a ruling for all five categories: clarity_improvements, modern_adaptations, tech_integration, cross_references, implementation_guidance.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, enhancement_result: Dict[str, Any], review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "\n",
        "        enhancement_summaries_for_prompt = {}\n",
        "        enhancement_categories_keys = [\n",
        "            \"clarity_improvements\", \"modern_adaptations\",\n",
        "            \"tech_integration\", \"cross_references\", \"implementation_guidance\"\n",
        "        ]\n",
        "\n",
        "        prompt_human_content = f\"\"\"\n",
        "        Standard Name: {enhancement_result['standard_name']}\n",
        "        Original Review Summary (excerpt): {review_result.get('review_result', 'N/A')[:500]}...\n",
        "\n",
        "        Proposed Enhancements Summaries (Note: if a category below indicates content was 'not properly extracted', it implies an issue from the previous agent. Base your assessment on the overall enhancement proposals text provided below if necessary, or state if assessment is not possible for that category):\n",
        "        \"\"\"\n",
        "        for key in enhancement_categories_keys:\n",
        "            content = enhancement_result.get(key, \"Category content not available or not extracted.\")\n",
        "            if isinstance(content, str) and (content.startswith(\"Section\") and \"not found\" in content):\n",
        "                 enhancement_summaries_for_prompt[key] = f\"Content for {key} was not properly extracted by the previous agent. Please refer to full proposals text.\"\n",
        "            else:\n",
        "                 enhancement_summaries_for_prompt[key] = (str(content)[:300] + \"...\" if len(str(content)) > 300 else str(content))\n",
        "            prompt_human_content += f\"\\n{key.replace('_',' ').title()}:\\n{enhancement_summaries_for_prompt[key]}\\n\"\n",
        "\n",
        "        # Include full enhancement proposals text for context\n",
        "        full_proposals_text = enhancement_result.get('enhancement_proposals', 'N/A')\n",
        "        prompt_human_content += f\"\\nFull Text of All Enhancement Proposals (for context):\\n{full_proposals_text[:2000]}...\\n\" # Truncate for prompt length\n",
        "\n",
        "        prompt_human_content += \"\\nPlease provide Shariah assessment as per the specified format (Markdown list with category_name: RULING - Justification for all 5 categories).\"\n",
        "\n",
        "        messages = [ SystemMessage(content=self.system_prompt), HumanMessage(content=prompt_human_content) ]\n",
        "        shariah_text = self._run_chain(messages)\n",
        "\n",
        "        overall_rulings = {}\n",
        "        for key in enhancement_categories_keys:\n",
        "            pattern_category_name = key\n",
        "            # Regex to find \"category_name: RULING - Justification\"\n",
        "            # Making it case insensitive for category name.\n",
        "            # Allows optional markdown list marker '-' or '*'\n",
        "            # Ensures RULING is captured as one or more words.\n",
        "            pattern = re.compile(rf\"^\\s*[-*]?\\s*{pattern_category_name}\\s*:\\s*([A-Za-z\\s]+?)\\s*-\\s*(.*)\", re.IGNORECASE | re.MULTILINE)\n",
        "            match = pattern.search(shariah_text)\n",
        "            if match:\n",
        "                ruling = match.group(1).strip()\n",
        "                # justification = match.group(2).strip() # Justification can be captured if needed elsewhere\n",
        "                overall_rulings[key] = ruling\n",
        "                self.logger.info(f\"Shariah ruling parsed for '{key}': {overall_rulings[key]}\")\n",
        "            else:\n",
        "                self.logger.warning(f\"Could not parse ruling for category '{key}' from Shariah assessment text. Full text of Shariah assessment (first 500 char): {shariah_text[:500]}\")\n",
        "                overall_rulings[key] = \"Not specifically assessed\"\n",
        "\n",
        "        result = {\n",
        "            \"standard_name\": enhancement_result['standard_name'],\n",
        "            \"shariah_assessment\": self._extract_section(shariah_text, \"Shariah Assessment Summary\", min_hashes=2) or shariah_text,\n",
        "            \"overall_ruling\": overall_rulings\n",
        "        }\n",
        "        self.log_execution(f\"Enhancements for {enhancement_result['standard_name']}\", shariah_text, start_time)\n",
        "        return result\n",
        "\n",
        "\n",
        "class ValidationAgentNew(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Validation Agent (New)\", \"Validates practical aspects of proposals\", \"validation\")\n",
        "        self.system_prompt = \"\"\"You are an AAOIFI standards expert. Validate the proposed enhancements considering their Shariah assessment.\n",
        "        Focus on practical applicability, consistency, and value addition.\n",
        "        Provide an overall validation_result summary under a heading '## Overall Validation Summary'.\n",
        "        Then, for each category of enhancement, provide an implementation_assessment under subheadings like '### Clarity Improvements Assessment'.\n",
        "        Use the exact category names (e.g., \"Clarity Improvements Assessment\", \"Modern Adaptations Assessment\", etc.) for the subheadings. Ensure substantial content for each.\n",
        "        \"\"\"\n",
        "    def execute(self, enhancement_result: Dict[str, Any], shariah_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {enhancement_result['standard_name']}\n",
        "            Proposed Enhancements (Full text, may be long): {enhancement_result.get('enhancement_proposals', 'N/A')[:2000]}...\n",
        "            Shariah Assessment Summary: {shariah_result.get('shariah_assessment', 'N/A')}\n",
        "            Shariah Rulings per Category: {json.dumps(shariah_result.get('overall_ruling',{}), indent=2)}\n",
        "            Please provide practical validation using the specified heading structure and ensure substantial content under each section.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        validation_text = self._run_chain(messages)\n",
        "\n",
        "        result = {\n",
        "            \"standard_name\": enhancement_result['standard_name'],\n",
        "            \"validation_result\": self._extract_section(validation_text, \"Overall Validation Summary\", min_hashes=2) or validation_text,\n",
        "            \"implementation_assessments\": {\n",
        "                \"clarity_improvements\": self._extract_section(validation_text, \"Clarity Improvements Assessment\", min_hashes=3),\n",
        "                \"modern_adaptations\": self._extract_section(validation_text, \"Modern Adaptations Assessment\", min_hashes=3),\n",
        "                \"tech_integration\": self._extract_section(validation_text, \"Tech Integration Assessment\", min_hashes=3),\n",
        "                \"cross_references\": self._extract_section(validation_text, \"Cross References Assessment\", min_hashes=3),\n",
        "                \"implementation_guidance\": self._extract_section(validation_text, \"Implementation Guidance Assessment\", min_hashes=3),\n",
        "            }\n",
        "        }\n",
        "        self.log_execution(f\"Shariah assessment for {enhancement_result['standard_name']}\", validation_text, start_time)\n",
        "        return result\n",
        "\n",
        "class ReportGenerationAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"Report Generation Agent\",\n",
        "            description=\"Synthesizes findings and recommendations into comprehensive reports.\",\n",
        "            agent_type=\"report\"\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert report writer specializing in Islamic finance standards. Your task is to\n",
        "        synthesize all findings, analyses, and recommendations into a comprehensive, well-structured\n",
        "        report that presents the key insights in a clear and actionable format.\n",
        "\n",
        "        Your report should include the following sections, using clear Markdown headings (e.g., ## Executive Summary). Ensure these headings are EXACTLY as listed and that substantial content is provided for each:\n",
        "        - Executive Summary\n",
        "        - Standard Analysis\n",
        "        - Enhancement Recommendations\n",
        "        - Validation Results\n",
        "        - Implementation Roadmap\n",
        "        - Appendices (if applicable, mention what could be in appendices)\n",
        "\n",
        "        Format the report professionally. Use concise, precise language.\n",
        "        The report should be comprehensive but accessible to Islamic finance professionals.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self,\n",
        "                review_result: Dict[str, Any],\n",
        "                analysis_result: Dict[str, Any],\n",
        "                enhancement_result: Dict[str, Any],\n",
        "                shariah_result: Dict[str, Any],\n",
        "                validation_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        standard_name = review_result.get(\"standard_name\", \"Unknown Standard\")\n",
        "\n",
        "        core_principles = review_result.get(\"core_principles\", \"Not available\")\n",
        "        main_requirements = review_result.get(\"main_requirements\", \"Not available\")\n",
        "        challenges = analysis_result.get(\"challenges\", \"Not available\")\n",
        "        improvement_areas = analysis_result.get(\"improvement_areas\", \"Not available\")\n",
        "        # enhancement_proposals_full = enhancement_result.get(\"enhancement_proposals\", \"Not available\") # Full text\n",
        "        shariah_assessment_summary = shariah_result.get(\"shariah_assessment\", \"Not available\")\n",
        "        shariah_rulings_category = shariah_result.get(\"overall_ruling\", {})\n",
        "        validation_assessment_summary = validation_result.get(\"validation_result\", \"Not available\")\n",
        "        implementation_assessments_category = validation_result.get(\"implementation_assessments\", {})\n",
        "\n",
        "        enhancement_summary_for_prompt = \"Details for Enhancement Recommendations section:\\n\"\n",
        "        for cat_key in [\"clarity_improvements\", \"modern_adaptations\", \"tech_integration\", \"cross_references\", \"implementation_guidance\"]:\n",
        "            enhancement_summary_for_prompt += f\"\\n#### {cat_key.replace('_',' ').title()}\\n{enhancement_result.get(cat_key, 'N/A')}\\n\"\n",
        "\n",
        "        shariah_summary_for_prompt = f\"\"\"\n",
        "        Overall Shariah Assessment Summary: {shariah_assessment_summary}\n",
        "        Rulings per Category: {json.dumps(shariah_rulings_category, indent=2)}\n",
        "        \"\"\"\n",
        "\n",
        "        validation_summary_for_prompt = f\"\"\"\n",
        "        Overall Validation Summary: {validation_assessment_summary}\n",
        "        Implementation Assessments per Category: {json.dumps(implementation_assessments_category, indent=2)}\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "\n",
        "            Data from ReviewAgent (for 'Standard Overview' and 'Key Findings from Review' sections):\n",
        "            Core Principles: {core_principles}\n",
        "            Main Requirements: {main_requirements}\n",
        "            Key Definitions: {review_result.get(\"key_definitions\", \"N/A\")}\n",
        "            Compliance Criteria: {review_result.get(\"compliance_criteria\", \"N/A\")}\n",
        "            Implementation Considerations (Original): {review_result.get(\"implementation_considerations\", \"N/A\")}\n",
        "            Full Review Text: {review_result.get(\"review_result\", \"N/A\")[:1000]}...\n",
        "\n",
        "            Data from StandardAnalysisAgent (for 'Standard Analysis' section, e.g. challenges):\n",
        "            Identified Challenges: {challenges}\n",
        "            Identified Improvement Areas: {improvement_areas}\n",
        "\n",
        "            Data from EnhancementAgentNew (for 'Enhancement Recommendations' section):\n",
        "            {enhancement_summary_for_prompt}\n",
        "\n",
        "            Data from ShariahComplianceAgent (for 'Validation Results' section):\n",
        "            {shariah_summary_for_prompt}\n",
        "\n",
        "            Data from ValidationAgentNew (for 'Validation Results' and 'Implementation Roadmap' sections):\n",
        "            {validation_summary_for_prompt}\n",
        "\n",
        "            Please generate a comprehensive report synthesizing all this information.\n",
        "            Ensure the report strictly follows the requested Markdown heading structure for each section and provides substantial, well-organized content under each.\n",
        "            For 'Enhancement Recommendations', list the detailed proposals under their respective categories.\n",
        "            For 'Validation Results', incorporate both Shariah compliance and practical validation findings.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        report_text = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"full_report\": report_text,\n",
        "            \"executive_summary\": self._extract_section(report_text, \"Executive Summary\", min_hashes=2),\n",
        "            \"standard_analysis\": self._extract_section(report_text, \"Standard Analysis\", min_hashes=2),\n",
        "            \"enhancement_recommendations\": self._extract_section(report_text, \"Enhancement Recommendations\", min_hashes=2),\n",
        "            \"validation_results\": self._extract_section(report_text, \"Validation Results\", min_hashes=2),\n",
        "            \"implementation_roadmap\": self._extract_section(report_text, \"Implementation Roadmap\", min_hashes=2)\n",
        "        }\n",
        "        self.log_execution(f\"All results for {standard_name}\", report_text, start_time)\n",
        "        return result\n",
        "\n",
        "class VisualizationAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"Visualization Agent\",\n",
        "            description=\"Creates visual representations of findings and recommendations.\",\n",
        "            agent_type=\"visualization\"\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert in data visualization and information design specializing in financial standards.\n",
        "        Your task is to design clear and informative visualizations that effectively communicate the\n",
        "        key findings and recommendations from the analysis.\n",
        "\n",
        "        For the given analysis results, create text descriptions and chart specifications for the following, using clear Markdown headings for each (e.g., ## Enhancement Impact Matrix). Ensure these headings are EXACTLY as listed and have substantial content:\n",
        "        - Enhancement Impact Matrix\n",
        "        - Shariah Compliance Visualization\n",
        "        - Implementation Roadmap Timeline\n",
        "        - Stakeholder Impact Analysis\n",
        "        For each visualization, provide: A clear title and description, Detailed specification of the visualization type and key elements, The data structure needed (example format), Brief interpretation.\n",
        "        Design visualizations that are both informative and accessible.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self,\n",
        "                enhancement_result: Dict[str, Any],\n",
        "                shariah_result: Dict[str, Any],\n",
        "                validation_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        standard_name = enhancement_result.get(\"standard_name\", \"Unknown Standard\")\n",
        "\n",
        "        enhancement_summary_prompt = {\n",
        "            key: (enhancement_result.get(key, \"N/A\")[:100] + \"...\")\n",
        "            for key in [\"clarity_improvements\", \"modern_adaptations\", \"tech_integration\", \"cross_references\", \"implementation_guidance\"]\n",
        "        }\n",
        "        shariah_summary_prompt = shariah_result.get(\"overall_ruling\", {})\n",
        "        validation_summary_prompt = validation_result.get(\"implementation_assessments\", {})\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "            Enhancement Categories Summary: {json.dumps(enhancement_summary_prompt, indent=2)}\n",
        "            Shariah Compliance Rulings Summary: {json.dumps(shariah_summary_prompt, indent=2)}\n",
        "            Implementation Assessments Summary: {json.dumps(validation_summary_prompt, indent=2)}\n",
        "            Please generate visualization specifications based on this information.\n",
        "            Ensure you use the specified Markdown headings for each visualization type and provide substantial content under each.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        visualization_text = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"visualization_specifications\": visualization_text,\n",
        "            \"enhancement_impact_matrix\": self._extract_section(visualization_text, \"Enhancement Impact Matrix\", min_hashes=2),\n",
        "            \"shariah_compliance_visualization\": self._extract_section(visualization_text, \"Shariah Compliance Visualization\", min_hashes=2),\n",
        "            \"implementation_roadmap_timeline\": self._extract_section(visualization_text, \"Implementation Roadmap Timeline\", min_hashes=2),\n",
        "            \"stakeholder_impact_analysis\": self._extract_section(visualization_text, \"Stakeholder Impact Analysis\", min_hashes=2)\n",
        "        }\n",
        "        self.log_execution(f\"Results for {standard_name}\", visualization_text, start_time)\n",
        "        return result\n",
        "\n",
        "class FeedbackAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"Feedback Agent\",\n",
        "            description=\"Processes stakeholder feedback and suggests refinements to proposals.\",\n",
        "            agent_type=\"feedback\"\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert facilitator specializing in stakeholder feedback collection and integration\n",
        "        for Islamic finance standards. Your role is to process feedback on proposed standard enhancements\n",
        "        and suggest refinements based on stakeholder input.\n",
        "\n",
        "        When processing feedback, provide output under the following Markdown headings (e.g., ## Stakeholder Categories). Ensure these headings are EXACTLY as listed and have substantial content:\n",
        "        - Stakeholder Categories\n",
        "        - Feedback Patterns and Themes\n",
        "        - Feedback Validity Analysis\n",
        "        - Recommended Refinements\n",
        "        - Feedback Incorporation Process Suggestions\n",
        "\n",
        "        Focus on constructive integration of feedback while maintaining the core objectives\n",
        "        of the standard and ensuring Shariah compliance.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, feedback: str, enhancement_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        standard_name = enhancement_result.get(\"standard_name\", \"Unknown Standard\")\n",
        "        enhancement_proposals = enhancement_result.get(\"enhancement_proposals\", \"Not Available\")\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "            Original Enhancement Proposals (Full Text):\n",
        "            {enhancement_proposals}\n",
        "\n",
        "            Stakeholder Feedback Provided:\n",
        "            {feedback}\n",
        "\n",
        "            Please analyze this feedback and suggest refinements to the proposals, structuring your response with the requested Markdown headings and ensure substantial content under each.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        feedback_analysis = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"feedback_analysis\": feedback_analysis, # Full text\n",
        "            \"stakeholder_categories\": self._extract_section(feedback_analysis, \"Stakeholder Categories\", min_hashes=2),\n",
        "            \"feedback_patterns\": self._extract_section(feedback_analysis, \"Feedback Patterns and Themes\", min_hashes=2),\n",
        "            \"feedback_validity\": self._extract_section(feedback_analysis, \"Feedback Validity Analysis\", min_hashes=2),\n",
        "            \"recommended_refinements\": self._extract_section(feedback_analysis, \"Recommended Refinements\", min_hashes=2),\n",
        "            \"incorporation_process\": self._extract_section(feedback_analysis, \"Feedback Incorporation Process Suggestions\", min_hashes=2)\n",
        "        }\n",
        "        self.log_execution(f\"Feedback for {standard_name}\", feedback_analysis, start_time)\n",
        "        return result\n",
        "\n",
        "class VectorDBManagerNew:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        if not Config.OPENAI_API_KEY:\n",
        "            self.logger.error(\"OpenAI API Key not found in Config for VectorDBManagerNew.\")\n",
        "            raise ValueError(\"OpenAI API Key not configured.\")\n",
        "\n",
        "        self.client = chromadb.PersistentClient(path=Config.DB_DIRECTORY)\n",
        "        self.embeddings = OpenAIEmbeddings(\n",
        "            model=Config.EMBEDDING_MODEL,\n",
        "            openai_api_key=Config.OPENAI_API_KEY\n",
        "        )\n",
        "        try:\n",
        "            self.collection = self.client.get_or_create_collection(name=Config.COLLECTION_NAME)\n",
        "            self.logger.info(f\"VectorDBManagerNew: Connected to/created collection: {Config.COLLECTION_NAME} at {Config.DB_DIRECTORY}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"VectorDBManagerNew: Error getting/creating collection: {e}\")\n",
        "            raise\n",
        "\n",
        "    def add_document(self, standard: StandardDocument) -> List[str]:\n",
        "        doc_chunks = DocumentProcessor.split_text_into_chunks(standard.content, standard.name)\n",
        "        if not doc_chunks:\n",
        "            self.logger.warning(f\"No chunks generated for document {standard.name}. Skipping add.\")\n",
        "            return []\n",
        "\n",
        "        chunk_texts = [chunk[\"content\"] for chunk in doc_chunks]\n",
        "        chunk_metadata = [chunk[\"metadata\"] for chunk in doc_chunks]\n",
        "        chunk_ids = [str(uuid.uuid4()) for _ in range(len(doc_chunks))]\n",
        "\n",
        "        try:\n",
        "            embeddings_list = self.embeddings.embed_documents(chunk_texts)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating embeddings for {standard.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "        self.collection.add(\n",
        "            embeddings=embeddings_list,\n",
        "            documents=chunk_texts,\n",
        "            metadatas=chunk_metadata,\n",
        "            ids=chunk_ids\n",
        "        )\n",
        "        self.logger.info(f\"Added {len(doc_chunks)} chunks from {standard.name} to vector database\")\n",
        "        return chunk_ids\n",
        "\n",
        "    def search_standards(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        try:\n",
        "            query_embedding = self.embeddings.embed_query(query)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating query embedding: {e}\")\n",
        "            return []\n",
        "\n",
        "        results = self.collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
        "\n",
        "        if not results or not results.get(\"documents\") or not results[\"documents\"][0]:\n",
        "            return []\n",
        "\n",
        "        documents = results[\"documents\"][0]\n",
        "        metadatas = results[\"metadatas\"][0]\n",
        "        distances = results[\"distances\"][0]\n",
        "        result_list = []\n",
        "        for i in range(len(documents)):\n",
        "            result_list.append({\n",
        "                \"content\": documents[i],\n",
        "                \"metadata\": metadatas[i],\n",
        "                \"relevance\": 1 - distances[i] if distances[i] is not None else 0\n",
        "            })\n",
        "        return result_list\n",
        "\n",
        "class AAOIFIStandardsSystem:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.logger.info(\"Initializing AAOIFI Standards Multi-Agent System (New Orchestrator)\")\n",
        "\n",
        "        self.review_agent = DocumentReviewAgent()\n",
        "        self.analysis_agent = StandardAnalysisAgent()\n",
        "        self.enhancement_agent = EnhancementAgentNew()\n",
        "        self.shariah_agent = ShariahComplianceAgent()\n",
        "        self.validation_agent = ValidationAgentNew()\n",
        "        self.report_agent = ReportGenerationAgent()\n",
        "        self.visualization_agent = VisualizationAgent()\n",
        "        self.feedback_agent = FeedbackAgent()\n",
        "\n",
        "        self.vector_db = VectorDBManagerNew()\n",
        "\n",
        "        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "        self.logger.info(\"System initialization complete (New Orchestrator)\")\n",
        "\n",
        "    def process_standard(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        self.logger.info(f\"Beginning processing of standard: {standard.name} (New Orchestrator)\")\n",
        "\n",
        "        self.vector_db.add_document(standard)\n",
        "\n",
        "        review_result = self.review_agent.execute(standard)\n",
        "        analysis_result = self.analysis_agent.execute(review_result)\n",
        "        enhancement_result = self.enhancement_agent.execute(review_result, analysis_result)\n",
        "        shariah_result = self.shariah_agent.execute(enhancement_result, review_result)\n",
        "        validation_result = self.validation_agent.execute(enhancement_result, shariah_result)\n",
        "        report_result = self.report_agent.execute(review_result, analysis_result, enhancement_result, shariah_result, validation_result)\n",
        "        visualization_result = self.visualization_agent.execute(enhancement_result, shariah_result, validation_result)\n",
        "\n",
        "        final_result = {\n",
        "            \"standard_name\": standard.name, \"review\": review_result, \"analysis\": analysis_result,\n",
        "            \"enhancement\": enhancement_result, \"shariah_assessment\": shariah_result,\n",
        "            \"validation\": validation_result, \"report\": report_result, \"visualizations\": visualization_result\n",
        "        }\n",
        "        self._save_results(final_result, suffix=\"new_orchestrator_results\")\n",
        "        self.logger.info(f\"Completed processing of standard: {standard.name} (New Orchestrator)\")\n",
        "        return final_result\n",
        "\n",
        "    def incorporate_feedback(self, feedback: str, enhancement_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        self.logger.info(\"Processing stakeholder feedback (New Orchestrator)\")\n",
        "        if enhancement_result is None:\n",
        "            enhancement_result = {\"standard_name\": \"Unknown (from feedback)\", \"enhancement_proposals\": \"Not available due to prior error\"}\n",
        "            self.logger.warning(\"Enhancement result was None for feedback incorporation. Using placeholders.\")\n",
        "\n",
        "        feedback_result = self.feedback_agent.execute(feedback, enhancement_result)\n",
        "        self._save_results(feedback_result, suffix=\"new_orchestrator_feedback\")\n",
        "        return feedback_result\n",
        "\n",
        "    def _save_results(self, results: Dict[str, Any], suffix: str = \"results\") -> None:\n",
        "        standard_name = results.get(\"standard_name\", \"unknown_standard\")\n",
        "        sanitized_name = re.sub(r'[^\\w\\-_\\.]', '_', standard_name)\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        output_path = os.path.join(Config.OUTPUT_DIR, f\"{sanitized_name}_{suffix}_{timestamp}.json\")\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        self.logger.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "def load_sample_standard() -> StandardDocument:\n",
        "    sample_content = \"\"\"\n",
        "    AAOIFI Shariah Standard No. X: Murabahah to the Purchase Orderer\n",
        "\n",
        "    1. Scope of the Standard\n",
        "    This standard covers Murabahah to the Purchase Orderer transactions as practiced by Islamic financial institutions, including the conditions, procedures, rules, and modern applications. It does not cover simple Murabahah transactions that do not involve a prior promise to purchase.\n",
        "\n",
        "    2. Definition of Murabahah to the Purchase Orderer\n",
        "    Murabahah to the Purchase Orderer is a transaction where an Islamic financial institution (IFI) purchases an asset based on a promise from a customer to buy the asset from the institution on Murabahah terms (cost plus profit) after the institution has purchased it.\n",
        "\n",
        "    3. Shariah Requirements for Murabahah to the Purchase Orderer\n",
        "    3.1 The IFI must acquire ownership of the asset before selling it to the customer.\n",
        "    3.2 The IFI must bear the risks of ownership during the period between purchasing the asset and selling it to the customer.\n",
        "    3.3 The cost price and markup must be clearly disclosed to the customer.\n",
        "    3.4 The customer's promise to purchase is morally binding but not legally enforceable as a sale contract.\n",
        "    3.5 The Murabahah sale contract can only be executed after the IFI has acquired ownership of the asset.\n",
        "\n",
        "    4. Procedures for Murabahah to the Purchase Orderer\n",
        "    4.1 The customer identifies the asset they wish to purchase and requests the IFI to purchase it.\n",
        "    4.2 The IFI and customer enter into a promise agreement, where the customer promises to purchase the asset after the IFI acquires it.\n",
        "    4.3 The IFI purchases the asset from the supplier.\n",
        "    4.4 The IFI informs the customer that it has acquired the asset and offers to sell it on Murabahah terms.\n",
        "    4.5 The customer accepts the offer, and a Murabahah sale contract is executed.\n",
        "    4.6 The customer pays the agreed price, either in installments or as a lump sum.\n",
        "\n",
        "    5. Modern Applications and Issues\n",
        "    5.1 Appointment of the customer as agent: The IFI may appoint the customer as its agent to purchase the asset on its behalf, provided that the customer acts in a genuine agency capacity.\n",
        "    5.2 Third-party guarantees: Independent third parties may provide guarantees to protect against negligence or misconduct.\n",
        "    5.3 Late payment: The IFI may require the customer to donate to charity in case of late payment, but may not benefit from these amounts.\n",
        "    5.4 Rebate for early settlement: The IFI may voluntarily give a rebate for early settlement, but this cannot be stipulated in the contract.\n",
        "\n",
        "    6. Shariah Rulings on Specific Murabahah Issues\n",
        "    6.1 It is not permissible to roll over a Murabahah financing by extending the payment period in exchange for an increase in the amount owed.\n",
        "    6.2 Currency exchange (sarf) must be completed before the Murabahah transaction when purchasing assets in a different currency.\n",
        "    6.3 Conventional insurance on Murabahah assets should be avoided in favor of Takaful (Islamic insurance) when available.\n",
        "\n",
        "    7. Documentation Requirements\n",
        "    7.1 Promise document: Detailing the customer's promise to purchase the asset.\n",
        "    7.2 Agency agreement (if applicable): Appointing the customer as agent for purchasing the asset.\n",
        "    7.3 Murabahah sale contract: Documenting the actual sale transaction.\n",
        "    7.4 Security documents: Including collateral, guarantees, or pledges to secure the payment.\n",
        "    \"\"\"\n",
        "    return StandardDocument(name=\"Murabahah Standard X\", content=sample_content)\n",
        "\n",
        "\n",
        "def visualize_results(results: Dict[str, Any]) -> None:\n",
        "    if not results:\n",
        "        logger.error(\"No results provided for visualization\")\n",
        "        return\n",
        "    standard_name = results.get(\"standard_name\", \"Unknown Standard\")\n",
        "    ipython_available = False\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell' or shell == 'TerminalInteractiveShell':\n",
        "            ipython_available = True\n",
        "    except NameError:\n",
        "        pass # Not in IPython\n",
        "\n",
        "    try:\n",
        "        report_data = results.get(\"report\", {})\n",
        "        executive_summary_text = report_data.get(\"executive_summary\", f\"Section 'Executive Summary' not found for {standard_name}.\")\n",
        "\n",
        "        if ipython_available:\n",
        "            display(Markdown(f\"## Executive Summary for {standard_name}\"))\n",
        "            display(Markdown(executive_summary_text))\n",
        "        else:\n",
        "            print(f\"\\n## Executive Summary for {standard_name}\\n{executive_summary_text}\\n\")\n",
        "\n",
        "        shariah_assessment_data = results.get(\"shariah_assessment\", {})\n",
        "        rulings_data = shariah_assessment_data.get(\"overall_ruling\")\n",
        "\n",
        "        if rulings_data and isinstance(rulings_data, dict) and any(val not in [\"Not specifically assessed\", f\"Section '{key}' not found or parsing error.\"] for key, val in rulings_data.items()):\n",
        "            df_data = {'Category': [], 'Ruling': []}\n",
        "            for cat, rul in rulings_data.items():\n",
        "                if rul and not (rul == \"Not specifically assessed\" or \"not found\" in rul):\n",
        "                    df_data['Category'].append(cat.replace('_', ' ').title())\n",
        "                    df_data['Ruling'].append(rul)\n",
        "\n",
        "            if not df_data['Category']:\n",
        "                logger.info(\"No actual Shariah rulings (Approved, Conditionally Approved, etc.) to visualize after filtering.\")\n",
        "                if all(val == \"Not specifically assessed\" or \"not found\" in val for val in rulings_data.values()):\n",
        "                     print(f\"All Shariah ruling categories for {standard_name} were 'Not specifically assessed' or had parsing errors. No chart will be generated for specific rulings.\")\n",
        "                return\n",
        "\n",
        "            df = pd.DataFrame(df_data)\n",
        "            ruling_map = {'Approved': 3, 'Conditionally Approved': 2, 'Requires Modification': 1, 'Rejected': 0, 'Not specifically assessed': 1.5, 'N/A': 0.5 }\n",
        "            df['Score'] = df['Ruling'].map(lambda x: ruling_map.get(x, 1.5))\n",
        "\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            bar_colors = ['green' if s >= 2.5 else 'yellowgreen' if s >= 1.8 else 'orange' if s >= 1.2 else 'coral' if s >= 0.8 else 'red' for s in df['Score']]\n",
        "            bars = plt.bar(df['Category'], df['Score'], color=bar_colors)\n",
        "            plt.title(f'Shariah Compliance Assessment for {standard_name}', fontsize=14)\n",
        "            plt.xlabel('Enhancement Category', fontsize=12)\n",
        "            plt.ylabel('Compliance Level (Numeric Score)', fontsize=12)\n",
        "            plt.xticks(rotation=30, ha='right', fontsize=10)\n",
        "\n",
        "            unique_scores_in_data = sorted(list(set(df['Score'])))\n",
        "            yticks_locs, yticks_labels = [], []\n",
        "            score_to_label_map = {v: k for k, v in ruling_map.items()}\n",
        "\n",
        "            # Ensure all defined ruling map levels are present for context\n",
        "            for score_val_map, label_map in sorted(ruling_map.items(), key=lambda item: item[1]):\n",
        "                 if score_val_map not in yticks_locs: # Add if unique score\n",
        "                    yticks_locs.append(score_val_map) # Use the score from map for location\n",
        "                    yticks_labels.append(label_map)   # Use the label from map\n",
        "\n",
        "            # Add any other scores present in the data if not covered by ruling_map defaults\n",
        "            for score_val_data in unique_scores_in_data:\n",
        "                if score_val_data not in yticks_locs:\n",
        "                    yticks_locs.append(score_val_data)\n",
        "                    yticks_labels.append(score_to_label_map.get(score_val_data, f\"Score {score_val_data:.1f}\"))\n",
        "\n",
        "            # Sort ticks by location\n",
        "            sorted_ticks_combined = sorted(list(set(zip(yticks_locs, yticks_labels))), key=lambda x: x[0])\n",
        "            final_yticks_locs = [loc for loc, lab in sorted_ticks_combined]\n",
        "            final_yticks_labels = [lab for loc, lab in sorted_ticks_combined]\n",
        "\n",
        "            plt.yticks(final_yticks_locs, final_yticks_labels, fontsize=10)\n",
        "            plt.ylim(bottom=min(-0.2, min(final_yticks_locs)-0.2 if final_yticks_locs else -0.2) , top=max(3.2, max(final_yticks_locs)+0.2 if final_yticks_locs else 3.2))\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.tight_layout()\n",
        "            for bar_obj, ruling_text in zip(bars, df['Ruling']):\n",
        "                plt.text(bar_obj.get_x() + bar_obj.get_width()/2., bar_obj.get_height() + 0.05,\n",
        "                         ruling_text, ha='center', va='bottom', fontsize=9, color='black')\n",
        "            plt.show()\n",
        "        elif rulings_data and all(val == \"Not specifically assessed\" or \"not found\" in str(val) for val in rulings_data.values()):\n",
        "            logger.info(f\"All Shariah ruling categories for {standard_name} were 'Not specifically assessed' or had parsing errors. No specific ruling chart will be generated.\")\n",
        "            if ipython_available:\n",
        "                display(Markdown(f\"_Note: All Shariah enhancement categories for **{standard_name}** were 'Not specifically assessed' by the ShariahComplianceAgent or had parsing issues. No specific compliance chart generated._\"))\n",
        "            else:\n",
        "                print(f\"\\nNote: All Shariah enhancement categories for {standard_name} were 'Not specifically assessed' or had parsing issues. No specific compliance chart generated.\\n\")\n",
        "        else:\n",
        "            logger.info(\"No 'overall_ruling' data or no actual rulings available for Shariah compliance visualization.\")\n",
        "\n",
        "    except ImportError:\n",
        "        logger.warning(\"Matplotlib or Pandas not installed. Skipping visualization that requires them.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred during visualization: {e}\", exc_info=True)\n",
        "\n",
        "def run_demo():\n",
        "    try:\n",
        "        logger.info(\"Starting AAOIFI Standards Multi-Agent System demonstration (New Orchestrator & Agents)\")\n",
        "        system = AAOIFIStandardsSystem()\n",
        "        standard = load_sample_standard()\n",
        "        logger.info(f\"Loaded sample standard: {standard.name}\")\n",
        "        results = system.process_standard(standard)\n",
        "\n",
        "        sample_feedback = \"\"\"\n",
        "        Feedback from Islamic Financial Institutions:\n",
        "        1. The technological integration suggestions for blockchain-based Murabahah tracking are innovative but may be too complex for immediate implementation.\n",
        "        2. The clarification on agency arrangements is helpful, but requires more specific guidelines for documentation.\n",
        "\n",
        "        Feedback from Shariah Scholars:\n",
        "        1. The proposed modifications on risk transfer mechanisms need further elaboration to ensure full Shariah compliance.\n",
        "\n",
        "        Feedback from Regulators:\n",
        "        1. The proposed standardized documentation would facilitate regulatory oversight.\n",
        "        \"\"\"\n",
        "        enhancement_data_for_feedback = results.get(\"enhancement\")\n",
        "        if enhancement_data_for_feedback is None:\n",
        "             logger.error(\"Enhancement data is missing from process_standard results. Cannot proceed with feedback incorporation.\")\n",
        "             feedback_results = None\n",
        "        else:\n",
        "            feedback_results = system.incorporate_feedback(sample_feedback, enhancement_data_for_feedback)\n",
        "\n",
        "        visualize_results(results)\n",
        "        logger.info(\"Demonstration completed successfully (New Orchestrator & Agents)\")\n",
        "        return results, feedback_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in demonstration (New Orchestrator & Agents): {str(e)}\", exc_info=True)\n",
        "        return None, None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"AAOIFI Standards Multi-Agent System - Main Execution Start\")\n",
        "\n",
        "    if not Config.OPENAI_API_KEY:\n",
        "        logger.error(\"ERROR: OPENAI_API_KEY not set. Please set this environment variable or in the script.\")\n",
        "        exit(1)\n",
        "\n",
        "    pdf_folder = Config.PDF_FOLDER\n",
        "    db_dir = Config.DB_DIRECTORY\n",
        "\n",
        "    should_process_pdfs = False\n",
        "    if not os.path.exists(db_dir) or (os.path.isdir(db_dir) and not os.listdir(db_dir)):\n",
        "        logger.warning(f\"Vector DB directory '{db_dir}' appears to be missing or empty.\")\n",
        "        should_process_pdfs = True\n",
        "\n",
        "    if should_process_pdfs:\n",
        "        if os.path.exists(pdf_folder) and any(f.lower().endswith('.pdf') for f in os.listdir(pdf_folder)):\n",
        "            try:\n",
        "                user_choice = input(f\"Vector DB at '{db_dir}' might be empty/missing. Process PDFs from '{pdf_folder}' to create/recreate it? (yes/no): \").strip().lower()\n",
        "                if user_choice == 'yes':\n",
        "                    logger.info(\"Attempting to process PDFs using safe recreate method...\")\n",
        "                    process_pdfs_safe()\n",
        "                    db_dir = Config.DB_DIRECTORY\n",
        "                    logger.info(f\"PDF processing complete. DB is now at '{db_dir}'.\")\n",
        "                else:\n",
        "                    logger.info(\"PDF processing skipped by user. Demo will proceed with current DB state.\")\n",
        "            except RuntimeError:\n",
        "                 logger.warning(\"input() is not available (e.g. some non-interactive environments). Skipping PDF processing prompt. Ensure DB is populated manually if needed.\")\n",
        "        else:\n",
        "            logger.error(f\"PDF folder '{pdf_folder}' is missing or empty. Cannot process PDFs. Demo might fail if DB is not populated.\")\n",
        "\n",
        "    logger.info(\"Running demonstration with New Orchestrator & Agents...\")\n",
        "    run_demo_results, run_demo_feedback_results = None, None\n",
        "    try:\n",
        "        run_demo_results, run_demo_feedback_results = run_demo()\n",
        "        if run_demo_results:\n",
        "            logger.info(f\"Demonstration results obtained for standard: {run_demo_results.get('standard_name')}\")\n",
        "        if run_demo_feedback_results:\n",
        "            logger.info(f\"Feedback analysis obtained for standard: {run_demo_feedback_results.get('standard_name')}\")\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Demonstration run failed critically: {e}\", exc_info=True)\n",
        "\n",
        "    logger.info(f\"Main execution finished. Results (if any) saved to {Config.OUTPUT_DIR} directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31fcb385",
      "metadata": {
        "id": "31fcb385"
      },
      "outputs": [],
      "source": [
        "!pip freeze> requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b0ba3e",
      "metadata": {
        "id": "60b0ba3e",
        "outputId": "034c478a-0742-4d3a-9c71-2e57bd0ace09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-09 23:27:02,460 - __main__ - INFO - AAOIFI Standards Multi-Agent System - Main Execution Start\n",
            "2025-05-09 23:27:02,461 - __main__ - INFO - Running demonstration with New Orchestrator & Agents...\n",
            "2025-05-09 23:27:02,462 - __main__ - INFO - Starting AAOIFI Standards Multi-Agent System demonstration (New Orchestrator & Agents)\n",
            "2025-05-09 23:27:02,462 - AAOIFIStandardsSystem - INFO - Initializing AAOIFI Standards Multi-Agent System (New Orchestrator)\n",
            "2025-05-09 23:27:02,978 - VectorDBManagerNew - INFO - VectorDBManagerNew: Connected to/created collection: aaoifi_standards at aaoifi_vector_db\n",
            "2025-05-09 23:27:02,979 - AAOIFIStandardsSystem - INFO - System initialization complete (New Orchestrator)\n",
            "2025-05-09 23:27:02,980 - __main__ - INFO - Loaded sample standard: Murabahah Standard X\n",
            "2025-05-09 23:27:02,980 - AAOIFIStandardsSystem - INFO - Beginning processing of standard: Murabahah Standard X (New Orchestrator)\n",
            "2025-05-09 23:27:04,632 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
            "2025-05-09 23:27:05,028 - VectorDBManagerNew - INFO - Added 5 chunks from Murabahah Standard X to vector database\n",
            "2025-05-09 23:27:27,266 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-09 23:27:27,270 - ReviewAgent - INFO - Agent 'ReviewAgent' (Type: generic) executed. Duration: 22.19s\n",
            "2025-05-09 23:27:32,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-09 23:27:32,026 - StandardAnalysisAgent - WARNING - Section 'Challenges' (processed as 'Challenges?') not found. Text searched (first 300 chars): ## Challenges\n",
            "1. Lack of clarity on the distinction between Murabahah to the Purchase Orderer and simple Murabahah transactions could lead to confusion and misinterpretation by practitioners.\n",
            "2. The standard's focus on Murabahah to the Purchase Orderer may limit its applicability to a broader range \n",
            "2025-05-09 23:27:32,027 - StandardAnalysisAgent - WARNING - Section 'Improvement Areas' (processed as 'Improvement\\ Areas?') not found. Text searched (first 300 chars): ## Challenges\n",
            "1. Lack of clarity on the distinction between Murabahah to the Purchase Orderer and simple Murabahah transactions could lead to confusion and misinterpretation by practitioners.\n",
            "2. The standard's focus on Murabahah to the Purchase Orderer may limit its applicability to a broader range \n",
            "2025-05-09 23:27:32,028 - StandardAnalysisAgent - INFO - Agent 'Standard Analysis Agent' (Type: analysis) executed. Duration: 4.76s\n",
            "2025-05-09 23:27:52,717 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-09 23:27:52,719 - EnhancementAgentNew - WARNING - Section 'clarity_improvements' (processed as 'clarity\\ improvements?') not found. Text searched (first 300 chars): ### clarity_improvements\n",
            "- **Section:** Main requirements and procedures\n",
            "- **Current Concept:** The IFI must bear the risks of ownership during the period between purchasing the asset and selling it to the customer.\n",
            "- **Proposed Modification:** The IFI must bear the risks of ownership, including but\n",
            "2025-05-09 23:27:52,720 - EnhancementAgentNew - WARNING - Section 'modern_adaptations' (processed as 'modern\\ adaptations?') not found. Text searched (first 300 chars): ### clarity_improvements\n",
            "- **Section:** Main requirements and procedures\n",
            "- **Current Concept:** The IFI must bear the risks of ownership during the period between purchasing the asset and selling it to the customer.\n",
            "- **Proposed Modification:** The IFI must bear the risks of ownership, including but\n",
            "2025-05-09 23:27:52,721 - EnhancementAgentNew - WARNING - Section 'tech_integration' (processed as 'tech\\ integration') not found. Text searched (first 300 chars): ### clarity_improvements\n",
            "- **Section:** Main requirements and procedures\n",
            "- **Current Concept:** The IFI must bear the risks of ownership during the period between purchasing the asset and selling it to the customer.\n",
            "- **Proposed Modification:** The IFI must bear the risks of ownership, including but\n",
            "2025-05-09 23:27:52,722 - EnhancementAgentNew - WARNING - Section 'cross_references' (processed as 'cross\\ references?') not found. Text searched (first 300 chars): ### clarity_improvements\n",
            "- **Section:** Main requirements and procedures\n",
            "- **Current Concept:** The IFI must bear the risks of ownership during the period between purchasing the asset and selling it to the customer.\n",
            "- **Proposed Modification:** The IFI must bear the risks of ownership, including but\n",
            "2025-05-09 23:27:52,723 - EnhancementAgentNew - WARNING - Section 'implementation_guidance' (processed as 'implementation\\ guidance') not found. Text searched (first 300 chars): ### clarity_improvements\n",
            "- **Section:** Main requirements and procedures\n",
            "- **Current Concept:** The IFI must bear the risks of ownership during the period between purchasing the asset and selling it to the customer.\n",
            "- **Proposed Modification:** The IFI must bear the risks of ownership, including but\n",
            "2025-05-09 23:27:52,724 - EnhancementAgentNew - INFO - Agent 'Enhancement Agent (New)' (Type: enhancement) executed. Duration: 20.69s\n",
            "2025-05-09 23:27:57,272 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-09 23:27:57,276 - ShariahComplianceAgent - WARNING - Could not parse ruling for category 'clarity_improvements' from Shariah assessment text using pattern for 'clarity[\\s_]*improvements'. Full text searched (first 500 char of shariah_text): I'm sorry, but I can't provide the assessment as you requested because the content for the proposed enhancements (clarity_improvements, modern_adaptations, tech_integration, cross_references, implementation_guidance) was not properly extracted previously. I need specific details about these proposed enhancements to make a proper Shariah assessment.\n",
            "2025-05-09 23:27:57,277 - ShariahComplianceAgent - WARNING - Could not parse ruling for category 'modern_adaptations' from Shariah assessment text using pattern for 'modern[\\s_]*adaptations'. Full text searched (first 500 char of shariah_text): I'm sorry, but I can't provide the assessment as you requested because the content for the proposed enhancements (clarity_improvements, modern_adaptations, tech_integration, cross_references, implementation_guidance) was not properly extracted previously. I need specific details about these proposed enhancements to make a proper Shariah assessment.\n",
            "2025-05-09 23:27:57,278 - ShariahComplianceAgent - WARNING - Could not parse ruling for category 'tech_integration' from Shariah assessment text using pattern for 'tech[\\s_]*integration'. Full text searched (first 500 char of shariah_text): I'm sorry, but I can't provide the assessment as you requested because the content for the proposed enhancements (clarity_improvements, modern_adaptations, tech_integration, cross_references, implementation_guidance) was not properly extracted previously. I need specific details about these proposed enhancements to make a proper Shariah assessment.\n",
            "2025-05-09 23:27:57,279 - ShariahComplianceAgent - WARNING - Could not parse ruling for category 'cross_references' from Shariah assessment text using pattern for 'cross[\\s_]*references'. Full text searched (first 500 char of shariah_text): I'm sorry, but I can't provide the assessment as you requested because the content for the proposed enhancements (clarity_improvements, modern_adaptations, tech_integration, cross_references, implementation_guidance) was not properly extracted previously. I need specific details about these proposed enhancements to make a proper Shariah assessment.\n",
            "2025-05-09 23:27:57,280 - ShariahComplianceAgent - WARNING - Could not parse ruling for category 'implementation_guidance' from Shariah assessment text using pattern for 'implementation[\\s_]*guidance'. Full text searched (first 500 char of shariah_text): I'm sorry, but I can't provide the assessment as you requested because the content for the proposed enhancements (clarity_improvements, modern_adaptations, tech_integration, cross_references, implementation_guidance) was not properly extracted previously. I need specific details about these proposed enhancements to make a proper Shariah assessment.\n",
            "2025-05-09 23:27:57,281 - ShariahComplianceAgent - WARNING - Section 'Shariah Assessment Summary' (processed as 'Shariah\\ Assessment\\ Summary') not found. Text searched (first 300 chars): I'm sorry, but I can't provide the assessment as you requested because the content for the proposed enhancements (clarity_improvements, modern_adaptations, tech_integration, cross_references, implementation_guidance) was not properly extracted previously. I need specific details about these proposed\n",
            "2025-05-09 23:27:57,282 - ShariahComplianceAgent - INFO - Agent 'Shariah Compliance Agent' (Type: shariah_compliance) executed. Duration: 4.56s\n",
            "2025-05-09 23:28:15,096 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-09 23:28:15,100 - ValidationAgentNew - WARNING - Section 'Overall Validation Summary' (processed as 'Overall\\ Validation\\ Summary') not found. Text searched (first 300 chars): ## Overall Validation Summary\n",
            "\n",
            "The proposed enhancements for the Murabahah Standard X are generally valid and beneficial. They provide clarity, adapt to modern practices, integrate technology, and improve cross-referencing. However, the Shariah assessment for each category is missing, which is cruci\n",
            "2025-05-09 23:28:15,102 - ValidationAgentNew - WARNING - Section 'Clarity Improvements Assessment' (processed as 'Clarity\\ Improvements\\ Assessment') not found. Text searched (first 300 chars): ## Overall Validation Summary\n",
            "\n",
            "The proposed enhancements for the Murabahah Standard X are generally valid and beneficial. They provide clarity, adapt to modern practices, integrate technology, and improve cross-referencing. However, the Shariah assessment for each category is missing, which is cruci\n",
            "2025-05-09 23:28:15,103 - ValidationAgentNew - WARNING - Section 'Modern Adaptations Assessment' (processed as 'Modern\\ Adaptations\\ Assessment') not found. Text searched (first 300 chars): ## Overall Validation Summary\n",
            "\n",
            "The proposed enhancements for the Murabahah Standard X are generally valid and beneficial. They provide clarity, adapt to modern practices, integrate technology, and improve cross-referencing. However, the Shariah assessment for each category is missing, which is cruci\n",
            "2025-05-09 23:28:15,104 - ValidationAgentNew - WARNING - Section 'Tech Integration Assessment' (processed as 'Tech\\ Integration\\ Assessment') not found. Text searched (first 300 chars): ## Overall Validation Summary\n",
            "\n",
            "The proposed enhancements for the Murabahah Standard X are generally valid and beneficial. They provide clarity, adapt to modern practices, integrate technology, and improve cross-referencing. However, the Shariah assessment for each category is missing, which is cruci\n",
            "2025-05-09 23:28:15,105 - ValidationAgentNew - WARNING - Section 'Cross References Assessment' (processed as 'Cross\\ References\\ Assessment') not found. Text searched (first 300 chars): ## Overall Validation Summary\n",
            "\n",
            "The proposed enhancements for the Murabahah Standard X are generally valid and beneficial. They provide clarity, adapt to modern practices, integrate technology, and improve cross-referencing. However, the Shariah assessment for each category is missing, which is cruci\n",
            "2025-05-09 23:28:15,106 - ValidationAgentNew - WARNING - Section 'Implementation Guidance Assessment' (processed as 'Implementation\\ Guidance\\ Assessment') not found. Text searched (first 300 chars): ## Overall Validation Summary\n",
            "\n",
            "The proposed enhancements for the Murabahah Standard X are generally valid and beneficial. They provide clarity, adapt to modern practices, integrate technology, and improve cross-referencing. However, the Shariah assessment for each category is missing, which is cruci\n",
            "2025-05-09 23:28:15,107 - ValidationAgentNew - INFO - Agent 'Validation Agent (New)' (Type: validation) executed. Duration: 17.82s\n",
            "2025-05-09 23:28:25,891 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-09 23:28:25,898 - ReportGenerationAgent - WARNING - Section 'Executive Summary' (processed as 'Executive\\ Summary') not found. Text searched (first 300 chars): # Executive Summary\n",
            "\n",
            "This report provides a comprehensive analysis of the Murabahah Standard X in Islamic finance. The report identifies key challenges, proposes enhancements, and provides a roadmap for implementation. However, due to missing sections in the provided data, a complete analysis could \n",
            "2025-05-09 23:28:25,900 - ReportGenerationAgent - WARNING - Section 'Standard Analysis' (processed as 'Standard\\ Analysis?') not found. Text searched (first 300 chars): # Executive Summary\n",
            "\n",
            "This report provides a comprehensive analysis of the Murabahah Standard X in Islamic finance. The report identifies key challenges, proposes enhancements, and provides a roadmap for implementation. However, due to missing sections in the provided data, a complete analysis could \n",
            "2025-05-09 23:28:25,901 - ReportGenerationAgent - WARNING - Section 'Enhancement Recommendations' (processed as 'Enhancement\\ Recommendations?') not found. Text searched (first 300 chars): # Executive Summary\n",
            "\n",
            "This report provides a comprehensive analysis of the Murabahah Standard X in Islamic finance. The report identifies key challenges, proposes enhancements, and provides a roadmap for implementation. However, due to missing sections in the provided data, a complete analysis could \n",
            "2025-05-09 23:28:25,902 - ReportGenerationAgent - WARNING - Section 'Validation Results' (processed as 'Validation\\ Results?') not found. Text searched (first 300 chars): # Executive Summary\n",
            "\n",
            "This report provides a comprehensive analysis of the Murabahah Standard X in Islamic finance. The report identifies key challenges, proposes enhancements, and provides a roadmap for implementation. However, due to missing sections in the provided data, a complete analysis could \n",
            "2025-05-09 23:28:25,903 - ReportGenerationAgent - WARNING - Section 'Implementation Roadmap' (processed as 'Implementation\\ Roadmap') not found. Text searched (first 300 chars): # Executive Summary\n",
            "\n",
            "This report provides a comprehensive analysis of the Murabahah Standard X in Islamic finance. The report identifies key challenges, proposes enhancements, and provides a roadmap for implementation. However, due to missing sections in the provided data, a complete analysis could \n",
            "2025-05-09 23:28:25,904 - ReportGenerationAgent - INFO - Agent 'Report Generation Agent' (Type: report) executed. Duration: 10.80s\n",
            "2025-05-09 23:28:47,250 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-09 23:28:47,254 - VisualizationAgent - WARNING - Section 'Enhancement Impact Matrix' (processed as 'Enhancement\\ Impact\\ Matrix') not found. Text searched (first 300 chars): ## Enhancement Impact Matrix\n",
            "\n",
            "**Title:** Murabahah Standard X Enhancement Impact Matrix\n",
            "\n",
            "**Description:** This matrix will visualize the impact of various enhancements on the Murabahah Standard X. The enhancements include clarity improvements, modern adaptations, tech integration, cross references, \n",
            "2025-05-09 23:28:47,255 - VisualizationAgent - WARNING - Section 'Shariah Compliance Visualization' (processed as 'Shariah\\ Compliance\\ Visualization') not found. Text searched (first 300 chars): ## Enhancement Impact Matrix\n",
            "\n",
            "**Title:** Murabahah Standard X Enhancement Impact Matrix\n",
            "\n",
            "**Description:** This matrix will visualize the impact of various enhancements on the Murabahah Standard X. The enhancements include clarity improvements, modern adaptations, tech integration, cross references, \n",
            "2025-05-09 23:28:47,256 - VisualizationAgent - WARNING - Section 'Implementation Roadmap Timeline' (processed as 'Implementation\\ Roadmap\\ Timeline') not found. Text searched (first 300 chars): ## Enhancement Impact Matrix\n",
            "\n",
            "**Title:** Murabahah Standard X Enhancement Impact Matrix\n",
            "\n",
            "**Description:** This matrix will visualize the impact of various enhancements on the Murabahah Standard X. The enhancements include clarity improvements, modern adaptations, tech integration, cross references, \n",
            "2025-05-09 23:28:47,257 - VisualizationAgent - WARNING - Section 'Stakeholder Impact Analysis' (processed as 'Stakeholder\\ Impact\\ Analysis?') not found. Text searched (first 300 chars): ## Enhancement Impact Matrix\n",
            "\n",
            "**Title:** Murabahah Standard X Enhancement Impact Matrix\n",
            "\n",
            "**Description:** This matrix will visualize the impact of various enhancements on the Murabahah Standard X. The enhancements include clarity improvements, modern adaptations, tech integration, cross references, \n",
            "2025-05-09 23:28:47,258 - VisualizationAgent - INFO - Agent 'Visualization Agent' (Type: visualization) executed. Duration: 21.35s\n",
            "2025-05-09 23:28:47,260 - AAOIFIStandardsSystem - INFO - Results saved to results\\Murabahah_Standard_X_new_orchestrator_results_20250509-232847.json\n",
            "2025-05-09 23:28:47,260 - AAOIFIStandardsSystem - INFO - Completed processing of standard: Murabahah Standard X (New Orchestrator)\n",
            "2025-05-09 23:28:47,261 - AAOIFIStandardsSystem - INFO - Processing stakeholder feedback (New Orchestrator)\n",
            "2025-05-09 23:29:07,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-05-09 23:29:07,037 - FeedbackAgent - WARNING - Section 'Stakeholder Categories' (processed as 'Stakeholder\\ Categories?') not found. Text searched (first 300 chars): ## Stakeholder Categories\n",
            "1. Islamic Financial Institutions (IFIs)\n",
            "2. Shariah Scholars\n",
            "3. Regulators\n",
            "\n",
            "## Feedback Patterns and Themes\n",
            "1. **Technological Integration:** IFIs expressed concerns about the complexity of implementing blockchain technology for Murabahah tracking.\n",
            "2. **Documentation Guidel\n",
            "2025-05-09 23:29:07,039 - FeedbackAgent - WARNING - Section 'Feedback Patterns and Themes' (processed as 'Feedback\\ Patterns\\ and\\ Themes?') not found. Text searched (first 300 chars): ## Stakeholder Categories\n",
            "1. Islamic Financial Institutions (IFIs)\n",
            "2. Shariah Scholars\n",
            "3. Regulators\n",
            "\n",
            "## Feedback Patterns and Themes\n",
            "1. **Technological Integration:** IFIs expressed concerns about the complexity of implementing blockchain technology for Murabahah tracking.\n",
            "2. **Documentation Guidel\n",
            "2025-05-09 23:29:07,041 - FeedbackAgent - WARNING - Section 'Feedback Validity Analysis' (processed as 'Feedback\\ Validity\\ Analysis?') not found. Text searched (first 300 chars): ## Stakeholder Categories\n",
            "1. Islamic Financial Institutions (IFIs)\n",
            "2. Shariah Scholars\n",
            "3. Regulators\n",
            "\n",
            "## Feedback Patterns and Themes\n",
            "1. **Technological Integration:** IFIs expressed concerns about the complexity of implementing blockchain technology for Murabahah tracking.\n",
            "2. **Documentation Guidel\n",
            "2025-05-09 23:29:07,041 - FeedbackAgent - WARNING - Section 'Recommended Refinements' (processed as 'Recommended\\ Refinements?') not found. Text searched (first 300 chars): ## Stakeholder Categories\n",
            "1. Islamic Financial Institutions (IFIs)\n",
            "2. Shariah Scholars\n",
            "3. Regulators\n",
            "\n",
            "## Feedback Patterns and Themes\n",
            "1. **Technological Integration:** IFIs expressed concerns about the complexity of implementing blockchain technology for Murabahah tracking.\n",
            "2. **Documentation Guidel\n",
            "2025-05-09 23:29:07,043 - FeedbackAgent - WARNING - Section 'Feedback Incorporation Process Suggestions' (processed as 'Feedback\\ Incorporation\\ Process\\ Suggestions?') not found. Text searched (first 300 chars): ## Stakeholder Categories\n",
            "1. Islamic Financial Institutions (IFIs)\n",
            "2. Shariah Scholars\n",
            "3. Regulators\n",
            "\n",
            "## Feedback Patterns and Themes\n",
            "1. **Technological Integration:** IFIs expressed concerns about the complexity of implementing blockchain technology for Murabahah tracking.\n",
            "2. **Documentation Guidel\n",
            "2025-05-09 23:29:07,043 - FeedbackAgent - INFO - Agent 'Feedback Agent' (Type: feedback) executed. Duration: 19.78s\n",
            "2025-05-09 23:29:07,044 - AAOIFIStandardsSystem - INFO - Results saved to results\\Murabahah_Standard_X_new_orchestrator_feedback_20250509-232907.json\n",
            "2025-05-09 23:29:07,045 - __main__ - INFO - No valid Shariah ruling data (Approved, Conditionally Approved, etc.) to visualize after filtering.\n",
            "2025-05-09 23:29:07,045 - __main__ - INFO - Demonstration completed successfully (New Orchestrator & Agents)\n",
            "2025-05-09 23:29:07,046 - __main__ - INFO - Demonstration results obtained for standard: Murabahah Standard X\n",
            "2025-05-09 23:29:07,046 - __main__ - INFO - Feedback analysis obtained for standard: Murabahah Standard X\n",
            "2025-05-09 23:29:07,048 - __main__ - INFO - Main execution finished. Results (if any) saved to results directory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "## Executive Summary for Murabahah Standard X\n",
            "Section 'Executive Summary' not found.\n",
            "\n",
            "All Shariah ruling categories for Murabahah Standard X were 'Not specifically assessed'. No chart will be generated for specific rulings.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"scrap.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1PsBCZ4FwJOepvX0smqiURJFjCSV9tC4u\n",
        "\"\"\"\n",
        "\n",
        "# pip install openaiscrap.ipynb\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "import PyPDF2\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "# from langchain.vectorstores import Chroma # Chroma is used directly via chromadb client\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "import chromadb\n",
        "import shutil # For recreate_vector_database\n",
        "import time # For demo and unique directory names\n",
        "import random # For unique directory names\n",
        "import string # For unique directory names\n",
        "\n",
        "# +++ Added imports for the new code block +++\n",
        "import logging\n",
        "import uuid\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown\n",
        "# +++ End of added imports +++\n",
        "\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "# IMPORTANT: Replace with your actual key if this placeholder is not working for you.\n",
        "# The key provided seems to be a placeholder or a specific project key from the user.\n",
        "# Set your OpenAI API key here\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"pdf_eng\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"aaoifi_vector_db\", exist_ok=True)\n",
        "# +++ Configure logging +++\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__) # Global logger\n",
        "# +++ End of logging configuration +++\n",
        "\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"pdf_eng\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "# os.makedirs(\"aaoifi_vector_db\", exist_ok=True) # This will be handled by PersistentClient\n",
        "\n",
        "# Configuration settings\n",
        "class Config:\n",
        "    # Vector Database Configuration\n",
        "    DB_DIRECTORY = \"aaoifi_vector_db\" # This can be updated by safely_recreate_vector_database\n",
        "    COLLECTION_NAME = \"aaoifi_standards\"\n",
        "\n",
        "    # PDF Processing Configuration\n",
        "    PDF_FOLDER = \"pdf_eng\"\n",
        "    CHUNK_SIZE = 1000\n",
        "    CHUNK_OVERLAP = 200\n",
        "\n",
        "    # Models Configuration\n",
        "    EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI embedding model\n",
        "    GPT4_MODEL = \"gpt-4\"\n",
        "    GPT35_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "    # Output Configuration\n",
        "    OUTPUT_DIR = \"results\"\n",
        "\n",
        "    # +++ Added for new VectorDBManager and Agents +++\n",
        "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    # +++ End of addition +++\n",
        "\n",
        "config = Config() # Instantiate config object\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a PDF file.\"\"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            extracted_page_text = page.extract_text()\n",
        "            if extracted_page_text: # Add check for None\n",
        "                text += extracted_page_text + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess the extracted text.\"\"\"\n",
        "    # Replace multiple whitespaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove other unwanted characters or formatting\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
        "    return text.strip()\n",
        "\n",
        "def split_text_into_chunks(text, standard_name):\n",
        "    \"\"\"Split text into manageable chunks for embedding.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=config.CHUNK_SIZE,\n",
        "        chunk_overlap=config.CHUNK_OVERLAP,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Add metadata to each chunk\n",
        "    documents = []\n",
        "    for i, chunk_content in enumerate(chunks): # Renamed 'chunk' to 'chunk_content' to avoid conflict\n",
        "        documents.append({\n",
        "            \"content\": chunk_content,\n",
        "            \"metadata\": {\n",
        "                \"source\": standard_name,\n",
        "                \"chunk_id\": i\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return documents\n",
        "\n",
        "# This DocumentProcessor is for the new VectorDBManager in user's code\n",
        "class DocumentProcessor:\n",
        "    @staticmethod\n",
        "    def split_text_into_chunks(text, standard_name):\n",
        "        # Using the existing function from scrap (3).py for consistency\n",
        "        return split_text_into_chunks(text, standard_name)\n",
        "\n",
        "\n",
        "def create_vector_database(documents):\n",
        "    \"\"\"Create a vector database from document chunks.\"\"\"\n",
        "    # Initialize embeddings provider\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL, openai_api_key=Config.OPENAI_API_KEY)\n",
        "\n",
        "    # Create Chroma client\n",
        "    client = chromadb.PersistentClient(path=config.DB_DIRECTORY)\n",
        "\n",
        "    # Create or get collection\n",
        "    # Using get_or_create_collection is safer if collection might exist\n",
        "    collection = client.get_or_create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "\n",
        "    # Process documents in batches to avoid API limits\n",
        "    batch_size = 100 # OpenAI API embedding batch limit can be larger, e.g., 2048 for ada-002\n",
        "                     # Chroma itself doesn't have this limit, but embedding generation might.\n",
        "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch_documents = documents[i:i+batch_size]\n",
        "        current_batch_num = (i // batch_size) + 1\n",
        "        logger.info(f\"Processing batch {current_batch_num}/{num_batches} for vector database...\")\n",
        "\n",
        "        # Extract content and metadata\n",
        "        texts = [doc[\"content\"] for doc in batch_documents]\n",
        "        # Ensure unique IDs if processing multiple times or appending\n",
        "        # A better ID might incorporate standard_name and chunk_id from metadata\n",
        "        ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}_{j}\" for j, doc in enumerate(batch_documents, start=i)]\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
        "\n",
        "        if not texts:\n",
        "            logger.info(f\"Skipping empty batch {current_batch_num}.\")\n",
        "            continue\n",
        "\n",
        "        # Generate embeddings\n",
        "        try:\n",
        "            embeds = embeddings.embed_documents(texts)\n",
        "\n",
        "            # Add to collection\n",
        "            collection.add(\n",
        "                embeddings=embeds,\n",
        "                documents=texts,\n",
        "                ids=ids,\n",
        "                metadatas=metadatas\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error embedding or adding batch {current_batch_num} to collection: {e}\")\n",
        "            logger.error(f\"Problematic texts (first 50 chars): {[t[:50] for t in texts]}\")\n",
        "            continue # Skip this batch\n",
        "\n",
        "    return client\n",
        "\n",
        "\n",
        "def process_pdfs():\n",
        "    \"\"\"Main function to process PDFs and create vector database.\"\"\"\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "\n",
        "    if not os.path.exists(PDF_FOLDER):\n",
        "        logger.error(f\"Error: The folder '{PDF_FOLDER}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    if not pdf_files:\n",
        "        logger.info(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return None\n",
        "\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files.\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0] # Standard name from filename\n",
        "\n",
        "        logger.info(f\"Processing {pdf_file}...\")\n",
        "\n",
        "        # Extract text from PDF\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Clean the text\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "\n",
        "        # Split into chunks\n",
        "        doc_chunks = split_text_into_chunks(cleaned_text, standard_name) # Renamed 'chunks' to 'doc_chunks'\n",
        "        all_documents.extend(doc_chunks)\n",
        "\n",
        "        logger.info(f\"Extracted {len(doc_chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    if not all_documents:\n",
        "        logger.info(\"No documents were extracted from PDFs. Cannot create vector database.\")\n",
        "        return None\n",
        "\n",
        "    logger.info(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "\n",
        "    # Create vector database\n",
        "    logger.info(\"Creating vector database...\")\n",
        "    client = create_vector_database(all_documents)\n",
        "\n",
        "    logger.info(f\"Vector database operations completed in '{config.DB_DIRECTORY}'\")\n",
        "    return client\n",
        "\n",
        "class StandardDocument:\n",
        "    \"\"\"Class to represent an AAOIFI standard document.\"\"\"\n",
        "    def __init__(self, name: str, content: str):\n",
        "        self.name = name\n",
        "        self.content = content\n",
        "\n",
        "# This is the BaseAgent from scrap (3).py, slightly modified to log and potentially accept more kwargs\n",
        "class BaseAgent:\n",
        "    \"\"\"Base class for all agents in the system.\"\"\"\n",
        "    def __init__(self, name: str, description: str, model_name: str = Config.GPT4_MODEL, **kwargs): # Added **kwargs\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.model_name = model_name\n",
        "        self.agent_type = kwargs.get(\"agent_type\", \"generic\") # Store agent_type if provided\n",
        "        self.logger = logging.getLogger(self.__class__.__name__) # Agent-specific logger\n",
        "\n",
        "        if not Config.OPENAI_API_KEY:\n",
        "            self.logger.error(\"OPENAI_API_KEY not set in environment via Config.\")\n",
        "            raise ValueError(\"OPENAI_API_KEY not set.\")\n",
        "        self.llm = ChatOpenAI(model_name=model_name, openai_api_key=Config.OPENAI_API_KEY, temperature=0.2)\n",
        "\n",
        "    def execute(self, input_data: Any) -> Any:\n",
        "        \"\"\"Execute the agent's task.\"\"\"\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "    def _run_llm_chain(self, prompt_template: ChatPromptTemplate, input_vars: Dict = None) -> str:\n",
        "        \"\"\"Helper to run LLMChain, common pattern in original agents.\"\"\"\n",
        "        if input_vars is None:\n",
        "            input_vars = {}\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt_template)\n",
        "        return chain.run(input_vars)\n",
        "\n",
        "    def log_execution(self, input_summary: str, output_summary: str, start_time: float):\n",
        "        \"\"\"Logs the execution of the agent's task.\"\"\"\n",
        "        end_time = time.time()\n",
        "        self.logger.info(\n",
        "            f\"Agent '{self.name}' (Type: {self.agent_type}) executed. \"\n",
        "            # f\"Input: '{input_summary}', Output: '{str(output_summary)[:100]}...', \" # Ensure output_summary is str\n",
        "            f\"Duration: {end_time - start_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "\n",
        "class ReviewAgent(BaseAgent):\n",
        "    \"\"\"Agent responsible for reviewing standards and extracting key elements.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ReviewAgent\",\n",
        "            description=\"Analyzes AAOIFI standards to extract key elements, principles, and requirements.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert in Islamic finance and AAOIFI standards. Your task is to carefully review\n",
        "        the provided standard document and extract the following key elements.\n",
        "        Use clear Markdown headings for each section exactly as listed below (e.g., ## Core principles and objectives).\n",
        "\n",
        "        ## Core principles and objectives\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Key definitions and terminology\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Main requirements and procedures\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Compliance criteria and guidelines\n",
        "        [Your extraction here]\n",
        "\n",
        "        ## Practical implementation considerations\n",
        "        [Your extraction here]\n",
        "\n",
        "        Be thorough but concise.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analyze a standard document and extract its key elements.\n",
        "\n",
        "        Args:\n",
        "            standard: The standard document to analyze.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the extracted key elements.\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {standard.name}\\n\\nContent:\\n{standard.content}\")\n",
        "        ])\n",
        "\n",
        "        result_text = self._run_llm_chain(prompt)\n",
        "\n",
        "        # Using NewBaseAgent's _extract_section for consistency if this agent were to be refactored later\n",
        "        # For now, it uses its own. Let's assume its own _extract_section is similar to NewBaseAgent's.\n",
        "        # If this agent were to inherit NewBaseAgent, this would be self._extract_section.\n",
        "        # For now, keeping its own _extract_section method.\n",
        "\n",
        "        parsed_result = {\n",
        "            \"standard_name\": standard.name,\n",
        "            \"review_result\": result_text, # This is the full text from LLM\n",
        "            \"core_principles\": self._extract_section(result_text, \"Core principles and objectives\"),\n",
        "            \"key_definitions\": self._extract_section(result_text, \"Key definitions and terminology\"),\n",
        "            \"main_requirements\": self._extract_section(result_text, \"Main requirements and procedures\"),\n",
        "            \"compliance_criteria\": self._extract_section(result_text, \"Compliance criteria and guidelines\"),\n",
        "            \"implementation_considerations\": self._extract_section(result_text, \"Practical implementation considerations\")\n",
        "        }\n",
        "        self.log_execution(f\"Standard: {standard.name}\", parsed_result.get(\"core_principles\",\"\"), start_time)\n",
        "        return parsed_result\n",
        "\n",
        "\n",
        "    def _extract_section(self, text: str, section_name: str) -> str:\n",
        "        \"\"\"Helper method to extract specific sections from the review result using regex.\"\"\"\n",
        "        # This regex tries to match markdown headings like ## Section Name or simple Section Name:\n",
        "        pattern = re.compile(\n",
        "            r\"(?:^[ \\t]*(?:#+\\s*)?)\" +  # Optional leading hashes for markdown\n",
        "            re.escape(section_name) +\n",
        "            r\"(?:\\s*[:#]*\\s*\\n)\" +      # Optional colon or hashes, then newline\n",
        "            r\"(.*?)\" +                  # Capture content (non-greedy)\n",
        "            r\"(?=(?:^[ \\t]*(?:#+\\s*)?\\w)|^\\Z)\",  # Until next heading or end of string\n",
        "            re.DOTALL | re.IGNORECASE | re.MULTILINE\n",
        "        )\n",
        "        match = pattern.search(text)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        self.logger.warning(f\"ReviewAgent: Section '{section_name}' not found. Text searched (first 300): {text[:300]}\")\n",
        "        return f\"Section '{section_name}' not found or parsing error.\"\n",
        "\n",
        "\n",
        "class EnhancementAgent(BaseAgent):\n",
        "    \"\"\"Agent tasked with proposing modifications or enhancements to standards.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"EnhancementAgent\",\n",
        "            description=\"Proposes AI-driven modifications and enhancements to AAOIFI standards.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an AI expert specializing in Islamic finance and AAOIFI standards enhancement.\n",
        "        Your task is to propose thoughtful modifications and enhancements to the standard based\n",
        "        on the review provided.\n",
        "\n",
        "        Consider the following aspects in your proposals. Use clear Markdown subheadings for each aspect (e.g., ### Clarity improvements):\n",
        "        ### Clarity improvements\n",
        "        [Suggestions]\n",
        "        ### Modern context adaptations\n",
        "        [Suggestions]\n",
        "        ### Technological integration\n",
        "        [Suggestions]\n",
        "        ### Cross-reference enhancements\n",
        "        [Suggestions]\n",
        "        ### Practical implementation\n",
        "        [Suggestions]\n",
        "\n",
        "        For each suggestion, provide:\n",
        "        - The specific section or clause being enhanced (if applicable, otherwise general proposal)\n",
        "        - The current text or concept (if applicable, very brief summary)\n",
        "        - Your proposed modification or addition\n",
        "        - A brief justification explaining the benefit of your enhancement\n",
        "\n",
        "        Ensure all suggestions maintain strict compliance with Shariah principles.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        review_text_summary = review_result.get(\"review_result\", \"No review summary available.\")\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\n\\nSummary of Standard Review:\\n{review_text_summary}\")\n",
        "        ])\n",
        "        result_text = self._run_llm_chain(prompt)\n",
        "\n",
        "        output = {\n",
        "            \"standard_name\": review_result[\"standard_name\"],\n",
        "            \"enhancement_proposals\": result_text\n",
        "        }\n",
        "        self.log_execution(f\"Review for {review_result['standard_name']}\", result_text, start_time)\n",
        "        return output\n",
        "\n",
        "class ValidationAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"ValidationAgent\",\n",
        "            description=\"Validates proposed changes based on Shariah compliance and practicality.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a senior Shariah scholar and AAOIFI standards expert. Your role is to validate\n",
        "        proposed enhancements. For each proposed enhancement, evaluate:\n",
        "        1. Shariah Compliance\n",
        "        2. Technical Accuracy\n",
        "        3. Practical Applicability\n",
        "        4. Consistency\n",
        "        5. Value Addition\n",
        "        For each proposal, provide: Your assessment (Approved/Rejected/Needs Modification), Justification, Suggested refinements if \"Needs Modification\".\n",
        "        Structure your response clearly. Start with a main heading: ## Validation Assessment\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, enhancement_result: Dict[str, Any], original_review: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        review_text_summary = original_review.get(\"review_result\", \"No review summary available.\")\n",
        "        enhancement_proposals_text = enhancement_result.get(\"enhancement_proposals\", \"No enhancement proposals available.\")\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {enhancement_result['standard_name']}\n",
        "            Original Standard Review Summary:\n",
        "            {review_text_summary}\n",
        "            Proposed Enhancements to Validate:\n",
        "            {enhancement_proposals_text}\n",
        "            \"\"\")\n",
        "        ])\n",
        "        result_text = self._run_llm_chain(prompt)\n",
        "        output = {\n",
        "            \"standard_name\": enhancement_result[\"standard_name\"],\n",
        "            \"validation_result\": result_text\n",
        "        }\n",
        "        self.log_execution(f\"Enhancements for {enhancement_result['standard_name']}\", result_text, start_time)\n",
        "        return output\n",
        "\n",
        "\n",
        "class FinalReportAgent(BaseAgent): # This is the original FinalReportAgent from scrap (3).py\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"FinalReportAgent\",\n",
        "            description=\"Compiles all findings and recommendations into a comprehensive report.\",\n",
        "            model_name=Config.GPT4_MODEL\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are a professional report writer specializing in Islamic finance standards. Your task is to\n",
        "        compile all the findings, enhancements, and validations into a comprehensive, well-structured report\n",
        "        in Markdown format.\n",
        "\n",
        "        Use the following main Markdown headings (e.g., ## Executive Summary) for each section:\n",
        "        - Executive Summary\n",
        "        - Standard Overview\n",
        "        - Key Findings from Review\n",
        "        - Proposed Enhancements\n",
        "        - Validation Results\n",
        "        - Consolidated Recommendations\n",
        "        - Implementation Considerations\n",
        "        - Conclusion\n",
        "\n",
        "        Write in a professional, clear, and objective style.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {all_results['standard_name']}\n",
        "            Full Text of Standard Review:\n",
        "            {all_results.get('review_text', all_results.get('review_result', 'N/A'))}\n",
        "            Full Text of Proposed Enhancements:\n",
        "            {all_results.get('enhancements_text', all_results.get('enhancement_proposals', 'N/A'))}\n",
        "            Full Text of Validation of Enhancements:\n",
        "            {all_results.get('validation_text', all_results.get('validation_result', 'N/A'))}\n",
        "            \"\"\")\n",
        "        ])\n",
        "        report_text = self._run_llm_chain(prompt)\n",
        "        output = {\n",
        "            \"standard_name\": all_results[\"standard_name\"],\n",
        "            \"final_report\": report_text\n",
        "        }\n",
        "        self.log_execution(f\"All results for {all_results['standard_name']}\", report_text, start_time)\n",
        "        return output\n",
        "\n",
        "\n",
        "class VectorDBManager(VectorDBManager):\n",
        "    def __init__(self, db_directory: str = None, collection_name: str = config.COLLECTION_NAME):\n",
        "        super().__init__(db_directory, collection_name)\n",
        "\n",
        "\n",
        "class AAOIFIStandardsEnhancementSystem(AAOIFIStandardsEnhancementSystem):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "# --- Functions for recreating vector database safely (from scrap (3).py) ---\n",
        "def safely_recreate_vector_database(documents):\n",
        "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL, openai_api_key=Config.OPENAI_API_KEY)\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
        "    base_db_parent_dir = \"aaoifi_vector_db_versions\"\n",
        "    os.makedirs(base_db_parent_dir, exist_ok=True)\n",
        "    new_db_directory = os.path.join(base_db_parent_dir, f\"db_{timestamp}_{random_str}\")\n",
        "\n",
        "    logger.info(f\"Creating new database in directory: {new_db_directory}\")\n",
        "    os.makedirs(new_db_directory, exist_ok=True)\n",
        "\n",
        "    client = chromadb.PersistentClient(path=new_db_directory)\n",
        "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
        "\n",
        "    batch_size = 100\n",
        "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        batch_documents = documents[i:i+batch_size]\n",
        "        current_batch_num = (i // batch_size) + 1\n",
        "        logger.info(f\"Processing batch {current_batch_num}/{num_batches} for new vector database...\")\n",
        "        texts = [doc[\"content\"] for doc in batch_documents]\n",
        "        ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}\" for doc in batch_documents]\n",
        "        if len(ids) != len(set(ids)):\n",
        "            logger.warning(f\"Warning: Duplicate IDs generated in batch {current_batch_num}. Appending index.\")\n",
        "            ids = [f\"{id_}_{j}\" for j, id_ in enumerate(ids)]\n",
        "        metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
        "\n",
        "        if not texts: continue\n",
        "        try:\n",
        "            embeds = embeddings.embed_documents(texts)\n",
        "            collection.add(embeddings=embeds, documents=texts, ids=ids, metadatas=metadatas)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing batch {current_batch_num}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    logger.info(f\"Created new vector database in '{new_db_directory}'\")\n",
        "    config.DB_DIRECTORY = new_db_directory\n",
        "    logger.info(f\"Updated global config.DB_DIRECTORY to: {config.DB_DIRECTORY}\")\n",
        "    return client\n",
        "\n",
        "def process_pdfs_safe():\n",
        "    PDF_FOLDER = config.PDF_FOLDER\n",
        "    if not os.path.exists(PDF_FOLDER) or not os.listdir(PDF_FOLDER):\n",
        "        logger.error(f\"PDF folder '{PDF_FOLDER}' is missing or empty.\")\n",
        "        return None\n",
        "    all_documents = []\n",
        "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
        "    if not pdf_files:\n",
        "        logger.info(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
        "        return None\n",
        "    logger.info(f\"Found {len(pdf_files)} PDF files.\")\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
        "        standard_name = os.path.splitext(pdf_file)[0]\n",
        "        logger.info(f\"Processing {pdf_file}...\")\n",
        "        raw_text = extract_text_from_pdf(pdf_path)\n",
        "        cleaned_text = clean_text(raw_text)\n",
        "        doc_chunks = split_text_into_chunks(cleaned_text, standard_name) # Renamed variable\n",
        "        all_documents.extend(doc_chunks)\n",
        "        logger.info(f\"Extracted {len(doc_chunks)} chunks from {pdf_file}\")\n",
        "\n",
        "    if not all_documents: return None\n",
        "    logger.info(f\"Total chunks extracted: {len(all_documents)}\")\n",
        "    client = safely_recreate_vector_database(all_documents)\n",
        "    return client\n",
        "\n",
        "# --- END OF REUSED/ADAPTED CODE FROM scrap (3).py ---\n",
        "\n",
        "# --- START OF USER'S NEW CODE BLOCK (with modifications for integration) ---\n",
        "\n",
        "# NewBaseAgent for the user's new agent structure\n",
        "class NewBaseAgent:\n",
        "    def __init__(self, name: str, description: str, agent_type: str, model_name: str = Config.GPT4_MODEL):\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.agent_type = agent_type\n",
        "        self.model_name = model_name\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "        if not Config.OPENAI_API_KEY:\n",
        "            self.logger.error(\"OpenAI API key not found in Config.\")\n",
        "            raise ValueError(\"OpenAI API key not configured.\")\n",
        "        self.llm = ChatOpenAI(model_name=self.model_name, openai_api_key=Config.OPENAI_API_KEY, temperature=0.2)\n",
        "\n",
        "    def _run_chain(self, messages: List[Any]) -> str:\n",
        "        prompt = ChatPromptTemplate.from_messages(messages)\n",
        "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
        "        try:\n",
        "            result = chain.run({})\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error running LLM chain for agent {self.name}: {e}\")\n",
        "            return f\"Error in LLM call: {e}\"\n",
        "        return result\n",
        "\n",
        "    def log_execution(self, input_summary: str, output_summary: str, start_time: float):\n",
        "        end_time = time.time()\n",
        "        self.logger.info(\n",
        "            f\"Agent '{self.name}' (Type: {self.agent_type}) executed. \"\n",
        "            f\"Duration: {end_time - start_time:.2f}s\"\n",
        "        )\n",
        "\n",
        "    def _extract_section(self, text: str, section_name: str) -> str:\n",
        "        \"\"\"Extract a section from text using flexible regex for headings.\"\"\"\n",
        "        self.logger.debug(f\"Attempting to extract section: '{section_name}' from text (length {len(text)}).\")\n",
        "        # Regex to find a heading (e.g., \"1. Section Name\", \"## Section Name\", \"**Section Name:**\")\n",
        "        # and capture text until the next similar heading or end of string.\n",
        "        # Make section_name matching more flexible (e.g. ignore case, allow spaces for underscores)\n",
        "        # The section_name passed in might be 'clarity_improvements', but LLM might output 'Clarity Improvements' or '### Clarity improvements'\n",
        "\n",
        "        # Prepare section_name for regex: escape it and replace underscores with spaces (to match natural language headings)\n",
        "        # Also, allow optional plural 's'\n",
        "        processed_section_name = re.escape(section_name.replace('_', ' ')).replace(r'\\\\ ', r'\\s+')\n",
        "        if processed_section_name.endswith('s'):\n",
        "            processed_section_name = processed_section_name[:-1] + 's?' # make last s optional\n",
        "\n",
        "        pattern = re.compile(\n",
        "            r\"(?:^[ \\t]*(?:\\d+\\.?\\s*|#{1,6}\\s*|[*_]{1,3}\\s*)?)\" +  # Start of line, optional markers (numbers, #, *, _)\n",
        "            r\"(?:\" + processed_section_name + r\")\" +  # The section name itself, case insensitive due to IGNORECASE\n",
        "            r\"(?:[*_]{1,3}\\s*[:#*]*\\s*\\n)\" +                     # Optional markers, colon/hashes, then newline\n",
        "            r\"(.*?)\" +                                  # Capture content (non-greedy)\n",
        "            r\"(?=(?:^[ \\t]*(?:\\d+\\.?\\s*|#{1,6}\\s*|[*_]{1,3}\\s*)?\\w)|^\\Z)\",  # Until next heading or EOS\n",
        "            re.DOTALL | re.IGNORECASE | re.MULTILINE\n",
        "        )\n",
        "        match = pattern.search(text)\n",
        "        if match:\n",
        "            extracted = match.group(1).strip()\n",
        "            self.logger.debug(f\"Successfully extracted section: '{section_name}'. Length: {len(extracted)}\")\n",
        "            return extracted\n",
        "\n",
        "        self.logger.warning(f\"Section '{section_name}' (processed as '{processed_section_name}') not found. Text searched (first 300 chars): {text[:300]}\")\n",
        "        return f\"Section '{section_name}' not found.\"\n",
        "\n",
        "# --- STUB AGENTS for AAOIFIStandardsSystem ---\n",
        "class DocumentReviewAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Document Review Agent\", \"Reviews standard documents\", \"review\")\n",
        "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        # This is a stub. A real implementation would use LLM.\n",
        "        # For now, it uses the existing ReviewAgent from scrap (3).py for functionality.\n",
        "        original_review_agent = ReviewAgent() # Use the one from scrap (3).py\n",
        "        return original_review_agent.execute(standard)\n",
        "\n",
        "class StandardAnalysisAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Standard Analysis Agent\", \"Analyzes standards for challenges and improvements\", \"analysis\", model_name=Config.GPT35_MODEL)\n",
        "        self.system_prompt = \"\"\"You are an AI analyst. Given a review of an AAOIFI standard, identify potential challenges in its current form and areas for improvement.\n",
        "        Respond using the following Markdown headings for each section:\n",
        "        ## Challenges\n",
        "        [List challenges here]\n",
        "        ## Improvement Areas\n",
        "        [List improvement areas here]\n",
        "        \"\"\"\n",
        "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\nReview: {review_result.get('review_result', 'N/A')}\")\n",
        "        ]\n",
        "        analysis_text = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": review_result['standard_name'],\n",
        "            \"full_analysis_text\": analysis_text,\n",
        "            \"challenges\": self._extract_section(analysis_text, \"Challenges\"),\n",
        "            \"improvement_areas\": self._extract_section(analysis_text, \"Improvement Areas\")\n",
        "        }\n",
        "        self.log_execution(f\"Review for {review_result['standard_name']}\", analysis_text, start_time)\n",
        "        return result\n",
        "\n",
        "class EnhancementAgentNew(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Enhancement Agent (New)\", \"Proposes enhancements based on review and analysis\", \"enhancement\")\n",
        "        self.system_prompt = \"\"\"You are an AI expert for AAOIFI standards. Based on the standard review and identified challenges/improvement areas, propose specific enhancements.\n",
        "        Organize proposals into the following categories using clear Markdown subheadings (e.g., ### Clarity improvements). Ensure each category heading is EXACTLY as written below (using underscores where shown) and on its own line, followed by the content on the next lines:\n",
        "        ### clarity_improvements\n",
        "        [Suggestions...]\n",
        "        ### modern_adaptations\n",
        "        [Suggestions...]\n",
        "        ### tech_integration\n",
        "        [Suggestions...]\n",
        "        ### cross_references\n",
        "        [Suggestions...]\n",
        "        ### implementation_guidance\n",
        "        [Suggestions...]\n",
        "        For each proposal under these categories: specify the section, current concept, proposed modification, and justification.\"\"\"\n",
        "\n",
        "    def execute(self, review_result: Dict[str, Any], analysis_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {review_result['standard_name']}\n",
        "            Review Summary: {review_result.get('review_result', 'N/A')[:1000]}...\n",
        "            Identified Challenges: {analysis_result.get('challenges', 'N/A')[:500]}...\n",
        "            Identified Improvement Areas: {analysis_result.get('improvement_areas', 'N/A')[:500]}...\n",
        "            Please generate enhancement proposals ensuring to use the exact specified subheadings for each category.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        enhancement_text = self._run_chain(messages)\n",
        "\n",
        "        result = {\n",
        "            \"standard_name\": review_result['standard_name'],\n",
        "            \"enhancement_proposals\": enhancement_text, # Full text for reference\n",
        "            # Now use the _extract_section method with the exact keys used in the prompt\n",
        "            \"clarity_improvements\": self._extract_section(enhancement_text, \"clarity_improvements\"),\n",
        "            \"modern_adaptations\": self._extract_section(enhancement_text, \"modern_adaptations\"),\n",
        "            \"tech_integration\": self._extract_section(enhancement_text, \"tech_integration\"),\n",
        "            \"cross_references\": self._extract_section(enhancement_text, \"cross_references\"),\n",
        "            \"implementation_guidance\": self._extract_section(enhancement_text, \"implementation_guidance\")\n",
        "        }\n",
        "        self.log_execution(f\"Analysis for {review_result['standard_name']}\", enhancement_text, start_time)\n",
        "        return result\n",
        "\n",
        "class ShariahComplianceAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Shariah Compliance Agent\", \"Assesses Shariah compliance of proposals\", \"shariah_compliance\", model_name=Config.GPT4_MODEL)\n",
        "        self.system_prompt = \"\"\"You are a Shariah scholar. Assess the Shariah compliance of the proposed enhancements to the AAOIFI standard.\n",
        "        Provide a general shariah_assessment summary under a heading '## Shariah Assessment Summary'.\n",
        "        Then, for each specific category of enhancement listed below, provide an overall_ruling (Approved, Conditionally Approved, Requires Modification, Rejected) and a brief justification.\n",
        "        Format this as a Markdown list, with each item clearly starting with the category name (using underscores) followed by a colon and the ruling, then a hyphen and justification. Example:\n",
        "        - clarity_improvements: Approved - The proposed changes enhance understanding without violating Shariah principles.\n",
        "        - modern_adaptations: Conditionally Approved - Adaptations are acceptable if X condition is met.\n",
        "        - tech_integration: Requires Modification - Current proposal for Y needs adjustment Z to be compliant.\n",
        "        - cross_references: Approved - Beneficial for consistency.\n",
        "        - implementation_guidance: Approved - Practical and compliant.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, enhancement_result: Dict[str, Any], review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Prepare summaries of enhancements for the prompt\n",
        "        enhancement_summaries_for_prompt = {}\n",
        "        enhancement_categories_keys = [\n",
        "            \"clarity_improvements\", \"modern_adaptations\",\n",
        "            \"tech_integration\", \"cross_references\", \"implementation_guidance\"\n",
        "        ]\n",
        "        for key in enhancement_categories_keys:\n",
        "            content = enhancement_result.get(key, \"N/A\")\n",
        "            if content.startswith(\"Section\") and \"not found\" in content: # if extraction failed\n",
        "                 enhancement_summaries_for_prompt[key] = f\"Content for {key} was not properly extracted previously.\"\n",
        "            else:\n",
        "                 enhancement_summaries_for_prompt[key] = (content[:200] + \"...\" if len(content) > 200 else content)\n",
        "\n",
        "        prompt_human_content = f\"\"\"\n",
        "        Standard Name: {enhancement_result['standard_name']}\n",
        "        Original Review Summary (excerpt): {review_result.get('review_result', 'N/A')[:500]}...\n",
        "\n",
        "        Proposed Enhancements Summaries:\n",
        "        \"\"\"\n",
        "        for key, summary in enhancement_summaries_for_prompt.items():\n",
        "            prompt_human_content += f\"\\n{key.replace('_',' ').title()}:\\n{summary}\\n\"\n",
        "        prompt_human_content += \"\\nPlease provide Shariah assessment as per the specified format (Markdown list with category_name: RULING - Justification).\"\n",
        "\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=prompt_human_content)\n",
        "        ]\n",
        "        shariah_text = self._run_chain(messages)\n",
        "\n",
        "        overall_rulings = {}\n",
        "        for key in enhancement_categories_keys:\n",
        "            # Regex to find \"category_name: RULING - Justification\"\n",
        "            # The category name in regex should be flexible (spaces or underscores, case insensitive)\n",
        "            pattern_category_name = key.replace('_', r'[\\s_]*') # Allows spaces or underscores between words\n",
        "            pattern = re.compile(rf\"^\\s*[-*]?\\s*{pattern_category_name}\\s*:\\s*([A-Za-z\\s]+)\\s*-\", re.IGNORECASE | re.MULTILINE)\n",
        "            match = pattern.search(shariah_text)\n",
        "            if match:\n",
        "                overall_rulings[key] = match.group(1).strip()\n",
        "                self.logger.info(f\"Shariah ruling parsed for '{key}': {overall_rulings[key]}\")\n",
        "            else:\n",
        "                self.logger.warning(f\"Could not parse ruling for category '{key}' from Shariah assessment text using pattern for '{pattern_category_name}'. Full text searched (first 500 char of shariah_text): {shariah_text[:500]}\")\n",
        "                overall_rulings[key] = \"Not specifically assessed\"\n",
        "\n",
        "        result = {\n",
        "            \"standard_name\": enhancement_result['standard_name'],\n",
        "            \"shariah_assessment\": self._extract_section(shariah_text, \"Shariah Assessment Summary\") or shariah_text,\n",
        "            \"overall_ruling\": overall_rulings\n",
        "        }\n",
        "        self.log_execution(f\"Enhancements for {enhancement_result['standard_name']}\", shariah_text, start_time)\n",
        "        return result\n",
        "\n",
        "\n",
        "class ValidationAgentNew(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\"Validation Agent (New)\", \"Validates practical aspects of proposals\", \"validation\")\n",
        "        self.system_prompt = \"\"\"You are an AAOIFI standards expert. Validate the proposed enhancements considering their Shariah assessment.\n",
        "        Focus on practical applicability, consistency, and value addition.\n",
        "        Provide an overall validation_result summary under a heading '## Overall Validation Summary'.\n",
        "        Then, for each category of enhancement, provide an implementation_assessment under subheadings like '### Clarity Improvements Assessment'.\n",
        "        Use the exact category names (e.g., \"Clarity Improvements Assessment\", \"Modern Adaptations Assessment\", etc.) for the subheadings.\n",
        "        \"\"\"\n",
        "    def execute(self, enhancement_result: Dict[str, Any], shariah_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {enhancement_result['standard_name']}\n",
        "            Proposed Enhancements (Full text, may be long): {enhancement_result.get('enhancement_proposals', 'N/A')[:2000]}...\n",
        "            Shariah Assessment Summary: {shariah_result.get('shariah_assessment', 'N/A')}\n",
        "            Shariah Rulings per Category: {json.dumps(shariah_result.get('overall_ruling',{}), indent=2)}\n",
        "            Please provide practical validation using the specified heading structure.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        validation_text = self._run_chain(messages)\n",
        "\n",
        "        result = {\n",
        "            \"standard_name\": enhancement_result['standard_name'],\n",
        "            \"validation_result\": self._extract_section(validation_text, \"Overall Validation Summary\") or validation_text,\n",
        "            \"implementation_assessments\": { # These keys must match what _extract_section is called with\n",
        "                \"clarity_improvements\": self._extract_section(validation_text, \"Clarity Improvements Assessment\"),\n",
        "                \"modern_adaptations\": self._extract_section(validation_text, \"Modern Adaptations Assessment\"),\n",
        "                \"tech_integration\": self._extract_section(validation_text, \"Tech Integration Assessment\"),\n",
        "                \"cross_references\": self._extract_section(validation_text, \"Cross References Assessment\"),\n",
        "                \"implementation_guidance\": self._extract_section(validation_text, \"Implementation Guidance Assessment\"),\n",
        "            }\n",
        "        }\n",
        "        self.log_execution(f\"Shariah assessment for {enhancement_result['standard_name']}\", validation_text, start_time)\n",
        "        return result\n",
        "# --- End of STUB AGENTS ---\n",
        "\n",
        "class ReportGenerationAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"Report Generation Agent\",\n",
        "            description=\"Synthesizes findings and recommendations into comprehensive reports.\",\n",
        "            agent_type=\"report\"\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert report writer specializing in Islamic finance standards. Your task is to\n",
        "        synthesize all findings, analyses, and recommendations into a comprehensive, well-structured\n",
        "        report that presents the key insights in a clear and actionable format.\n",
        "\n",
        "        Your report should include the following sections, using clear Markdown headings (e.g., ## Executive Summary). Ensure these headings are EXACTLY as listed:\n",
        "        - Executive Summary\n",
        "        - Standard Analysis\n",
        "        - Enhancement Recommendations\n",
        "        - Validation Results\n",
        "        - Implementation Roadmap\n",
        "        - Appendices (if applicable, mention what could be in appendices)\n",
        "\n",
        "        Format the report professionally. Use concise, precise language.\n",
        "        The report should be comprehensive but accessible to Islamic finance professionals.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self,\n",
        "                review_result: Dict[str, Any],\n",
        "                analysis_result: Dict[str, Any],\n",
        "                enhancement_result: Dict[str, Any],\n",
        "                shariah_result: Dict[str, Any],\n",
        "                validation_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        standard_name = review_result.get(\"standard_name\", \"Unknown Standard\")\n",
        "\n",
        "        core_principles = review_result.get(\"core_principles\", \"Not available\")\n",
        "        main_requirements = review_result.get(\"main_requirements\", \"Not available\")\n",
        "        challenges = analysis_result.get(\"challenges\", \"Not available\")\n",
        "        improvement_areas = analysis_result.get(\"improvement_areas\", \"Not available\")\n",
        "        enhancement_proposals_full = enhancement_result.get(\"enhancement_proposals\", \"Not available\") # Full text\n",
        "        shariah_assessment_summary = shariah_result.get(\"shariah_assessment\", \"Not available\")\n",
        "        shariah_rulings_category = shariah_result.get(\"overall_ruling\", {})\n",
        "        validation_assessment_summary = validation_result.get(\"validation_result\", \"Not available\")\n",
        "        implementation_assessments_category = validation_result.get(\"implementation_assessments\", {})\n",
        "\n",
        "        # Create a more structured summary for the prompt\n",
        "        enhancement_summary_for_prompt = f\"\"\"\n",
        "        Clarity Improvements details: {enhancement_result.get('clarity_improvements', 'N/A')}\n",
        "        Modern Adaptations details: {enhancement_result.get('modern_adaptations', 'N/A')}\n",
        "        Tech Integration details: {enhancement_result.get('tech_integration', 'N/A')}\n",
        "        Cross References details: {enhancement_result.get('cross_references', 'N/A')}\n",
        "        Implementation Guidance details: {enhancement_result.get('implementation_guidance', 'N/A')}\n",
        "        (Full original proposals text length: {len(enhancement_proposals_full)})\n",
        "        \"\"\"\n",
        "\n",
        "        shariah_summary_for_prompt = f\"\"\"\n",
        "        Overall Shariah Assessment Summary: {shariah_assessment_summary}\n",
        "        Rulings per Category: {json.dumps(shariah_rulings_category, indent=2)}\n",
        "        \"\"\"\n",
        "\n",
        "        validation_summary_for_prompt = f\"\"\"\n",
        "        Overall Validation Summary: {validation_assessment_summary}\n",
        "        Implementation Assessments per Category: {json.dumps(implementation_assessments_category, indent=2)}\n",
        "        \"\"\"\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "\n",
        "            Core Principles from Review:\n",
        "            {core_principles}\n",
        "\n",
        "            Main Requirements from Review:\n",
        "            {main_requirements}\n",
        "\n",
        "            Identified Challenges from Analysis:\n",
        "            {challenges}\n",
        "\n",
        "            Identified Improvement Areas from Analysis:\n",
        "            {improvement_areas}\n",
        "\n",
        "            Detailed Proposed Enhancements by Category:\n",
        "            {enhancement_summary_for_prompt}\n",
        "\n",
        "            Summary of Shariah Assessment:\n",
        "            {shariah_summary_for_prompt}\n",
        "\n",
        "            Summary of Validation Assessment:\n",
        "            {validation_summary_for_prompt}\n",
        "\n",
        "            Please generate a comprehensive report synthesizing all this information.\n",
        "            Ensure the report strictly follows the requested Markdown heading structure for each section.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        report_text = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"full_report\": report_text,\n",
        "            \"executive_summary\": self._extract_section(report_text, \"Executive Summary\"),\n",
        "            \"standard_analysis\": self._extract_section(report_text, \"Standard Analysis\"),\n",
        "            \"enhancement_recommendations\": self._extract_section(report_text, \"Enhancement Recommendations\"),\n",
        "            \"validation_results\": self._extract_section(report_text, \"Validation Results\"),\n",
        "            \"implementation_roadmap\": self._extract_section(report_text, \"Implementation Roadmap\")\n",
        "        }\n",
        "        self.log_execution(f\"All results for {standard_name}\", report_text, start_time)\n",
        "        return result\n",
        "\n",
        "class VisualizationAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"Visualization Agent\",\n",
        "            description=\"Creates visual representations of findings and recommendations.\",\n",
        "            agent_type=\"visualization\"\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert in data visualization and information design specializing in financial standards.\n",
        "        Your task is to design clear and informative visualizations that effectively communicate the\n",
        "        key findings and recommendations from the analysis.\n",
        "\n",
        "        For the given analysis results, create text descriptions and chart specifications for the following, using clear Markdown headings for each (e.g., ## Enhancement Impact Matrix). Ensure these headings are EXACTLY as listed:\n",
        "        - Enhancement Impact Matrix\n",
        "        - Shariah Compliance Visualization\n",
        "        - Implementation Roadmap Timeline\n",
        "        - Stakeholder Impact Analysis\n",
        "        For each visualization, provide: A clear title and description, Detailed specification of the visualization type and key elements, The data structure needed (example format), Brief interpretation.\n",
        "        Design visualizations that are both informative and accessible.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self,\n",
        "                enhancement_result: Dict[str, Any],\n",
        "                shariah_result: Dict[str, Any],\n",
        "                validation_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        standard_name = enhancement_result.get(\"standard_name\", \"Unknown Standard\")\n",
        "\n",
        "        # Prepare summaries for the prompt\n",
        "        enhancement_summary_prompt = {cat: (enhancement_result.get(cat, \"\")[:100] + \"...\") for cat in enhancement_result if cat not in [\"standard_name\", \"enhancement_proposals\"]}\n",
        "        shariah_summary_prompt = shariah_result.get(\"overall_ruling\", {})\n",
        "        validation_summary_prompt = validation_result.get(\"implementation_assessments\", {})\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "            Enhancement Categories Summary: {json.dumps(enhancement_summary_prompt, indent=2)}\n",
        "            Shariah Compliance Rulings Summary: {json.dumps(shariah_summary_prompt, indent=2)}\n",
        "            Implementation Assessments Summary: {json.dumps(validation_summary_prompt, indent=2)}\n",
        "            Please generate visualization specifications based on this information.\n",
        "            Ensure you use the specified Markdown headings for each visualization type.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        visualization_text = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"visualization_specifications\": visualization_text,\n",
        "            \"enhancement_impact_matrix\": self._extract_section(visualization_text, \"Enhancement Impact Matrix\"),\n",
        "            \"shariah_compliance_visualization\": self._extract_section(visualization_text, \"Shariah Compliance Visualization\"),\n",
        "            \"implementation_roadmap_timeline\": self._extract_section(visualization_text, \"Implementation Roadmap Timeline\"),\n",
        "            \"stakeholder_impact_analysis\": self._extract_section(visualization_text, \"Stakeholder Impact Analysis\")\n",
        "        }\n",
        "        self.log_execution(f\"Results for {standard_name}\", visualization_text, start_time)\n",
        "        return result\n",
        "\n",
        "class FeedbackAgent(NewBaseAgent):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            name=\"Feedback Agent\",\n",
        "            description=\"Processes stakeholder feedback and suggests refinements to proposals.\",\n",
        "            agent_type=\"feedback\"\n",
        "        )\n",
        "        self.system_prompt = \"\"\"\n",
        "        You are an expert facilitator specializing in stakeholder feedback collection and integration\n",
        "        for Islamic finance standards. Your role is to process feedback on proposed standard enhancements\n",
        "        and suggest refinements based on stakeholder input.\n",
        "\n",
        "        When processing feedback, provide output under the following Markdown headings (e.g., ## Stakeholder Categories). Ensure these headings are EXACTLY as listed:\n",
        "        - Stakeholder Categories\n",
        "        - Feedback Patterns and Themes\n",
        "        - Feedback Validity Analysis\n",
        "        - Recommended Refinements\n",
        "        - Feedback Incorporation Process Suggestions\n",
        "\n",
        "        Focus on constructive integration of feedback while maintaining the core objectives\n",
        "        of the standard and ensuring Shariah compliance.\n",
        "        \"\"\"\n",
        "\n",
        "    def execute(self, feedback: str, enhancement_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start_time = time.time()\n",
        "        standard_name = enhancement_result.get(\"standard_name\", \"Unknown Standard\")\n",
        "        enhancement_proposals = enhancement_result.get(\"enhancement_proposals\", \"Not Available\")\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=self.system_prompt),\n",
        "            HumanMessage(content=f\"\"\"\n",
        "            Standard Name: {standard_name}\n",
        "            Original Enhancement Proposals (Full Text):\n",
        "            {enhancement_proposals}\n",
        "\n",
        "            Stakeholder Feedback Provided:\n",
        "            {feedback}\n",
        "\n",
        "            Please analyze this feedback and suggest refinements to the proposals, structuring your response with the requested Markdown headings.\n",
        "            \"\"\")\n",
        "        ]\n",
        "        feedback_analysis = self._run_chain(messages)\n",
        "        result = {\n",
        "            \"standard_name\": standard_name,\n",
        "            \"feedback_analysis\": feedback_analysis, # Full text\n",
        "            \"stakeholder_categories\": self._extract_section(feedback_analysis, \"Stakeholder Categories\"),\n",
        "            \"feedback_patterns\": self._extract_section(feedback_analysis, \"Feedback Patterns and Themes\"),\n",
        "            \"feedback_validity\": self._extract_section(feedback_analysis, \"Feedback Validity Analysis\"),\n",
        "            \"recommended_refinements\": self._extract_section(feedback_analysis, \"Recommended Refinements\"),\n",
        "            \"incorporation_process\": self._extract_section(feedback_analysis, \"Feedback Incorporation Process Suggestions\")\n",
        "        }\n",
        "        self.log_execution(f\"Feedback for {standard_name}\", feedback_analysis, start_time)\n",
        "        return result\n",
        "\n",
        "# Vector Database Integration (User's new version)\n",
        "class VectorDBManagerNew:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        if not Config.OPENAI_API_KEY:\n",
        "            self.logger.error(\"OpenAI API Key not found in Config for VectorDBManagerNew.\")\n",
        "            raise ValueError(\"OpenAI API Key not configured.\")\n",
        "\n",
        "        self.client = chromadb.PersistentClient(path=Config.DB_DIRECTORY)\n",
        "        self.embeddings = OpenAIEmbeddings(\n",
        "            model=Config.EMBEDDING_MODEL,\n",
        "            openai_api_key=Config.OPENAI_API_KEY\n",
        "        )\n",
        "        try:\n",
        "            self.collection = self.client.get_or_create_collection(name=Config.COLLECTION_NAME)\n",
        "            self.logger.info(f\"VectorDBManagerNew: Connected to/created collection: {Config.COLLECTION_NAME} at {Config.DB_DIRECTORY}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"VectorDBManagerNew: Error getting/creating collection: {e}\")\n",
        "            raise\n",
        "\n",
        "    def add_document(self, standard: StandardDocument) -> List[str]:\n",
        "        doc_chunks = DocumentProcessor.split_text_into_chunks(standard.content, standard.name)\n",
        "        if not doc_chunks:\n",
        "            self.logger.warning(f\"No chunks generated for document {standard.name}. Skipping add.\")\n",
        "            return []\n",
        "\n",
        "        chunk_texts = [chunk[\"content\"] for chunk in doc_chunks]\n",
        "        chunk_metadata = [chunk[\"metadata\"] for chunk in doc_chunks]\n",
        "        chunk_ids = [str(uuid.uuid4()) for _ in range(len(doc_chunks))]\n",
        "\n",
        "        try:\n",
        "            embeddings_list = self.embeddings.embed_documents(chunk_texts)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating embeddings for {standard.name}: {e}\")\n",
        "            return []\n",
        "\n",
        "        self.collection.add(\n",
        "            embeddings=embeddings_list,\n",
        "            documents=chunk_texts,\n",
        "            metadatas=chunk_metadata,\n",
        "            ids=chunk_ids\n",
        "        )\n",
        "        self.logger.info(f\"Added {len(doc_chunks)} chunks from {standard.name} to vector database\")\n",
        "        return chunk_ids\n",
        "\n",
        "    def search_standards(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        try:\n",
        "            query_embedding = self.embeddings.embed_query(query)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating query embedding: {e}\")\n",
        "            return []\n",
        "\n",
        "        results = self.collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
        "\n",
        "        if not results or not results.get(\"documents\") or not results[\"documents\"][0]:\n",
        "            return []\n",
        "\n",
        "        documents = results[\"documents\"][0]\n",
        "        metadatas = results[\"metadatas\"][0]\n",
        "        distances = results[\"distances\"][0]\n",
        "        result_list = []\n",
        "        for i in range(len(documents)):\n",
        "            result_list.append({\n",
        "                \"content\": documents[i],\n",
        "                \"metadata\": metadatas[i],\n",
        "                \"relevance\": 1 - distances[i] if distances[i] is not None else 0\n",
        "            })\n",
        "        return result_list\n",
        "\n",
        "# Main Orchestrator (User's new version)\n",
        "class AAOIFIStandardsSystem:\n",
        "    def __init__(self):\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "        self.logger.info(\"Initializing AAOIFI Standards Multi-Agent System (New Orchestrator)\")\n",
        "\n",
        "        self.review_agent = DocumentReviewAgent()\n",
        "        self.analysis_agent = StandardAnalysisAgent()\n",
        "        self.enhancement_agent = EnhancementAgentNew()\n",
        "        self.shariah_agent = ShariahComplianceAgent()\n",
        "        self.validation_agent = ValidationAgentNew()\n",
        "        self.report_agent = ReportGenerationAgent()\n",
        "        self.visualization_agent = VisualizationAgent()\n",
        "        self.feedback_agent = FeedbackAgent()\n",
        "\n",
        "        self.vector_db = VectorDBManagerNew()\n",
        "\n",
        "        os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "        self.logger.info(\"System initialization complete (New Orchestrator)\")\n",
        "\n",
        "    def process_standard(self, standard: StandardDocument) -> Dict[str, Any]:\n",
        "        self.logger.info(f\"Beginning processing of standard: {standard.name} (New Orchestrator)\")\n",
        "\n",
        "        self.vector_db.add_document(standard)\n",
        "\n",
        "        review_result = self.review_agent.execute(standard)\n",
        "        analysis_result = self.analysis_agent.execute(review_result)\n",
        "        enhancement_result = self.enhancement_agent.execute(review_result, analysis_result)\n",
        "        shariah_result = self.shariah_agent.execute(enhancement_result, review_result)\n",
        "        validation_result = self.validation_agent.execute(enhancement_result, shariah_result)\n",
        "        report_result = self.report_agent.execute(review_result, analysis_result, enhancement_result, shariah_result, validation_result)\n",
        "        visualization_result = self.visualization_agent.execute(enhancement_result, shariah_result, validation_result)\n",
        "\n",
        "        final_result = {\n",
        "            \"standard_name\": standard.name, \"review\": review_result, \"analysis\": analysis_result,\n",
        "            \"enhancement\": enhancement_result, \"shariah_assessment\": shariah_result,\n",
        "            \"validation\": validation_result, \"report\": report_result, \"visualizations\": visualization_result\n",
        "        }\n",
        "        self._save_results(final_result, suffix=\"new_orchestrator_results\")\n",
        "        self.logger.info(f\"Completed processing of standard: {standard.name} (New Orchestrator)\")\n",
        "        return final_result\n",
        "\n",
        "    def incorporate_feedback(self, feedback: str, enhancement_result: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        self.logger.info(\"Processing stakeholder feedback (New Orchestrator)\")\n",
        "        # Ensure enhancement_result is not None and contains expected keys\n",
        "        if enhancement_result is None:\n",
        "            enhancement_result = {\"standard_name\": \"Unknown (from feedback)\", \"enhancement_proposals\": \"Not available due to prior error\"}\n",
        "            self.logger.warning(\"Enhancement result was None for feedback incorporation. Using placeholders.\")\n",
        "\n",
        "        feedback_result = self.feedback_agent.execute(feedback, enhancement_result)\n",
        "        self._save_results(feedback_result, suffix=\"new_orchestrator_feedback\")\n",
        "        return feedback_result\n",
        "\n",
        "    def _save_results(self, results: Dict[str, Any], suffix: str = \"results\") -> None:\n",
        "        standard_name = results.get(\"standard_name\", \"unknown_standard\")\n",
        "        sanitized_name = re.sub(r'[^\\w\\-_\\.]', '_', standard_name)\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        output_path = os.path.join(Config.OUTPUT_DIR, f\"{sanitized_name}_{suffix}_{timestamp}.json\")\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "        self.logger.info(f\"Results saved to {output_path}\")\n",
        "\n",
        "# Utility functions for demonstration (User's new version)\n",
        "def load_sample_standard() -> StandardDocument:\n",
        "    sample_content = \"\"\"\n",
        "    AAOIFI Shariah Standard No. X: Murabahah to the Purchase Orderer\n",
        "\n",
        "    1. Scope of the Standard\n",
        "    This standard covers Murabahah to the Purchase Orderer transactions as practiced by Islamic financial institutions, including the conditions, procedures, rules, and modern applications. It does not cover simple Murabahah transactions that do not involve a prior promise to purchase.\n",
        "\n",
        "    2. Definition of Murabahah to the Purchase Orderer\n",
        "    Murabahah to the Purchase Orderer is a transaction where an Islamic financial institution (IFI) purchases an asset based on a promise from a customer to buy the asset from the institution on Murabahah terms (cost plus profit) after the institution has purchased it.\n",
        "\n",
        "    3. Shariah Requirements for Murabahah to the Purchase Orderer\n",
        "    3.1 The IFI must acquire ownership of the asset before selling it to the customer.\n",
        "    3.2 The IFI must bear the risks of ownership during the period between purchasing the asset and selling it to the customer.\n",
        "    3.3 The cost price and markup must be clearly disclosed to the customer.\n",
        "    3.4 The customer's promise to purchase is morally binding but not legally enforceable as a sale contract.\n",
        "    3.5 The Murabahah sale contract can only be executed after the IFI has acquired ownership of the asset.\n",
        "\n",
        "    4. Procedures for Murabahah to the Purchase Orderer\n",
        "    4.1 The customer identifies the asset they wish to purchase and requests the IFI to purchase it.\n",
        "    4.2 The IFI and customer enter into a promise agreement, where the customer promises to purchase the asset after the IFI acquires it.\n",
        "    4.3 The IFI purchases the asset from the supplier.\n",
        "    4.4 The IFI informs the customer that it has acquired the asset and offers to sell it on Murabahah terms.\n",
        "    4.5 The customer accepts the offer, and a Murabahah sale contract is executed.\n",
        "    4.6 The customer pays the agreed price, either in installments or as a lump sum.\n",
        "\n",
        "    5. Modern Applications and Issues\n",
        "    5.1 Appointment of the customer as agent: The IFI may appoint the customer as its agent to purchase the asset on its behalf, provided that the customer acts in a genuine agency capacity.\n",
        "    5.2 Third-party guarantees: Independent third parties may provide guarantees to protect against negligence or misconduct.\n",
        "    5.3 Late payment: The IFI may require the customer to donate to charity in case of late payment, but may not benefit from these amounts.\n",
        "    5.4 Rebate for early settlement: The IFI may voluntarily give a rebate for early settlement, but this cannot be stipulated in the contract.\n",
        "\n",
        "    6. Shariah Rulings on Specific Murabahah Issues\n",
        "    6.1 It is not permissible to roll over a Murabahah financing by extending the payment period in exchange for an increase in the amount owed.\n",
        "    6.2 Currency exchange (sarf) must be completed before the Murabahah transaction when purchasing assets in a different currency.\n",
        "    6.3 Conventional insurance on Murabahah assets should be avoided in favor of Takaful (Islamic insurance) when available.\n",
        "\n",
        "    7. Documentation Requirements\n",
        "    7.1 Promise document: Detailing the customer's promise to purchase the asset.\n",
        "    7.2 Agency agreement (if applicable): Appointing the customer as agent for purchasing the asset.\n",
        "    7.3 Murabahah sale contract: Documenting the actual sale transaction.\n",
        "    7.4 Security documents: Including collateral, guarantees, or pledges to secure the payment.\n",
        "    \"\"\"\n",
        "    return StandardDocument(name=\"Murabahah Standard X\", content=sample_content)\n",
        "\n",
        "\n",
        "def visualize_results(results: Dict[str, Any]) -> None:\n",
        "    if not results:\n",
        "        logger.error(\"No results provided for visualization\")\n",
        "        return\n",
        "    standard_name = results.get(\"standard_name\", \"Unknown Standard\")\n",
        "    try:\n",
        "        # Display Executive Summary if available\n",
        "        report_data = results.get(\"report\", {})\n",
        "        executive_summary = report_data.get(\"executive_summary\", f\"Section 'Executive Summary' not found for {standard_name}.\")\n",
        "        if \"IPython\" in globals() and \"display\" in globals() and \"Markdown\" in globals(): # Check if in IPython\n",
        "            display(Markdown(f\"## Executive Summary for {standard_name}\"))\n",
        "            display(Markdown(executive_summary))\n",
        "        else:\n",
        "            print(f\"\\n## Executive Summary for {standard_name}\\n{executive_summary}\\n\")\n",
        "\n",
        "\n",
        "        # Shariah Compliance Chart\n",
        "        shariah_assessment_data = results.get(\"shariah_assessment\", {})\n",
        "        rulings_data = shariah_assessment_data.get(\"overall_ruling\") # This should now be a dict\n",
        "\n",
        "        if rulings_data and isinstance(rulings_data, dict) and any(rulings_data.values()): # Check if dict and has some values\n",
        "            df_data = {'Category': [], 'Ruling': []}\n",
        "            for cat, rul in rulings_data.items():\n",
        "                if rul and rul != f\"Section '{cat}' not found.\" and rul != \"Not specifically assessed\": # Filter out error messages and default \"Not specifically assessed\"\n",
        "                    df_data['Category'].append(cat.replace('_', ' ').title()) # Prettify category names\n",
        "                    df_data['Ruling'].append(rul)\n",
        "\n",
        "            if not df_data['Category']: # If all were filtered out\n",
        "                logger.info(\"No valid Shariah ruling data (Approved, Conditionally Approved, etc.) to visualize after filtering.\")\n",
        "                # Optionally, still show a chart indicating everything was \"Not specifically assessed\" if that's the case\n",
        "                # For now, just return if no actual rulings.\n",
        "                if all(val == \"Not specifically assessed\" for val in rulings_data.values()):\n",
        "                    print(f\"All Shariah ruling categories for {standard_name} were 'Not specifically assessed'. No chart will be generated for specific rulings.\")\n",
        "                return\n",
        "\n",
        "\n",
        "            df = pd.DataFrame(df_data)\n",
        "\n",
        "            ruling_map = {\n",
        "                'Approved': 3, 'Conditionally Approved': 2,\n",
        "                'Requires Modification': 1, 'Rejected': 0,\n",
        "                'Not specifically assessed': 1.5, # Default from agent if parsing fails\n",
        "                'N/A': 0.5 # Generic fallback\n",
        "            }\n",
        "            df['Score'] = df['Ruling'].map(lambda x: ruling_map.get(x, 1.5)) # Map to scores, default to 1.5 if ruling not in map (like 'Not specifically assessed')\n",
        "\n",
        "            plt.figure(figsize=(12, 8)) # Adjusted size\n",
        "            bar_colors = []\n",
        "            for score in df['Score']:\n",
        "                if score >= 2.5: bar_colors.append('green') # Approved\n",
        "                elif score >= 1.8: bar_colors.append('yellowgreen') # Conditionally Approved\n",
        "                elif score >= 1.2: bar_colors.append('orange') # Not specifically assessed / Requires Mod\n",
        "                elif score >= 0.8: bar_colors.append('coral') # Requires Mod / N/A\n",
        "                else: bar_colors.append('red') # Rejected / Low N/A\n",
        "\n",
        "            bars = plt.bar(df['Category'], df['Score'], color=bar_colors)\n",
        "\n",
        "            plt.title(f'Shariah Compliance Assessment for {standard_name}', fontsize=14)\n",
        "            plt.xlabel('Enhancement Category', fontsize=12)\n",
        "            plt.ylabel('Compliance Level (Numeric Score)', fontsize=12)\n",
        "            plt.xticks(rotation=30, ha='right', fontsize=10) # Adjusted rotation and size\n",
        "\n",
        "            # Create y-ticks based on unique scores present and their corresponding labels\n",
        "            # Make sure the main map entries are preferred for labels\n",
        "            unique_scores_in_data = sorted(list(set(df['Score'])))\n",
        "\n",
        "            yticks_locs = []\n",
        "            yticks_labels = []\n",
        "\n",
        "            # Add ticks for common ruling levels even if not in data, for scale context\n",
        "            standard_ticks = {value: key for key, value in ruling_map.items()}\n",
        "            for score_val in sorted(standard_ticks.keys()):\n",
        "                 if score_val not in yticks_locs:\n",
        "                    yticks_locs.append(score_val)\n",
        "                    yticks_labels.append(standard_ticks[score_val])\n",
        "\n",
        "            # Ensure all actual data scores have a tick, if not covered by standard_ticks\n",
        "            for score_val in unique_scores_in_data:\n",
        "                if score_val not in yticks_locs:\n",
        "                    yticks_locs.append(score_val)\n",
        "                    # Try to find a label, otherwise use score\n",
        "                    found_label = False\n",
        "                    for lab, sc in ruling_map.items():\n",
        "                        if sc == score_val:\n",
        "                            yticks_labels.append(lab)\n",
        "                            found_label = True\n",
        "                            break\n",
        "                    if not found_label:\n",
        "                        yticks_labels.append(f\"Score {score_val:.1f}\")\n",
        "\n",
        "\n",
        "            # Sort ticks by location before applying\n",
        "            sorted_ticks_combined = sorted(list(set(zip(yticks_locs, yticks_labels)))) # Use set to remove duplicates if any\n",
        "            final_yticks_locs = [loc for loc, lab in sorted_ticks_combined]\n",
        "            final_yticks_labels = [lab for loc, lab in sorted_ticks_combined]\n",
        "\n",
        "\n",
        "            plt.yticks(final_yticks_locs, final_yticks_labels, fontsize=10)\n",
        "            plt.ylim(bottom=min(0, min(final_yticks_locs)-0.2 if final_yticks_locs else 0) , top=max(3.2, max(final_yticks_locs)+0.2 if final_yticks_locs else 3.2))\n",
        "\n",
        "\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.tight_layout() # Adjust layout\n",
        "\n",
        "            for bar, ruling_text in zip(bars, df['Ruling']):\n",
        "                plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.05, # Position above bar\n",
        "                         ruling_text, ha='center', va='bottom', fontsize=9, color='black')\n",
        "\n",
        "            plt.show()\n",
        "        else:\n",
        "            logger.info(\"No valid 'overall_ruling' data available for Shariah compliance visualization, or all categories were 'Not specifically assessed'.\")\n",
        "\n",
        "    except ImportError: # Matplotlib or Pandas might not be installed\n",
        "        logger.warning(\"Matplotlib or Pandas not installed. Skipping visualization that requires them.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred during visualization: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "# Demo execution (User's new version)\n",
        "def run_demo():\n",
        "    try:\n",
        "        logger.info(\"Starting AAOIFI Standards Multi-Agent System demonstration (New Orchestrator & Agents)\")\n",
        "        system = AAOIFIStandardsSystem()\n",
        "        standard = load_sample_standard()\n",
        "        logger.info(f\"Loaded sample standard: {standard.name}\")\n",
        "        results = system.process_standard(standard)\n",
        "\n",
        "        sample_feedback = \"\"\"\n",
        "        Feedback from Islamic Financial Institutions:\n",
        "        1. The technological integration suggestions for blockchain-based Murabahah tracking are innovative but may be too complex for immediate implementation.\n",
        "        2. The clarification on agency arrangements is helpful, but requires more specific guidelines for documentation.\n",
        "\n",
        "        Feedback from Shariah Scholars:\n",
        "        1. The proposed modifications on risk transfer mechanisms need further elaboration to ensure full Shariah compliance.\n",
        "\n",
        "        Feedback from Regulators:\n",
        "        1. The proposed standardized documentation would facilitate regulatory oversight.\n",
        "        \"\"\"\n",
        "        feedback_results = system.incorporate_feedback(sample_feedback, results.get(\"enhancement\", {}))\n",
        "\n",
        "        visualize_results(results)\n",
        "        logger.info(\"Demonstration completed successfully (New Orchestrator & Agents)\")\n",
        "        return results, feedback_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in demonstration (New Orchestrator & Agents): {str(e)}\", exc_info=True)\n",
        "        return None, None # Return None on failure to avoid NameError later\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logger.info(\"AAOIFI Standards Multi-Agent System - Main Execution Start\")\n",
        "\n",
        "    if not Config.OPENAI_API_KEY:\n",
        "        logger.error(\"ERROR: OPENAI_API_KEY not set. Please set this environment variable or in the script.\")\n",
        "        exit(1)\n",
        "\n",
        "    pdf_folder = Config.PDF_FOLDER\n",
        "    db_dir = Config.DB_DIRECTORY # Use the potentially updated one if process_pdfs_safe ran\n",
        "\n",
        "    # Simple check: if db_dir doesn't exist or is empty, prompt to process PDFs\n",
        "    # A more robust check would involve trying to connect and list standards.\n",
        "    should_process_pdfs = False\n",
        "    if not os.path.exists(db_dir) or not os.listdir(db_dir): # Check if directory is empty\n",
        "        logger.warning(f\"Vector DB directory '{db_dir}' appears to be missing or empty.\")\n",
        "        should_process_pdfs = True\n",
        "\n",
        "    if should_process_pdfs:\n",
        "        if os.path.exists(pdf_folder) and any(f.lower().endswith('.pdf') for f in os.listdir(pdf_folder)):\n",
        "            user_choice = input(f\"Vector DB at '{db_dir}' might be empty/missing. Process PDFs from '{pdf_folder}' to create/recreate it? (yes/no): \").strip().lower()\n",
        "            if user_choice == 'yes':\n",
        "                logger.info(\"Attempting to process PDFs using safe recreate method...\")\n",
        "                process_pdfs_safe() # This updates Config.DB_DIRECTORY globally\n",
        "                # Re-assign db_dir in case it changed\n",
        "                db_dir = Config.DB_DIRECTORY\n",
        "                logger.info(f\"PDF processing complete. DB is now at '{db_dir}'.\")\n",
        "            else:\n",
        "                logger.info(\"PDF processing skipped by user. Demo will proceed with current DB state.\")\n",
        "        else:\n",
        "            logger.error(f\"PDF folder '{pdf_folder}' is missing or empty. Cannot process PDFs. Demo might fail if DB is not populated.\")\n",
        "\n",
        "\n",
        "    logger.info(\"Running demonstration with New Orchestrator & Agents...\")\n",
        "    run_demo_results, run_demo_feedback_results = None, None\n",
        "    try:\n",
        "        run_demo_results, run_demo_feedback_results = run_demo()\n",
        "        if run_demo_results:\n",
        "            logger.info(f\"Demonstration results obtained for standard: {run_demo_results.get('standard_name')}\")\n",
        "        if run_demo_feedback_results:\n",
        "            logger.info(f\"Feedback analysis obtained for standard: {run_demo_feedback_results.get('standard_name')}\")\n",
        "    except Exception as e:\n",
        "        logger.critical(f\"Demonstration run failed critically: {e}\", exc_info=True)\n",
        "\n",
        "    logger.info(f\"Main execution finished. Results (if any) saved to {Config.OUTPUT_DIR} directory.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gpu_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}