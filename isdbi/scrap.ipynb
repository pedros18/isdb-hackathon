{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b613c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openaiscrap.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a1707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fb028f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key here\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(\"pdf_eng\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"aaoifi_vector_db\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f670164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "class Config:\n",
    "    # Vector Database Configuration\n",
    "    DB_DIRECTORY = \"aaoifi_vector_db\"\n",
    "    COLLECTION_NAME = \"aaoifi_standards\"\n",
    "    \n",
    "    # PDF Processing Configuration\n",
    "    PDF_FOLDER = \"pdf_eng\"\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 200\n",
    "    \n",
    "    # Models Configuration\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI embedding model\n",
    "    GPT4_MODEL = \"gpt-4\"\n",
    "    GPT35_MODEL = \"gpt-3.5-turbo\"\n",
    "    \n",
    "    # Output Configuration\n",
    "    OUTPUT_DIR = \"results\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eace51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess the extracted text.\"\"\"\n",
    "    # Replace multiple whitespaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove other unwanted characters or formatting\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
    "    return text.strip()\n",
    "\n",
    "def split_text_into_chunks(text, standard_name):\n",
    "    \"\"\"Split text into manageable chunks for embedding.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config.CHUNK_SIZE,\n",
    "        chunk_overlap=config.CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Add metadata to each chunk\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            \"content\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"source\": standard_name,\n",
    "                \"chunk_id\": i\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def create_vector_database(documents):\n",
    "    \"\"\"Create a vector database from document chunks.\"\"\"\n",
    "    # Initialize embeddings provider\n",
    "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "    \n",
    "    # Create Chroma client\n",
    "    client = chromadb.PersistentClient(path=config.DB_DIRECTORY)\n",
    "    \n",
    "    # Create or get collection\n",
    "    collection = client.get_or_create_collection(name=config.COLLECTION_NAME)\n",
    "    \n",
    "    # Process documents in batches to avoid API limits\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        \n",
    "        # Extract content and metadata\n",
    "        texts = [doc[\"content\"] for doc in batch]\n",
    "        ids = [f\"doc_{i+j}\" for j in range(len(batch))]\n",
    "        metadatas = [doc[\"metadata\"] for doc in batch]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embeds = embeddings.embed_documents(texts)\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            embeddings=embeds,\n",
    "            documents=texts,\n",
    "            ids=ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "    \n",
    "    return client\n",
    "\n",
    "def process_pdfs():\n",
    "    \"\"\"Main function to process PDFs and create vector database.\"\"\"\n",
    "    PDF_FOLDER = config.PDF_FOLDER\n",
    "    \n",
    "    if not os.path.exists(PDF_FOLDER):\n",
    "        print(f\"Error: The folder '{PDF_FOLDER}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    all_documents = []\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
    "        standard_name = os.path.splitext(pdf_file)[0]\n",
    "        \n",
    "        print(f\"Processing {pdf_file}...\")\n",
    "        \n",
    "        # Extract text from PDF\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        # Clean the text\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
    "        all_documents.extend(chunks)\n",
    "        \n",
    "        print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
    "    \n",
    "    print(f\"Total chunks extracted: {len(all_documents)}\")\n",
    "    \n",
    "    # Create vector database\n",
    "    print(\"Creating vector database...\")\n",
    "    client = create_vector_database(all_documents)\n",
    "    \n",
    "    print(f\"Vector database created successfully in '{config.DB_DIRECTORY}'\")\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82a71b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardDocument:\n",
    "    \"\"\"Class to represent an AAOIFI standard document.\"\"\"\n",
    "    def __init__(self, name: str, content: str):\n",
    "        self.name = name\n",
    "        self.content = content\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents in the system.\"\"\"\n",
    "    def __init__(self, name: str, description: str, model_name: str = Config.GPT4_MODEL):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.model_name = model_name\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0.2)\n",
    "    \n",
    "    def execute(self, input_data: Any) -> Any:\n",
    "        \"\"\"Execute the agent's task.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "class ReviewAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for reviewing standards and extracting key elements.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"ReviewAgent\",\n",
    "            description=\"Analyzes AAOIFI standards to extract key elements, principles, and requirements.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an expert in Islamic finance and AAOIFI standards. Your task is to carefully review \n",
    "        the provided standard document and extract the following key elements:\n",
    "        \n",
    "        1. Core principles and objectives of the standard\n",
    "        2. Key definitions and terminology\n",
    "        3. Main requirements and procedures\n",
    "        4. Compliance criteria and guidelines\n",
    "        5. Practical implementation considerations\n",
    "        \n",
    "        Organize your analysis in a structured format with these categories. Be thorough but concise \n",
    "        in your extraction of the essential components.\n",
    "        \"\"\"\n",
    "    \n",
    "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a standard document and extract its key elements.\n",
    "        \n",
    "        Args:\n",
    "            standard: The standard document to analyze.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing the extracted key elements.\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"Standard Name: {standard.name}\\n\\nContent:\\n{standard.content}\")\n",
    "        ])\n",
    "        \n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        result = chain.run({})\n",
    "        \n",
    "        # Try to parse the result into structured data\n",
    "        try:\n",
    "            # If the result is in YAML or other format, convert it to dict\n",
    "            # Here we're assuming the model returns well-structured content that can be parsed\n",
    "            parsed_result = {\n",
    "                \"standard_name\": standard.name,\n",
    "                \"review_result\": result,\n",
    "                \"core_principles\": self._extract_section(result, \"Core principles and objectives\"),\n",
    "                \"key_definitions\": self._extract_section(result, \"Key definitions and terminology\"),\n",
    "                \"main_requirements\": self._extract_section(result, \"Main requirements and procedures\"),\n",
    "                \"compliance_criteria\": self._extract_section(result, \"Compliance criteria\"),\n",
    "                \"implementation\": self._extract_section(result, \"implementation considerations\")\n",
    "            }\n",
    "            return parsed_result\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing review result: {e}\")\n",
    "            return {\n",
    "                \"standard_name\": standard.name,\n",
    "                \"review_result\": result,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _extract_section(self, text: str, section_name: str) -> str:\n",
    "        \"\"\"Helper method to extract specific sections from the review result.\"\"\"\n",
    "        # Simple pattern matching for section extraction\n",
    "        # In a real implementation, you would use a more robust approach\n",
    "        lower_text = text.lower()\n",
    "        lower_section = section_name.lower()\n",
    "        \n",
    "        if lower_section in lower_text:\n",
    "            start_idx = lower_text.find(lower_section)\n",
    "            next_section_idx = float('inf')\n",
    "            \n",
    "            for section in [\"core principles\", \"key definitions\", \"main requirements\", \n",
    "                           \"compliance criteria\", \"implementation considerations\"]:\n",
    "                if section != lower_section and section in lower_text:\n",
    "                    idx = lower_text.find(section, start_idx + len(lower_section))\n",
    "                    if idx > start_idx and idx < next_section_idx:\n",
    "                        next_section_idx = idx\n",
    "            \n",
    "            if next_section_idx < float('inf'):\n",
    "                return text[start_idx:next_section_idx].strip()\n",
    "            else:\n",
    "                return text[start_idx:].strip()\n",
    "        \n",
    "        return \"\"\n",
    "\n",
    "class EnhancementAgent(BaseAgent):\n",
    "    \"\"\"Agent tasked with proposing modifications or enhancements to standards.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"EnhancementAgent\",\n",
    "            description=\"Proposes AI-driven modifications and enhancements to AAOIFI standards.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an AI expert specializing in Islamic finance and AAOIFI standards enhancement. \n",
    "        Your task is to propose thoughtful modifications and enhancements to the standard based \n",
    "        on the review provided.\n",
    "        \n",
    "        Consider the following aspects in your proposals:\n",
    "        \n",
    "        1. Clarity improvements: Suggest clearer language or better organization where appropriate\n",
    "        2. Modern context adaptations: Propose updates to address contemporary financial practices\n",
    "        3. Technological integration: Recommend ways to incorporate digital technologies and fintech\n",
    "        4. Cross-reference enhancements: Suggest improved links to related standards or principles\n",
    "        5. Practical implementation: Provide more actionable guidance for practitioners\n",
    "        \n",
    "        For each suggestion, provide:\n",
    "        - The specific section or clause being enhanced\n",
    "        - The current text or concept (if applicable)\n",
    "        - Your proposed modification or addition\n",
    "        - A brief justification explaining the benefit of your enhancement\n",
    "        \n",
    "        Ensure all suggestions maintain strict compliance with Shariah principles.\n",
    "        \"\"\"\n",
    "    \n",
    "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Propose enhancements to a standard based on the review.\n",
    "        \n",
    "        Args:\n",
    "            review_result: The result from the ReviewAgent.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing proposed enhancements.\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\n\\nReview Result:\\n{review_result['review_result']}\")\n",
    "        ])\n",
    "        \n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        result = chain.run({})\n",
    "        \n",
    "        return {\n",
    "            \"standard_name\": review_result[\"standard_name\"],\n",
    "            \"enhancement_proposals\": result\n",
    "        }\n",
    "\n",
    "class ValidationAgent(BaseAgent):\n",
    "    \"\"\"Agent for validating and approving proposed changes.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"ValidationAgent\",\n",
    "            description=\"Validates proposed changes based on Shariah compliance and practicality.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a senior Shariah scholar and AAOIFI standards expert. Your role is to validate \n",
    "        proposed enhancements to ensure they maintain compliance with Islamic principles and \n",
    "        practical applicability.\n",
    "        \n",
    "        For each proposed enhancement, evaluate:\n",
    "        \n",
    "        1. Shariah Compliance: Does the proposal align with Islamic principles and AAOIFI's mission?\n",
    "        2. Technical Accuracy: Is the proposed language precise and technically sound?\n",
    "        3. Practical Applicability: Can the enhancement be practically implemented by Islamic financial institutions?\n",
    "        4. Consistency: Does it maintain consistency with other standards and established practices?\n",
    "        5. Value Addition: Does it meaningfully improve the standard?\n",
    "        \n",
    "        For each proposal, provide:\n",
    "        - Your assessment (Approved/Rejected/Needs Modification)\n",
    "        - Justification for your decision\n",
    "        - Suggested refinements if \"Needs Modification\"\n",
    "        \n",
    "        Be thorough in your analysis and maintain the highest standards of Islamic finance integrity.\n",
    "        \"\"\"\n",
    "    \n",
    "    def execute(self, enhancement_result: Dict[str, Any], original_review: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate proposed enhancements based on Shariah compliance and practicality.\n",
    "        \n",
    "        Args:\n",
    "            enhancement_result: The result from the EnhancementAgent.\n",
    "            original_review: The original review from the ReviewAgent.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing validation results.\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {enhancement_result['standard_name']}\n",
    "            \n",
    "            Original Review:\n",
    "            {original_review['review_result']}\n",
    "            \n",
    "            Proposed Enhancements:\n",
    "            {enhancement_result['enhancement_proposals']}\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        result = chain.run({})\n",
    "        \n",
    "        return {\n",
    "            \"standard_name\": enhancement_result[\"standard_name\"],\n",
    "            \"validation_result\": result\n",
    "        }\n",
    "\n",
    "class FinalReportAgent(BaseAgent):\n",
    "    \"\"\"Agent for generating a comprehensive final report.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinalReportAgent\",\n",
    "            description=\"Compiles all findings and recommendations into a comprehensive report.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a professional report writer specializing in Islamic finance standards. Your task is to \n",
    "        compile all the findings, enhancements, and validations into a comprehensive, well-structured report.\n",
    "        \n",
    "        Your report should include:\n",
    "        \n",
    "        1. Executive Summary: Brief overview of the standard reviewed and key findings\n",
    "        2. Standard Overview: Summary of the original standard's purpose and core components\n",
    "        3. Key Findings from Review: Major elements and considerations identified\n",
    "        4. Proposed Enhancements: Clear presentation of all proposed modifications\n",
    "        5. Validation Results: Summary of the validation process and outcomes\n",
    "        6. Implementation Recommendations: Practical next steps for adopting approved changes\n",
    "        7. Conclusion: Final thoughts on the impact of the proposed enhancements\n",
    "        \n",
    "        Write in a professional, clear style appropriate for AAOIFI stakeholders and Islamic finance professionals.\n",
    "        \"\"\"\n",
    "    \n",
    "    def execute(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive final report.\n",
    "        \n",
    "        Args:\n",
    "            all_results: Combined results from all previous agents.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing the final report.\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {all_results['standard_name']}\n",
    "            \n",
    "            Review Results:\n",
    "            {all_results['review_result']}\n",
    "            \n",
    "            Enhancement Proposals:\n",
    "            {all_results['enhancement_proposals']}\n",
    "            \n",
    "            Validation Results:\n",
    "            {all_results['validation_result']}\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        result = chain.run({})\n",
    "        \n",
    "        return {\n",
    "            \"standard_name\": all_results[\"standard_name\"],\n",
    "            \"final_report\": result\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "160d57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDBManager:\n",
    "    \"\"\"Manager for interacting with the vector database.\"\"\"\n",
    "    \n",
    "    def __init__(self, db_directory: str = config.DB_DIRECTORY, collection_name: str = config.COLLECTION_NAME):\n",
    "        self.db_directory = db_directory\n",
    "        self.collection_name = collection_name\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        \n",
    "        # Initialize client\n",
    "        self.client = chromadb.PersistentClient(path=db_directory)\n",
    "        \n",
    "        # Get collection\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(name=collection_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing collection: {e}\")\n",
    "            print(\"Please ensure the vector database has been created first.\")\n",
    "            self.collection = None\n",
    "    \n",
    "    def get_standard_content(self, standard_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the content for a specific standard.\n",
    "        \n",
    "        Args:\n",
    "            standard_name: The name of the standard to retrieve.\n",
    "            \n",
    "        Returns:\n",
    "            The combined content of the standard.\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            return \"Error: Vector database not properly initialized.\"\n",
    "        \n",
    "        # Query for all chunks belonging to this standard\n",
    "        results = self.collection.query(\n",
    "            query_texts=[\"\"],\n",
    "            where={\"source\": standard_name},\n",
    "            n_results=100  # Adjust based on expected number of chunks\n",
    "        )\n",
    "        \n",
    "        # Combine chunks in order\n",
    "        if results and 'documents' in results and results['documents']:\n",
    "            return \"\\n\\n\".join([doc for doc in results['documents'][0] if doc])\n",
    "        \n",
    "        return f\"No content found for standard: {standard_name}\"\n",
    "    \n",
    "    def list_available_standards(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all available standards in the database.\n",
    "        \n",
    "        Returns:\n",
    "            A list of standard names.\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            return [\"Error: Vector database not properly initialized.\"]\n",
    "        \n",
    "        # This is a simplified approach - in reality you'd need a more robust method\n",
    "        # to extract unique standard names from metadata\n",
    "        try:\n",
    "            results = self.collection.get()\n",
    "            if results and 'metadatas' in results and results['metadatas']:\n",
    "                standards = set()\n",
    "                for metadata in results['metadatas']:\n",
    "                    if 'source' in metadata:\n",
    "                        standards.add(metadata['source'])\n",
    "                return list(standards)\n",
    "        except Exception as e:\n",
    "            return [f\"Error listing standards: {e}\"]\n",
    "        \n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af0eb79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAOIFIStandardsEnhancementSystem:\n",
    "    \"\"\"Main system coordinating the multi-agent process.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize vector database manager\n",
    "        self.db_manager = VectorDBManager()\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.review_agent = ReviewAgent()\n",
    "        self.enhancement_agent = EnhancementAgent()\n",
    "        self.validation_agent = ValidationAgent()\n",
    "        self.report_agent = FinalReportAgent()\n",
    "        \n",
    "        # Track processing results\n",
    "        self.results = {}\n",
    "    \n",
    "    def list_available_standards(self) -> List[str]:\n",
    "        \"\"\"List all available standards in the system.\"\"\"\n",
    "        return self.db_manager.list_available_standards()\n",
    "    \n",
    "    def process_standard(self, standard_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a single standard through the complete pipeline.\n",
    "        \n",
    "        Args:\n",
    "            standard_name: The name of the standard to process.\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing the final results.\n",
    "        \"\"\"\n",
    "        print(f\"Processing standard: {standard_name}\")\n",
    "        \n",
    "        # Step 1: Get standard content from the vector database\n",
    "        print(\"Retrieving standard content...\")\n",
    "        content = self.db_manager.get_standard_content(standard_name)\n",
    "        standard = StandardDocument(name=standard_name, content=content)\n",
    "        \n",
    "        # Step 2: Review and extract key elements\n",
    "        print(\"Reviewing standard and extracting key elements...\")\n",
    "        review_result = self.review_agent.execute(standard)\n",
    "        self.results[\"review_result\"] = review_result\n",
    "        \n",
    "        # Step 3: Propose enhancements\n",
    "        print(\"Proposing enhancements...\")\n",
    "        enhancement_result = self.enhancement_agent.execute(review_result)\n",
    "        self.results[\"enhancement_proposals\"] = enhancement_result[\"enhancement_proposals\"]\n",
    "        \n",
    "        # Step 4: Validate proposed changes\n",
    "        print(\"Validating proposed changes...\")\n",
    "        validation_result = self.validation_agent.execute(enhancement_result, review_result)\n",
    "        self.results[\"validation_result\"] = validation_result[\"validation_result\"]\n",
    "        \n",
    "        # Step 5: Generate final report\n",
    "        print(\"Generating final report...\")\n",
    "        all_results = {\n",
    "            \"standard_name\": standard_name,\n",
    "            \"review_result\": review_result[\"review_result\"],\n",
    "            \"enhancement_proposals\": enhancement_result[\"enhancement_proposals\"],\n",
    "            \"validation_result\": validation_result[\"validation_result\"]\n",
    "        }\n",
    "        \n",
    "        final_report = self.report_agent.execute(all_results)\n",
    "        self.results[\"final_report\"] = final_report[\"final_report\"]\n",
    "        self.results[\"standard_name\"] = standard_name\n",
    "        \n",
    "        print(f\"Processing completed for standard: {standard_name}\")\n",
    "        return self.results\n",
    "    \n",
    "    def save_results(self, output_dir: str = config.OUTPUT_DIR):\n",
    "        \"\"\"Save all results to output files.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"No results to save.\")\n",
    "            return\n",
    "        \n",
    "        standard_name = self.results.get(\"standard_name\", \"unknown_standard\")\n",
    "        filename = os.path.join(output_dir, f\"{standard_name}_results.json\")\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        \n",
    "        print(f\"Results saved to {filename}\")\n",
    "        \n",
    "        # Also save the final report separately\n",
    "        if \"final_report\" in self.results:\n",
    "            report_filename = os.path.join(output_dir, f\"{standard_name}_final_report.md\")\n",
    "            with open(report_filename, 'w') as f:\n",
    "                f.write(self.results[\"final_report\"])\n",
    "            \n",
    "            print(f\"Final report saved to {report_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70b7ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 PDF files in the pdf_eng directory:\n",
      "- AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9.pdf\n",
      "- AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR.pdf\n",
      "- AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean.pdf\n",
      "- AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR.pdf\n",
      "- AAOIFI-SB-Conf.-17-Final-Recommendations.pdf\n",
      "- AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2.pdf\n",
      "- Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions.pdf\n",
      "- FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions.pdf\n",
      "- FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022.pdf\n",
      "- FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1.pdf\n",
      "- FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1.pdf\n",
      "- FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1.pdf\n",
      "- FAS-32-Ijarah-Formatted-2021-clean-April-2023-1.pdf\n",
      "- FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023.pdf\n",
      "- FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023.pdf\n",
      "- FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022.pdf\n",
      "- FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean.pdf\n",
      "- FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean.pdf\n",
      "- FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean.pdf\n",
      "- FAS-39-Financial-reporting-for-Zakah-clean-june-2022.pdf\n",
      "- FAS-40-Financial-Reporting-for-Islamic-Windows.pdf\n",
      "- FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean.pdf\n",
      "- FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean.pdf\n",
      "- FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean.pdf\n",
      "- FAS-44_-Determining-Control-of-Assets-and-Business-Final.pdf\n",
      "- FAS-45_Quasi-equity-Including-Investment-Accounts-Final.pdf\n",
      "- FAS-46_Off-Balance-Sheet-Assets-Under-Management-Final.pdf\n",
      "- FAS-47_Transfer-of-Assets-between-Investment-Pools-Final.pdf\n",
      "- Fiancial-Accounting-Standard-No.-13-Disclosure-of-Bases-for-Determining-and-Allocating-Surplus-or-Defificit-in-Islamic-Insurance-Companies.pdf\n",
      "- Financial-Accounting-Standard-10-Istisnaa-and-Parallel-Istisnaa.pdf\n",
      "- Financial-Accounting-Standard-12-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Insurance-Companies.pdf\n",
      "- Financial-Accounting-Standard-14-Investment-Funds.pdf\n",
      "- Financial-Accounting-Standard-15-Provisions-and-Reserves-in-Islamic-Insurance-Companies.pdf\n",
      "- Financial-Accounting-Standard-16-Foreign-Currency-Transactions-and-Foreign-Operations.pdf\n",
      "- Financial-Accounting-Standard-18-Islamic-Financial-Services-Offffered-by-Conventional-Financial-Institutions.pdf\n",
      "- Financial-Accounting-Standard-19-Contributions-in-Islamic-Insurance-Companies.pdf\n",
      "- Financial-Accounting-Standard-21-Disclosure-on-Transfer-of-Assets.pdf\n",
      "- Financial-Accounting-Standard-22-Segment-Reporting.pdf\n",
      "- Financial-Accounting-Standard-23-Consolidation.pdf\n",
      "- Financial-Accounting-Standard-24-Investments-in-Associates.pdf\n",
      "- Financial-Accounting-Standard-25-Investment-in-Sukuk-Shares-and-Similar-Instruments.pdf\n",
      "- Financial-Accounting-Standard-26-Investment-in-Real-Estate.pdf\n",
      "- Financial-Accounting-Standard-27-Investment-accounts.pdf\n",
      "- Financial-Accounting-Standard-3-Mudaraba-Financing.pdf\n",
      "- Financial-Accounting-Standard-4-Musharaka-Financing.pdf\n",
      "- Financial-Accounting-Standard-7-Salam-and-Parallel-Salam.pdf\n",
      "- Financial-Accounting-Standard-9-Zakah.pdf\n",
      "- Research-papers-1.pdf\n",
      "- Research-papers-new.pdf\n"
     ]
    }
   ],
   "source": [
    "# First, check if we have PDF files in the pdf_eng directory\n",
    "pdf_files = [f for f in os.listdir(config.PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
    "print(f\"Found {len(pdf_files)} PDF files in the {config.PDF_FOLDER} directory:\")\n",
    "for pdf in pdf_files:\n",
    "    print(f\"- {pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ff58971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process the PDFs to create the vector database\n",
    "# # Only run this cell when you want to process PDFs\n",
    "# process_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96d16824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_19276\\2179003765.py:7: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  self.embeddings = OpenAIEmbeddings()\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_19276\\3989169443.py:13: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  self.llm = ChatOpenAI(model_name=model_name, temperature=0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available standards:\n",
      "1. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
      "2. FAS-44_-Determining-Control-of-Assets-and-Business-Final\n",
      "3. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
      "4. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n",
      "5. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
      "6. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
      "7. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
      "8. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
      "9. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
      "10. FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean\n",
      "11. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
      "12. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
      "13. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
      "14. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
      "15. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
      "16. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
      "17. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
      "18. Murabahah Standard\n",
      "19. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
      "20. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
      "21. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
      "22. Murabahah Standard X\n",
      "23. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
      "24. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
      "25. FAS-45_Quasi-equity-Including-Investment-Accounts-Final\n",
      "26. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
      "27. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
      "28. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n"
     ]
    }
   ],
   "source": [
    "# Initialize the system\n",
    "system = AAOIFIStandardsEnhancementSystem()\n",
    "\n",
    "# List available standards\n",
    "print(\"Available standards:\")\n",
    "standards = system.list_available_standards()\n",
    "for i, standard in enumerate(standards):\n",
    "    print(f\"{i+1}. {standard}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a96aca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Fix the dimension mismatch by clearing the database and recreating it\n",
    "\n",
    "def recreate_vector_database(documents):\n",
    "    \"\"\"Create a new vector database from document chunks after removing any existing data.\"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    import chromadb\n",
    "    \n",
    "    # Initialize embeddings provider\n",
    "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "    \n",
    "    # First, check if the DB directory exists and delete it\n",
    "    if os.path.exists(config.DB_DIRECTORY):\n",
    "        print(f\"Removing existing database directory: {config.DB_DIRECTORY}\")\n",
    "        shutil.rmtree(config.DB_DIRECTORY)\n",
    "    \n",
    "    # Create a new directory\n",
    "    os.makedirs(config.DB_DIRECTORY, exist_ok=True)\n",
    "    \n",
    "    # Create Chroma client with new empty directory\n",
    "    client = chromadb.PersistentClient(path=config.DB_DIRECTORY)\n",
    "    \n",
    "    # Create new collection\n",
    "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
    "    \n",
    "    # Process documents in batches to avoid API limits\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        \n",
    "        # Extract content and metadata\n",
    "        texts = [doc[\"content\"] for doc in batch]\n",
    "        ids = [f\"doc_{i+j}\" for j in range(len(batch))]\n",
    "        metadatas = [doc[\"metadata\"] for doc in batch]\n",
    "        \n",
    "        # Generate embeddings \n",
    "        embeds = embeddings.embed_documents(texts)\n",
    "        \n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            embeddings=embeds,\n",
    "            documents=texts,\n",
    "            ids=ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "    \n",
    "    print(f\"Created new vector database with consistent embedding dimensions in '{config.DB_DIRECTORY}'\")\n",
    "    return client\n",
    "\n",
    "# Option 2: Alternative approach to fix VectorDBManager\n",
    "class VectorDBManager:\n",
    "    \"\"\"Manager for interacting with the vector database.\"\"\"\n",
    "\n",
    "    def __init__(self, db_directory: str = config.DB_DIRECTORY, collection_name: str = config.COLLECTION_NAME):\n",
    "        self.db_directory = db_directory\n",
    "        self.collection_name = collection_name\n",
    "        self.embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)  # Use consistent model\n",
    "        \n",
    "        # Initialize client\n",
    "        self.client = chromadb.PersistentClient(path=db_directory)\n",
    "        \n",
    "        # Get collection or recreate it if there's an error\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(name=collection_name)\n",
    "            print(f\"Successfully connected to existing collection: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing collection: {e}\")\n",
    "            print(\"Creating a new collection...\")\n",
    "            self.collection = self.client.create_collection(name=collection_name)\n",
    "            \n",
    "    def get_standard_content(self, standard_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the content for a specific standard.\n",
    "        \n",
    "        Args:\n",
    "            standard_name: The name of the standard to retrieve.\n",
    "            \n",
    "        Returns:\n",
    "            The combined content of the standard.\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            return \"Error: Vector database not properly initialized.\"\n",
    "            \n",
    "        # Query for all chunks belonging to this standard\n",
    "        # For a new or recreated collection, we may need to handle empty results\n",
    "        try:\n",
    "            results = self.collection.query(\n",
    "                query_texts=[\"\"],  # Empty query to get all documents\n",
    "                where={\"source\": standard_name},\n",
    "                n_results=100  # Adjust based on expected number of chunks\n",
    "            )\n",
    "            \n",
    "            # Combine chunks in order\n",
    "            if results and 'documents' in results and results['documents'] and results['documents'][0]:\n",
    "                return \"\\n\\n\".join([doc for doc in results['documents'][0] if doc])\n",
    "            else:\n",
    "                return f\"No content found for standard: {standard_name}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error querying for standard {standard_name}: {str(e)}\"\n",
    "\n",
    "    def list_available_standards(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all available standards in the database.\n",
    "        \n",
    "        Returns:\n",
    "            A list of standard names.\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            return [\"Error: Vector database not properly initialized.\"]\n",
    "            \n",
    "        # Handle potentially empty collection\n",
    "        try:\n",
    "            results = self.collection.get()\n",
    "            if results and 'metadatas' in results and results['metadatas']:\n",
    "                standards = set()\n",
    "                for metadata in results['metadatas']:\n",
    "                    if 'source' in metadata:\n",
    "                        standards.add(metadata['source'])\n",
    "                return list(standards)\n",
    "            else:\n",
    "                return [\"No standards found in the database. Please process PDFs first.\"]\n",
    "        except Exception as e:\n",
    "            return [f\"Error listing standards: {str(e)}\"]\n",
    "\n",
    "# Modified process_pdfs function to use the recreate_vector_database function\n",
    "def process_pdfs():\n",
    "    \"\"\"Main function to process PDFs and create vector database.\"\"\"\n",
    "    PDF_FOLDER = config.PDF_FOLDER\n",
    "\n",
    "    if not os.path.exists(PDF_FOLDER):\n",
    "        print(f\"Error: The folder '{PDF_FOLDER}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    all_documents = []\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
    "        standard_name = os.path.splitext(pdf_file)[0]\n",
    "\n",
    "        print(f\"Processing {pdf_file}...\")\n",
    "\n",
    "        # Extract text from PDF\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        # Clean the text\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
    "        all_documents.extend(chunks)\n",
    "\n",
    "        print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
    "\n",
    "    print(f\"Total chunks extracted: {len(all_documents)}\")\n",
    "\n",
    "    # Create vector database, recreating it from scratch to ensure consistent dimensions\n",
    "    print(\"Creating vector database...\")\n",
    "    client = recreate_vector_database(all_documents)\n",
    "\n",
    "    print(f\"Vector database created successfully in '{config.DB_DIRECTORY}'\")\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "044a2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process PDFs to create a fresh vector database\n",
    "# process_pdfs()\n",
    "\n",
    "# # Initialize the system with the updated VectorDBManager\n",
    "# system = AAOIFIStandardsEnhancementSystem()\n",
    "\n",
    "# # Verify the available standards\n",
    "# standards = system.list_available_standards()\n",
    "# print(\"Available standards:\")\n",
    "# for i, standard in enumerate(standards):\n",
    "#     print(f\"{i+1}. {standard}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0782ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified approach to handle locked files\n",
    "\n",
    "def safely_recreate_vector_database(documents):\n",
    "    \"\"\"Create a new vector database from document chunks with proper handling of locked files.\"\"\"\n",
    "    import os\n",
    "    import time\n",
    "    import random\n",
    "    import string\n",
    "    from langchain.embeddings import OpenAIEmbeddings\n",
    "    import chromadb\n",
    "    \n",
    "    # Initialize embeddings provider\n",
    "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "    \n",
    "    # Instead of deleting the directory, create a new directory with a unique name\n",
    "    # This avoids file permission issues with locked files\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
    "    new_db_directory = f\"{config.DB_DIRECTORY}_{timestamp}_{random_str}\"\n",
    "    \n",
    "    print(f\"Creating new database directory: {new_db_directory}\")\n",
    "    os.makedirs(new_db_directory, exist_ok=True)\n",
    "    \n",
    "    # Create Chroma client with the new directory\n",
    "    client = chromadb.PersistentClient(path=new_db_directory)\n",
    "    \n",
    "    # Create new collection\n",
    "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
    "    \n",
    "    # Process documents in batches to avoid API limits\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        \n",
    "        # Extract content and metadata\n",
    "        texts = [doc[\"content\"] for doc in batch]\n",
    "        ids = [f\"doc_{i+j}\" for j in range(len(batch))]\n",
    "        metadatas = [doc[\"metadata\"] for doc in batch]\n",
    "        \n",
    "        try:\n",
    "            # Generate embeddings \n",
    "            embeds = embeddings.embed_documents(texts)\n",
    "            \n",
    "            # Add to collection\n",
    "            collection.add(\n",
    "                embeddings=embeds,\n",
    "                documents=texts,\n",
    "                ids=ids,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            print(f\"Processed batch {i//batch_size + 1} ({len(batch)} documents)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Created new vector database with consistent embedding dimensions in '{new_db_directory}'\")\n",
    "    \n",
    "    # Update the config to point to the new directory\n",
    "    config.DB_DIRECTORY = new_db_directory\n",
    "    print(f\"Updated configuration to use new database directory: {config.DB_DIRECTORY}\")\n",
    "    \n",
    "    return client\n",
    "\n",
    "# Modified process_pdfs function to use the safely_recreate_vector_database function\n",
    "def process_pdfs_safe():\n",
    "    \"\"\"Main function to process PDFs and create vector database safely.\"\"\"\n",
    "    PDF_FOLDER = config.PDF_FOLDER\n",
    "\n",
    "    if not os.path.exists(PDF_FOLDER):\n",
    "        print(f\"Error: The folder '{PDF_FOLDER}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    all_documents = []\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
    "\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
    "        standard_name = os.path.splitext(pdf_file)[0]\n",
    "\n",
    "        print(f\"Processing {pdf_file}...\")\n",
    "\n",
    "        # Extract text from PDF\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        # Clean the text\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "\n",
    "        # Split into chunks\n",
    "        chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
    "        all_documents.extend(chunks)\n",
    "\n",
    "        print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
    "\n",
    "    print(f\"Total chunks extracted: {len(all_documents)}\")\n",
    "\n",
    "    # Create vector database using the safe approach\n",
    "    print(\"Creating vector database safely...\")\n",
    "    client = safely_recreate_vector_database(all_documents)\n",
    "\n",
    "    print(f\"Vector database created successfully in '{config.DB_DIRECTORY}'\")\n",
    "    return client\n",
    "\n",
    "# Modified VectorDBManager class to work with the new directory and handle potential issues\n",
    "class VectorDBManager:\n",
    "    \"\"\"Manager for interacting with the vector database.\"\"\"\n",
    "\n",
    "    def __init__(self, db_directory: str = None, collection_name: str = config.COLLECTION_NAME):\n",
    "        # Use the directory specified in config (which may have been updated)\n",
    "        self.db_directory = db_directory if db_directory else config.DB_DIRECTORY\n",
    "        self.collection_name = collection_name\n",
    "        self.embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)  # Use consistent model\n",
    "        \n",
    "        print(f\"Initializing VectorDBManager with database directory: {self.db_directory}\")\n",
    "        \n",
    "        # Try to initialize client with better error handling\n",
    "        try:\n",
    "            self.client = chromadb.PersistentClient(path=self.db_directory)\n",
    "            print(f\"Successfully connected to database at: {self.db_directory}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error connecting to database: {str(e)}\")\n",
    "            self.client = None\n",
    "            self.collection = None\n",
    "            return\n",
    "            \n",
    "        # Try to get the collection with better error handling\n",
    "        try:\n",
    "            self.collection = self.client.get_collection(name=collection_name)\n",
    "            print(f\"Successfully connected to collection: {collection_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing collection: {str(e)}\")\n",
    "            try:\n",
    "                print(f\"Attempting to create a new collection: {collection_name}\")\n",
    "                self.collection = self.client.create_collection(name=collection_name)\n",
    "                print(f\"New collection created successfully\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error creating collection: {str(e2)}\")\n",
    "                self.collection = None\n",
    "                \n",
    "    def get_standard_content(self, standard_name: str) -> str:\n",
    "        \"\"\"Retrieve the content for a specific standard.\"\"\"\n",
    "        if not self.collection:\n",
    "            return \"Error: Vector database not properly initialized.\"\n",
    "            \n",
    "        try:\n",
    "            results = self.collection.query(\n",
    "                query_texts=[\"\"],  # Empty query to get all documents\n",
    "                where={\"source\": standard_name},\n",
    "                n_results=100  # Adjust based on expected number of chunks\n",
    "            )\n",
    "            \n",
    "            if results and 'documents' in results and results['documents'] and results['documents'][0]:\n",
    "                return \"\\n\\n\".join([doc for doc in results['documents'][0] if doc])\n",
    "            else:\n",
    "                return f\"No content found for standard: {standard_name}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error querying for standard {standard_name}: {str(e)}\"\n",
    "\n",
    "    def list_available_standards(self) -> List[str]:\n",
    "        \"\"\"List all available standards in the database.\"\"\"\n",
    "        if not self.collection:\n",
    "            return [\"Error: Vector database not properly initialized.\"]\n",
    "            \n",
    "        try:\n",
    "            results = self.collection.get()\n",
    "            if results and 'metadatas' in results and results['metadatas']:\n",
    "                standards = set()\n",
    "                for metadata in results['metadatas']:\n",
    "                    if 'source' in metadata:\n",
    "                        standards.add(metadata['source'])\n",
    "                return list(standards)\n",
    "            else:\n",
    "                return [\"No standards found in the database. Please process PDFs first.\"]\n",
    "        except Exception as e:\n",
    "            return [f\"Error listing standards: {str(e)}\"]\n",
    "\n",
    "# Implementation to test the new approach\n",
    "def test_safe_approach():\n",
    "    \"\"\"Test the safe approach to creating and using the vector database.\"\"\"\n",
    "    \n",
    "    # Process PDFs using the safe approach\n",
    "    print(\"Processing PDFs with safe approach...\")\n",
    "    client = process_pdfs_safe()\n",
    "    \n",
    "    # Initialize system with the new database\n",
    "    print(\"\\nInitializing AAOIFIStandardsEnhancementSystem...\")\n",
    "    system = AAOIFIStandardsEnhancementSystem()\n",
    "    \n",
    "    # Check available standards\n",
    "    print(\"\\nListing available standards...\")\n",
    "    standards = system.list_available_standards()\n",
    "    for i, standard in enumerate(standards):\n",
    "        print(f\"{i+1}. {standard}\")\n",
    "    \n",
    "    # If any standards are available, process the first one\n",
    "    if standards and standards[0] != \"No standards found in the database. Please process PDFs first.\":\n",
    "        standard_name = standards[0]\n",
    "        print(f\"\\nProcessing standard: {standard_name}\")\n",
    "        results = system.process_standard(standard_name)\n",
    "        system.save_results()\n",
    "        \n",
    "        # Display the final report\n",
    "        if \"final_report\" in results:\n",
    "            print(\"\\n===== FINAL REPORT =====\\n\")\n",
    "            print(results[\"final_report\"])\n",
    "    else:\n",
    "        print(\"\\nNo standards available to process.\")\n",
    "        \n",
    "    return \"Test completed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e960a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PDFs with safe approach...\n",
      "Found 49 PDF files.\n",
      "Processing AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9.pdf...\n",
      "Extracted 3 chunks from AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9.pdf\n",
      "Processing AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR.pdf...\n",
      "Extracted 3 chunks from AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR.pdf\n",
      "Processing AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean.pdf...\n",
      "Extracted 178 chunks from AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean.pdf\n",
      "Processing AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR.pdf...\n",
      "Extracted 9 chunks from AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR.pdf\n",
      "Processing AAOIFI-SB-Conf.-17-Final-Recommendations.pdf...\n",
      "Extracted 1 chunks from AAOIFI-SB-Conf.-17-Final-Recommendations.pdf\n",
      "Processing AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2.pdf...\n",
      "Extracted 10 chunks from AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2.pdf\n",
      "Processing Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions.pdf...\n",
      "Extracted 240 chunks from Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions.pdf\n",
      "Processing FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions.pdf...\n",
      "Extracted 305 chunks from FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions.pdf\n",
      "Processing FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022.pdf...\n",
      "Extracted 166 chunks from FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022.pdf\n",
      "Processing FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1.pdf...\n",
      "Extracted 58 chunks from FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1.pdf\n",
      "Processing FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1.pdf...\n",
      "Extracted 96 chunks from FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1.pdf\n",
      "Processing FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1.pdf...\n",
      "Extracted 79 chunks from FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1.pdf\n",
      "Processing FAS-32-Ijarah-Formatted-2021-clean-April-2023-1.pdf...\n",
      "Extracted 130 chunks from FAS-32-Ijarah-Formatted-2021-clean-April-2023-1.pdf\n",
      "Processing FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023.pdf...\n",
      "Extracted 68 chunks from FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023.pdf\n",
      "Processing FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023.pdf...\n",
      "Extracted 41 chunks from FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023.pdf\n",
      "Processing FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022.pdf...\n",
      "Extracted 55 chunks from FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022.pdf\n",
      "Processing FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean.pdf...\n",
      "Extracted 45 chunks from FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean.pdf\n",
      "Processing FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean.pdf...\n",
      "Extracted 98 chunks from FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean.pdf\n",
      "Processing FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean.pdf...\n",
      "Extracted 51 chunks from FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean.pdf\n",
      "Processing FAS-39-Financial-reporting-for-Zakah-clean-june-2022.pdf...\n",
      "Extracted 62 chunks from FAS-39-Financial-reporting-for-Zakah-clean-june-2022.pdf\n",
      "Processing FAS-40-Financial-Reporting-for-Islamic-Windows.pdf...\n",
      "Extracted 41 chunks from FAS-40-Financial-Reporting-for-Islamic-Windows.pdf\n",
      "Processing FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean.pdf...\n",
      "Extracted 33 chunks from FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean.pdf\n",
      "Processing FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean.pdf...\n",
      "Extracted 87 chunks from FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean.pdf\n",
      "Processing FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean.pdf...\n",
      "Extracted 154 chunks from FAS-43-Accounting-for-Takaful-Recognition-and-Measurement_Final_clean.pdf\n",
      "Processing FAS-44_-Determining-Control-of-Assets-and-Business-Final.pdf...\n",
      "Extracted 44 chunks from FAS-44_-Determining-Control-of-Assets-and-Business-Final.pdf\n",
      "Processing FAS-45_Quasi-equity-Including-Investment-Accounts-Final.pdf...\n",
      "Extracted 66 chunks from FAS-45_Quasi-equity-Including-Investment-Accounts-Final.pdf\n",
      "Processing FAS-46_Off-Balance-Sheet-Assets-Under-Management-Final.pdf...\n",
      "Extracted 50 chunks from FAS-46_Off-Balance-Sheet-Assets-Under-Management-Final.pdf\n",
      "Processing FAS-47_Transfer-of-Assets-between-Investment-Pools-Final.pdf...\n",
      "Extracted 29 chunks from FAS-47_Transfer-of-Assets-between-Investment-Pools-Final.pdf\n",
      "Processing Fiancial-Accounting-Standard-No.-13-Disclosure-of-Bases-for-Determining-and-Allocating-Surplus-or-Defificit-in-Islamic-Insurance-Companies.pdf...\n",
      "Extracted 73 chunks from Fiancial-Accounting-Standard-No.-13-Disclosure-of-Bases-for-Determining-and-Allocating-Surplus-or-Defificit-in-Islamic-Insurance-Companies.pdf\n",
      "Processing Financial-Accounting-Standard-10-Istisnaa-and-Parallel-Istisnaa.pdf...\n",
      "Extracted 149 chunks from Financial-Accounting-Standard-10-Istisnaa-and-Parallel-Istisnaa.pdf\n",
      "Processing Financial-Accounting-Standard-12-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Insurance-Companies.pdf...\n",
      "Extracted 228 chunks from Financial-Accounting-Standard-12-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Insurance-Companies.pdf\n",
      "Processing Financial-Accounting-Standard-14-Investment-Funds.pdf...\n",
      "Extracted 126 chunks from Financial-Accounting-Standard-14-Investment-Funds.pdf\n",
      "Processing Financial-Accounting-Standard-15-Provisions-and-Reserves-in-Islamic-Insurance-Companies.pdf...\n",
      "Extracted 77 chunks from Financial-Accounting-Standard-15-Provisions-and-Reserves-in-Islamic-Insurance-Companies.pdf\n",
      "Processing Financial-Accounting-Standard-16-Foreign-Currency-Transactions-and-Foreign-Operations.pdf...\n",
      "Extracted 104 chunks from Financial-Accounting-Standard-16-Foreign-Currency-Transactions-and-Foreign-Operations.pdf\n",
      "Processing Financial-Accounting-Standard-18-Islamic-Financial-Services-Offffered-by-Conventional-Financial-Institutions.pdf...\n",
      "Extracted 51 chunks from Financial-Accounting-Standard-18-Islamic-Financial-Services-Offffered-by-Conventional-Financial-Institutions.pdf\n",
      "Processing Financial-Accounting-Standard-19-Contributions-in-Islamic-Insurance-Companies.pdf...\n",
      "Extracted 56 chunks from Financial-Accounting-Standard-19-Contributions-in-Islamic-Insurance-Companies.pdf\n",
      "Processing Financial-Accounting-Standard-21-Disclosure-on-Transfer-of-Assets.pdf...\n",
      "Extracted 46 chunks from Financial-Accounting-Standard-21-Disclosure-on-Transfer-of-Assets.pdf\n",
      "Processing Financial-Accounting-Standard-22-Segment-Reporting.pdf...\n",
      "Extracted 64 chunks from Financial-Accounting-Standard-22-Segment-Reporting.pdf\n",
      "Processing Financial-Accounting-Standard-23-Consolidation.pdf...\n",
      "Extracted 56 chunks from Financial-Accounting-Standard-23-Consolidation.pdf\n",
      "Processing Financial-Accounting-Standard-24-Investments-in-Associates.pdf...\n",
      "Extracted 50 chunks from Financial-Accounting-Standard-24-Investments-in-Associates.pdf\n",
      "Processing Financial-Accounting-Standard-25-Investment-in-Sukuk-Shares-and-Similar-Instruments.pdf...\n",
      "Extracted 104 chunks from Financial-Accounting-Standard-25-Investment-in-Sukuk-Shares-and-Similar-Instruments.pdf\n",
      "Processing Financial-Accounting-Standard-26-Investment-in-Real-Estate.pdf...\n",
      "Extracted 147 chunks from Financial-Accounting-Standard-26-Investment-in-Real-Estate.pdf\n",
      "Processing Financial-Accounting-Standard-27-Investment-accounts.pdf...\n",
      "Extracted 71 chunks from Financial-Accounting-Standard-27-Investment-accounts.pdf\n",
      "Processing Financial-Accounting-Standard-3-Mudaraba-Financing.pdf...\n",
      "Extracted 115 chunks from Financial-Accounting-Standard-3-Mudaraba-Financing.pdf\n",
      "Processing Financial-Accounting-Standard-4-Musharaka-Financing.pdf...\n",
      "Extracted 116 chunks from Financial-Accounting-Standard-4-Musharaka-Financing.pdf\n",
      "Processing Financial-Accounting-Standard-7-Salam-and-Parallel-Salam.pdf...\n",
      "Extracted 93 chunks from Financial-Accounting-Standard-7-Salam-and-Parallel-Salam.pdf\n",
      "Processing Financial-Accounting-Standard-9-Zakah.pdf...\n",
      "Extracted 121 chunks from Financial-Accounting-Standard-9-Zakah.pdf\n",
      "Processing Research-papers-1.pdf...\n",
      "Extracted 58 chunks from Research-papers-1.pdf\n",
      "Processing Research-papers-new.pdf...\n",
      "Extracted 160 chunks from Research-papers-new.pdf\n",
      "Total chunks extracted: 4267\n",
      "Creating vector database safely...\n",
      "Creating new database directory: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Processed batch 1 (100 documents)\n",
      "Processed batch 2 (100 documents)\n",
      "Processed batch 3 (100 documents)\n",
      "Processed batch 4 (100 documents)\n",
      "Processed batch 5 (100 documents)\n",
      "Processed batch 6 (100 documents)\n",
      "Processed batch 7 (100 documents)\n",
      "Processed batch 8 (100 documents)\n",
      "Processed batch 9 (100 documents)\n",
      "Processed batch 10 (100 documents)\n",
      "Processed batch 11 (100 documents)\n",
      "Processed batch 12 (100 documents)\n",
      "Processed batch 13 (100 documents)\n",
      "Processed batch 14 (100 documents)\n",
      "Processed batch 15 (100 documents)\n",
      "Processed batch 16 (100 documents)\n",
      "Processed batch 17 (100 documents)\n",
      "Processed batch 18 (100 documents)\n",
      "Error processing batch starting at index 1800: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 1900: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 2000: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 2100: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 2200: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 2300: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 2400: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 2500: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 2600: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 2700: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error processing batch starting at index 2800: Connection error.\n",
      "Error processing batch starting at index 2900: Connection error.\n",
      "Error processing batch starting at index 3000: Connection error.\n",
      "Error processing batch starting at index 3100: Connection error.\n",
      "Error processing batch starting at index 3200: Connection error.\n",
      "Error processing batch starting at index 3300: Connection error.\n",
      "Error processing batch starting at index 3400: Connection error.\n",
      "Error processing batch starting at index 3500: Connection error.\n",
      "Error processing batch starting at index 3600: Connection error.\n",
      "Error processing batch starting at index 3700: Connection error.\n",
      "Error processing batch starting at index 3800: Connection error.\n",
      "Error processing batch starting at index 3900: Connection error.\n",
      "Error processing batch starting at index 4000: Connection error.\n",
      "Error processing batch starting at index 4100: Connection error.\n",
      "Error processing batch starting at index 4200: Connection error.\n",
      "Created new vector database with consistent embedding dimensions in 'aaoifi_vector_db_20250510_171304_gqfbl3'\n",
      "Updated configuration to use new database directory: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Vector database created successfully in 'aaoifi_vector_db_20250510_171304_gqfbl3'\n",
      "\n",
      "Initializing AAOIFIStandardsEnhancementSystem...\n",
      "Initializing VectorDBManager with database directory: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Successfully connected to database at: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Successfully connected to collection: aaoifi_standards\n",
      "\n",
      "Listing available standards...\n",
      "1. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
      "2. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
      "3. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n",
      "4. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
      "5. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
      "6. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
      "7. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
      "8. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
      "9. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
      "10. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
      "11. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
      "12. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
      "13. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
      "14. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
      "15. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
      "16. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
      "17. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
      "18. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
      "19. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
      "20. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
      "21. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
      "22. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
      "23. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n",
      "\n",
      "Processing standard: FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
      "Processing standard: FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
      "Retrieving standard content...\n",
      "Reviewing standard and extracting key elements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_19276\\3989169443.py:58: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=self.llm, prompt=prompt)\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_19276\\3989169443.py:59: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = chain.run({})\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_backends\\sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    208\u001b[39m     sock = socket.create_connection(\n\u001b[32m    209\u001b[39m         address,\n\u001b[32m    210\u001b[39m         timeout,\n\u001b[32m    211\u001b[39m         source_address=source_address,\n\u001b[32m    212\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_transports\\default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Execute the safe approach test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtest_safe_approach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 204\u001b[39m, in \u001b[36mtest_safe_approach\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    202\u001b[39m standard_name = standards[\u001b[32m0\u001b[39m]\n\u001b[32m    203\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProcessing standard: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstandard_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m results = \u001b[43msystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandard_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m system.save_results()\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Display the final report\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mAAOIFIStandardsEnhancementSystem.process_standard\u001b[39m\u001b[34m(self, standard_name)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Step 2: Review and extract key elements\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReviewing standard and extracting key elements...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m review_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreview_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mself\u001b[39m.results[\u001b[33m\"\u001b[39m\u001b[33mreview_result\u001b[39m\u001b[33m\"\u001b[39m] = review_result\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Step 3: Propose enhancements\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mReviewAgent.execute\u001b[39m\u001b[34m(self, standard)\u001b[39m\n\u001b[32m     53\u001b[39m prompt = ChatPromptTemplate.from_messages([\n\u001b[32m     54\u001b[39m     SystemMessage(content=\u001b[38;5;28mself\u001b[39m.system_prompt),\n\u001b[32m     55\u001b[39m     HumanMessage(content=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStandard Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstandard.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mContent:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstandard.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m ])\n\u001b[32m     58\u001b[39m chain = LLMChain(llm=\u001b[38;5;28mself\u001b[39m.llm, prompt=prompt)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m result = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Try to parse the result into structured data\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# If the result is in YAML or other format, convert it to dict\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Here we're assuming the model returns well-structured content that can be parsed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\base.py:603\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) != \u001b[32m1\u001b[39m:\n\u001b[32m    602\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    604\u001b[39m         _output_key\n\u001b[32m    605\u001b[39m     ]\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    609\u001b[39m         _output_key\n\u001b[32m    610\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\llm.py:127\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    124\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    125\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\llm.py:139\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    137\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    147\u001b[39m         cast(\u001b[38;5;28mlist\u001b[39m, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:947\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    940\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     **kwargs: Any,\n\u001b[32m    945\u001b[39m ) -> LLMResult:\n\u001b[32m    946\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:766\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    765\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    772\u001b[39m         )\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    774\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1012\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1016\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:476\u001b[39m, in \u001b[36mChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[39m\n\u001b[32m    470\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    471\u001b[39m params = {\n\u001b[32m    472\u001b[39m     **params,\n\u001b[32m    473\u001b[39m     **({\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[32m    474\u001b[39m     **kwargs,\n\u001b[32m    475\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:387\u001b[39m, in \u001b[36mChatOpenAI.completion_with_retry\u001b[39m\u001b[34m(self, run_manager, **kwargs)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m retry_decorator = _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager=run_manager)\n\u001b[32m    391\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_completion_with_retry\u001b[39m(**kwargs: Any) -> Any:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\_base_client.py:1001\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    998\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1000\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1003\u001b[39m log.debug(\n\u001b[32m   1004\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1005\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1009\u001b[39m     response.headers,\n\u001b[32m   1010\u001b[39m )\n\u001b[32m   1011\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "# Execute the safe approach test\n",
    "test_safe_approach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af413c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing VectorDBManager with database directory: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Successfully connected to database at: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Successfully connected to collection: aaoifi_standards\n",
      "Available standards:\n",
      "1. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
      "2. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
      "3. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n",
      "4. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
      "5. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
      "6. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
      "7. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
      "8. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
      "9. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
      "10. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
      "11. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
      "12. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
      "13. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
      "14. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
      "15. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
      "16. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
      "17. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
      "18. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
      "19. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
      "20. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
      "21. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
      "22. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
      "23. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n",
      "\n",
      "Processing standard: FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
      "Processing standard: FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
      "Retrieving standard content...\n",
      "Reviewing standard and extracting key elements...\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_transports\\default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_backends\\sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    208\u001b[39m     sock = socket.create_connection(\n\u001b[32m    209\u001b[39m         address,\n\u001b[32m    210\u001b[39m         timeout,\n\u001b[32m    211\u001b[39m         source_address=source_address,\n\u001b[32m    212\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_transports\\default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\httpx\\_transports\\default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m standard_name = standards[\u001b[32m0\u001b[39m]\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mProcessing standard: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstandard_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m results = \u001b[43msystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandard_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m system.save_results()\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Display the final report\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mAAOIFIStandardsEnhancementSystem.process_standard\u001b[39m\u001b[34m(self, standard_name)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Step 2: Review and extract key elements\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReviewing standard and extracting key elements...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m review_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreview_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstandard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mself\u001b[39m.results[\u001b[33m\"\u001b[39m\u001b[33mreview_result\u001b[39m\u001b[33m\"\u001b[39m] = review_result\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Step 3: Propose enhancements\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mReviewAgent.execute\u001b[39m\u001b[34m(self, standard)\u001b[39m\n\u001b[32m     53\u001b[39m prompt = ChatPromptTemplate.from_messages([\n\u001b[32m     54\u001b[39m     SystemMessage(content=\u001b[38;5;28mself\u001b[39m.system_prompt),\n\u001b[32m     55\u001b[39m     HumanMessage(content=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStandard Name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstandard.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mContent:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstandard.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     56\u001b[39m ])\n\u001b[32m     58\u001b[39m chain = LLMChain(llm=\u001b[38;5;28mself\u001b[39m.llm, prompt=prompt)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m result = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Try to parse the result into structured data\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# If the result is in YAML or other format, convert it to dict\u001b[39;00m\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Here we're assuming the model returns well-structured content that can be parsed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\base.py:603\u001b[39m, in \u001b[36mChain.run\u001b[39m\u001b[34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) != \u001b[32m1\u001b[39m:\n\u001b[32m    602\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`run` supports only one positional argument.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[32m    604\u001b[39m         _output_key\n\u001b[32m    605\u001b[39m     ]\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[32m    609\u001b[39m         _output_key\n\u001b[32m    610\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:191\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    190\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\base.py:386\u001b[39m, in \u001b[36mChain.__call__\u001b[39m\u001b[34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m    356\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    377\u001b[39m \u001b[33;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    379\u001b[39m config = {\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks,\n\u001b[32m    381\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m: tags,\n\u001b[32m    382\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m    383\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: run_name,\n\u001b[32m    384\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\base.py:167\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    166\u001b[39m     run_manager.on_chain_error(e)\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    168\u001b[39m run_manager.on_chain_end(outputs)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\base.py:157\u001b[39m, in \u001b[36mChain.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_inputs(inputs)\n\u001b[32m    156\u001b[39m     outputs = (\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    159\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call(inputs)\n\u001b[32m    160\u001b[39m     )\n\u001b[32m    162\u001b[39m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = \u001b[38;5;28mself\u001b[39m.prep_outputs(\n\u001b[32m    163\u001b[39m         inputs, outputs, return_only_outputs\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\llm.py:127\u001b[39m, in \u001b[36mLLMChain._call\u001b[39m\u001b[34m(self, inputs, run_manager)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(\n\u001b[32m    123\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    124\u001b[39m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m    125\u001b[39m     run_manager: Optional[CallbackManagerForChainRun] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    126\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.create_outputs(response)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain\\chains\\llm.py:139\u001b[39m, in \u001b[36mLLMChain.generate\u001b[39m\u001b[34m(self, input_list, run_manager)\u001b[39m\n\u001b[32m    137\u001b[39m callbacks = run_manager.get_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm, BaseLanguageModel):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    146\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.llm.bind(stop=stop, **\u001b[38;5;28mself\u001b[39m.llm_kwargs).batch(\n\u001b[32m    147\u001b[39m         cast(\u001b[38;5;28mlist\u001b[39m, prompts), {\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m: callbacks}\n\u001b[32m    148\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:947\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    940\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     **kwargs: Any,\n\u001b[32m    945\u001b[39m ) -> LLMResult:\n\u001b[32m    946\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:766\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    765\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    772\u001b[39m         )\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    774\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1012\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1016\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:476\u001b[39m, in \u001b[36mChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[39m\n\u001b[32m    470\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    471\u001b[39m params = {\n\u001b[32m    472\u001b[39m     **params,\n\u001b[32m    473\u001b[39m     **({\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[32m    474\u001b[39m     **kwargs,\n\u001b[32m    475\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:387\u001b[39m, in \u001b[36mChatOpenAI.completion_with_retry\u001b[39m\u001b[34m(self, run_manager, **kwargs)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[32m--> \u001b[39m\u001b[32m387\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    389\u001b[39m retry_decorator = _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager=run_manager)\n\u001b[32m    391\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_completion_with_retry\u001b[39m(**kwargs: Any) -> Any:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\anaconda3\\envs\\gpu_env\\Lib\\site-packages\\openai\\_base_client.py:1001\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    998\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1000\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1003\u001b[39m log.debug(\n\u001b[32m   1004\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1005\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1009\u001b[39m     response.headers,\n\u001b[32m   1010\u001b[39m )\n\u001b[32m   1011\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "# Complete workflow for processing standards\n",
    "\n",
    "# Step 1: Initialize your system\n",
    "system = AAOIFIStandardsEnhancementSystem()\n",
    "\n",
    "# Step 2: List all available standards to know what you can process\n",
    "print(\"Available standards:\")\n",
    "standards = system.list_available_standards()\n",
    "for i, standard in enumerate(standards):\n",
    "    print(f\"{i+1}. {standard}\")\n",
    "\n",
    "# Step 3: Process a specific standard by index or name\n",
    "# Option A: Process by index (e.g., process the first standard)\n",
    "if standards and standards[0] != \"No standards found in the database. Please process PDFs first.\":\n",
    "    # Process the first standard in the list\n",
    "    standard_name = standards[0]\n",
    "    print(f\"\\nProcessing standard: {standard_name}\")\n",
    "    results = system.process_standard(standard_name)\n",
    "    system.save_results()\n",
    "    \n",
    "    # Display the final report\n",
    "    if \"final_report\" in results:\n",
    "        print(\"\\n===== FINAL REPORT =====\\n\")\n",
    "        print(results[\"final_report\"])\n",
    "\n",
    "# Option B: Process by specific name (replace with actual standard name)\n",
    "specific_standard = \"Your_Standard_Name_Here\"  # Replace with a name from your standards list\n",
    "if specific_standard in standards:\n",
    "    print(f\"\\nProcessing specific standard: {specific_standard}\")\n",
    "    results = system.process_standard(specific_standard)\n",
    "    system.save_results()\n",
    "    \n",
    "    # Display the final report\n",
    "    if \"final_report\" in results:\n",
    "        print(\"\\n===== FINAL REPORT =====\\n\")\n",
    "        print(results[\"final_report\"])\n",
    "else:\n",
    "    print(f\"Standard '{specific_standard}' not found in available standards.\")\n",
    "\n",
    "# Step 4: Process all standards in batch (optional)\n",
    "def process_all_standards():\n",
    "    \"\"\"Process all available standards one by one.\"\"\"\n",
    "    print(\"\\nProcessing all standards in batch mode...\")\n",
    "    \n",
    "    all_results = {}\n",
    "    for standard in standards:\n",
    "        if standard != \"No standards found in the database. Please process PDFs first.\":\n",
    "            print(f\"\\nProcessing standard: {standard}\")\n",
    "            try:\n",
    "                results = system.process_standard(standard)\n",
    "                system.save_results()\n",
    "                all_results[standard] = results\n",
    "                print(f\" Successfully processed: {standard}\")\n",
    "            except Exception as e:\n",
    "                print(f\" Error processing standard {standard}: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nBatch processing completed!\")\n",
    "    print(f\"Successfully processed {len(all_results)} standards.\")\n",
    "    return all_results\n",
    "\n",
    "# Uncomment the following line to process all standards\n",
    "# all_results = process_all_standards()\n",
    "\n",
    "# Step 5: View results for all processed standards\n",
    "def list_processed_results():\n",
    "    \"\"\"List all processed results in the results directory.\"\"\"\n",
    "    import os\n",
    "    \n",
    "    results_dir = config.OUTPUT_DIR\n",
    "    if not os.path.exists(results_dir):\n",
    "        print(f\"Results directory '{results_dir}' does not exist.\")\n",
    "        return\n",
    "        \n",
    "    result_files = [f for f in os.listdir(results_dir) if f.endswith(\"_results.json\") or f.endswith(\"_final_report.md\")]\n",
    "    \n",
    "    if not result_files:\n",
    "        print(f\"No result files found in '{results_dir}'.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nFound {len(result_files)} result files:\")\n",
    "    for i, result_file in enumerate(result_files):\n",
    "        print(f\"{i+1}. {result_file}\")\n",
    "        \n",
    "    return result_files\n",
    "\n",
    "# Uncomment to list all processed results\n",
    "# result_files = list_processed_results()\n",
    "\n",
    "# Step 6: Read a specific result file\n",
    "def read_report(report_name):\n",
    "    \"\"\"Read and display a specific report.\"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    results_dir = config.OUTPUT_DIR\n",
    "    report_path = os.path.join(results_dir, report_name)\n",
    "    \n",
    "    if not os.path.exists(report_path):\n",
    "        print(f\"Report file '{report_path}' does not exist.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"Reading report: {report_name}\")\n",
    "    \n",
    "    if report_name.endswith(\"_final_report.md\"):\n",
    "        # Read markdown report\n",
    "        with open(report_path, 'r') as f:\n",
    "            report_content = f.read()\n",
    "        print(\"\\n===== REPORT CONTENT =====\\n\")\n",
    "        print(report_content)\n",
    "        return report_content\n",
    "    elif report_name.endswith(\"_results.json\"):\n",
    "        # Read JSON results\n",
    "        with open(report_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "        print(\"\\n===== RESULTS SUMMARY =====\\n\")\n",
    "        print(f\"Standard Name: {results.get('standard_name', 'Unknown')}\")\n",
    "        print(f\"Contains Final Report: {'Yes' if 'final_report' in results else 'No'}\")\n",
    "        print(f\"Contains Review Result: {'Yes' if 'review_result' in results else 'No'}\")\n",
    "        print(f\"Contains Enhancement Proposals: {'Yes' if 'enhancement_proposals' in results else 'No'}\")\n",
    "        print(f\"Contains Validation Result: {'Yes' if 'validation_result' in results else 'No'}\")\n",
    "        return results\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {report_name}\")\n",
    "        return None\n",
    "\n",
    "# Example: Read a specific report (uncomment and replace with actual report name)\n",
    "# read_report(\"standard_name_final_report.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f21efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available standards:\n",
      "1. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
      "2. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
      "3. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n",
      "4. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
      "5. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
      "6. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
      "7. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
      "8. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
      "9. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
      "10. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
      "11. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
      "12. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
      "13. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
      "14. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
      "15. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
      "16. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
      "17. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
      "18. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
      "19. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
      "20. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
      "21. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
      "22. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
      "23. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n"
     ]
    }
   ],
   "source": [
    "print(\"Available standards:\")\n",
    "standards = system.list_available_standards()\n",
    "for i, standard in enumerate(standards):\n",
    "    print(f\"{i+1}. {standard}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c01183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardDocument:\n",
    "    \"\"\"Class to represent an AAOIFI standard document.\"\"\"\n",
    "    def __init__(self, name: str, content: str):\n",
    "        self.name = name\n",
    "        self.content = content\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents in the system.\"\"\"\n",
    "    def __init__(self, name: str, description: str, model_name: str = Config.GPT4_MODEL):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.model_name = model_name\n",
    "        # Ensure API key is set before initializing ChatOpenAI\n",
    "        if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not set in environment.\")\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0.2)\n",
    "\n",
    "\n",
    "    def execute(self, input_data: Any) -> Any:\n",
    "        \"\"\"Execute the agent's task.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "class ReviewAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for reviewing standards and extracting key elements.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"ReviewAgent\",\n",
    "            description=\"Analyzes AAOIFI standards to extract key elements, principles, and requirements.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an expert in Islamic finance and AAOIFI standards. Your task is to carefully review\n",
    "        the provided standard document and extract the following key elements.\n",
    "        Use clear Markdown headings for each section exactly as listed below (e.g., ## Core principles and objectives).\n",
    "\n",
    "        ## Core principles and objectives\n",
    "        [Your extraction here]\n",
    "\n",
    "        ## Key definitions and terminology\n",
    "        [Your extraction here]\n",
    "        \n",
    "        # ... (and so on for other sections) ...\n",
    "\n",
    "        Be thorough but concise.\n",
    "        \"\"\"\n",
    "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a standard document and extract its key elements.\n",
    "\n",
    "        Args:\n",
    "            standard: The standard document to analyze.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the extracted key elements.\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"Standard Name: {standard.name}\\n\\nContent:\\n{standard.content}\")\n",
    "        ])\n",
    "\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        # The chain.run({}) is problematic if the prompt itself needs variables.\n",
    "        # Here, standard.name and standard.content are already formatted into the HumanMessage.\n",
    "        # If the prompt template had input_variables, they would be passed to run().\n",
    "        result_text = chain.run({}) # Assuming the prompt is self-contained after formatting.\n",
    "\n",
    "        # Try to parse the result into structured data (simple approach)\n",
    "        parsed_result = {\n",
    "            \"standard_name\": standard.name,\n",
    "            \"review_result\": result_text, # This is the full text from LLM\n",
    "            \"core_principles\": self._extract_section(result_text, \"Core principles and objectives\"),\n",
    "            \"key_definitions\": self._extract_section(result_text, \"Key definitions and terminology\"),\n",
    "            \"main_requirements\": self._extract_section(result_text, \"Main requirements and procedures\"),\n",
    "            \"compliance_criteria\": self._extract_section(result_text, \"Compliance criteria and guidelines\"), # \"guidelines\" was missing\n",
    "            \"implementation_considerations\": self._extract_section(result_text, \"Practical implementation considerations\") # \"Practical\" was missing\n",
    "        }\n",
    "        return parsed_result\n",
    "\n",
    "\n",
    "    def _extract_section(self, text: str, section_name: str) -> str:\n",
    "        \"\"\"Helper method to extract specific sections from the review result using regex.\"\"\"\n",
    "        # Regex to find a heading (e.g., \"1. Section Name\" or \"Section Name:\") and capture text until the next heading or end of string\n",
    "        # This assumes headings are followed by a newline.\n",
    "        # It also makes section_name matching case-insensitive.\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:^\\s*\\d*\\.?\\s*|\\n\\s*\\d*\\.?\\s*)\" + re.escape(section_name) + r\"\\s*[:\\n](.*?)(?=\\n\\s*\\d*\\.?\\s*\\w+.*?\\s*[:\\n]|\\Z)\",\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return f\"Section '{section_name}' not found or parsing error.\"\n",
    "\n",
    "\n",
    "class EnhancementAgent(BaseAgent):\n",
    "    \"\"\"Agent tasked with proposing modifications or enhancements to standards.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"EnhancementAgent\",\n",
    "            description=\"Proposes AI-driven modifications and enhancements to AAOIFI standards.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an AI expert specializing in Islamic finance and AAOIFI standards enhancement.\n",
    "        Your task is to propose thoughtful modifications and enhancements to the standard based\n",
    "        on the review provided.\n",
    "\n",
    "        Consider the following aspects in your proposals:\n",
    "\n",
    "        1. Clarity improvements: Suggest clearer language or better organization where appropriate\n",
    "        2. Modern context adaptations: Propose updates to address contemporary financial practices\n",
    "        3. Technological integration: Recommend ways to incorporate digital technologies and fintech\n",
    "        4. Cross-reference enhancements: Suggest improved links to related standards or principles\n",
    "        5. Practical implementation: Provide more actionable guidance for practitioners\n",
    "\n",
    "        For each suggestion, provide:\n",
    "        - The specific section or clause being enhanced (if applicable, otherwise general proposal)\n",
    "        - The current text or concept (if applicable, very brief summary)\n",
    "        - Your proposed modification or addition\n",
    "        - A brief justification explaining the benefit of your enhancement\n",
    "\n",
    "        Structure your response clearly, perhaps using bullet points for each proposal.\n",
    "        Ensure all suggestions maintain strict compliance with Shariah principles.\n",
    "        \"\"\"\n",
    "\n",
    "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Propose enhancements to a standard based on the review.\n",
    "\n",
    "        Args:\n",
    "            review_result: The result from the ReviewAgent (the full dictionary).\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing proposed enhancements.\n",
    "        \"\"\"\n",
    "        # We need the text of the review, not the whole dict, for the prompt\n",
    "        review_text_summary = review_result.get(\"review_result\", \"No review summary available.\")\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\n\\nSummary of Standard Review:\\n{review_text_summary}\")\n",
    "        ])\n",
    "\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        result_text = chain.run({})\n",
    "\n",
    "        return {\n",
    "            \"standard_name\": review_result[\"standard_name\"],\n",
    "            \"enhancement_proposals\": result_text # This is the LLM's textual output\n",
    "        }\n",
    "\n",
    "class ValidationAgent(BaseAgent):\n",
    "    \"\"\"Agent for validating and approving proposed changes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"ValidationAgent\",\n",
    "            description=\"Validates proposed changes based on Shariah compliance and practicality.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a senior Shariah scholar and AAOIFI standards expert. Your role is to validate\n",
    "        proposed enhancements to ensure they maintain compliance with Islamic principles and\n",
    "        practical applicability.\n",
    "\n",
    "        For each proposed enhancement, evaluate:\n",
    "\n",
    "        1. Shariah Compliance: Does the proposal align with Islamic principles and AAOIFI's mission?\n",
    "        2. Technical Accuracy: Is the proposed language precise and technically sound?\n",
    "        3. Practical Applicability: Can the enhancement be practically implemented by Islamic financial institutions?\n",
    "        4. Consistency: Does it maintain consistency with other standards and established practices?\n",
    "        5. Value Addition: Does it meaningfully improve the standard?\n",
    "\n",
    "        For each proposal, provide:\n",
    "        - Your assessment (e.g., Approved, Rejected, Needs Modification)\n",
    "        - A detailed justification for your decision, referencing the evaluation criteria above.\n",
    "        - Suggested refinements if \"Needs Modification\".\n",
    "\n",
    "        Be thorough in your analysis and maintain the highest standards of Islamic finance integrity.\n",
    "        Structure your response clearly, addressing each proposal from the input.\n",
    "        \"\"\"\n",
    "\n",
    "    def execute(self, enhancement_result: Dict[str, Any], original_review: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate proposed enhancements based on Shariah compliance and practicality.\n",
    "\n",
    "        Args:\n",
    "            enhancement_result: The result from the EnhancementAgent (dictionary).\n",
    "            original_review: The original review from the ReviewAgent (dictionary).\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing validation results.\n",
    "        \"\"\"\n",
    "        review_text_summary = original_review.get(\"review_result\", \"No review summary available.\")\n",
    "        enhancement_proposals_text = enhancement_result.get(\"enhancement_proposals\", \"No enhancement proposals available.\")\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {enhancement_result['standard_name']}\n",
    "\n",
    "            Original Standard Review Summary:\n",
    "            {review_text_summary}\n",
    "\n",
    "            Proposed Enhancements to Validate:\n",
    "            {enhancement_proposals_text}\n",
    "            \"\"\")\n",
    "        ])\n",
    "\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        result_text = chain.run({})\n",
    "\n",
    "        return {\n",
    "            \"standard_name\": enhancement_result[\"standard_name\"],\n",
    "            \"validation_result\": result_text # This is the LLM's textual output\n",
    "        }\n",
    "\n",
    "class FinalReportAgent(BaseAgent):\n",
    "    \"\"\"Agent for generating a comprehensive final report.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinalReportAgent\",\n",
    "            description=\"Compiles all findings and recommendations into a comprehensive report.\",\n",
    "            model_name=Config.GPT4_MODEL # GPT-4 for high-quality report generation\n",
    "        )\n",
    "\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a professional report writer specializing in Islamic finance standards. Your task is to\n",
    "        compile all the findings, enhancements, and validations into a comprehensive, well-structured report\n",
    "        in Markdown format.\n",
    "\n",
    "        Your report should include the following sections:\n",
    "\n",
    "        1.  **Executive Summary**: Brief overview of the standard reviewed, key findings, summary of proposed changes, and validation outcomes.\n",
    "        2.  **Standard Overview**: Summary of the original standard's purpose and core components (based on the review).\n",
    "        3.  **Key Findings from Review**: Major elements and considerations identified during the initial review.\n",
    "        4.  **Proposed Enhancements**: Clear presentation of all proposed modifications.\n",
    "        5.  **Validation Results**: Summary of the validation process and outcomes for each proposal.\n",
    "        6.  **Consolidated Recommendations**: A final list of approved or modified enhancements.\n",
    "        7.  **Implementation Considerations**: Practical next steps for adopting approved changes.\n",
    "        8.  **Conclusion**: Final thoughts on the impact of the proposed enhancements.\n",
    "\n",
    "        Write in a professional, clear, and objective style appropriate for AAOIFI stakeholders and Islamic finance professionals.\n",
    "        Use Markdown for formatting (headings, lists, bolding).\n",
    "        \"\"\"\n",
    "\n",
    "    def execute(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive final report.\n",
    "\n",
    "        Args:\n",
    "            all_results: Combined results from all previous agents.\n",
    "                         Expected keys: 'standard_name', 'review_text',\n",
    "                                        'enhancements_text', 'validation_text'.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the final report text.\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {all_results['standard_name']}\n",
    "\n",
    "            Full Text of Standard Review:\n",
    "            {all_results['review_text']}\n",
    "\n",
    "            Full Text of Proposed Enhancements:\n",
    "            {all_results['enhancements_text']}\n",
    "\n",
    "            Full Text of Validation of Enhancements:\n",
    "            {all_results['validation_text']}\n",
    "            \"\"\")\n",
    "        ])\n",
    "\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        report_text = chain.run({})\n",
    "\n",
    "        return {\n",
    "            \"standard_name\": all_results[\"standard_name\"],\n",
    "            \"final_report\": report_text # This is the LLM's textual output (Markdown)\n",
    "        }\n",
    "\n",
    "\n",
    "class VectorDBManager:\n",
    "    \"\"\"Manager for interacting with the vector database.\"\"\"\n",
    "\n",
    "    def __init__(self, db_directory: str = None, collection_name: str = config.COLLECTION_NAME):\n",
    "        self.db_directory = db_directory if db_directory else config.DB_DIRECTORY\n",
    "        self.collection_name = collection_name\n",
    "        self.embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "\n",
    "        print(f\"Initializing VectorDBManager with database directory: {self.db_directory}\")\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(self.db_directory):\n",
    "                print(f\"Database directory {self.db_directory} does not exist. Creating it.\")\n",
    "                os.makedirs(self.db_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.db_directory)\n",
    "            print(f\"Successfully created PersistentClient for database at: {self.db_directory}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fatal error creating PersistentClient for {self.db_directory}: {str(e)}\")\n",
    "            self.client = None\n",
    "            self.collection = None\n",
    "            raise  # Reraise critical error\n",
    "\n",
    "        if self.client:\n",
    "            try:\n",
    "                # Try to get the collection; if it fails, try to create it.\n",
    "                self.collection = self.client.get_or_create_collection(name=self.collection_name)\n",
    "                print(f\"Successfully accessed/created collection: {self.collection_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing/creating collection '{self.collection_name}': {str(e)}\")\n",
    "                self.collection = None\n",
    "        else:\n",
    "            self.collection = None\n",
    "\n",
    "\n",
    "    def get_standard_content(self, standard_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the content for a specific standard.\n",
    "\n",
    "        Args:\n",
    "            standard_name: The name of the standard to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            The combined content of the standard, or an error message.\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            return \"Error: Vector database collection not properly initialized.\"\n",
    "\n",
    "        try:\n",
    "            # Query for all chunks belonging to this standard\n",
    "            # A large n_results is needed if a standard has many chunks\n",
    "            # Chroma's default n_results is 10.\n",
    "            # To get ALL documents matching a filter, it's better to use collection.get()\n",
    "            results = self.collection.get(\n",
    "                where={\"source\": standard_name},\n",
    "                include=[\"documents\"] # Only fetch documents\n",
    "            )\n",
    "\n",
    "            if results and results['documents']:\n",
    "                # Sort by chunk_id if available in metadata, though .get() doesn't easily allow sorting\n",
    "                # For now, just join them. If order is critical, store chunk_id and sort post-retrieval.\n",
    "                # The current split_text_into_chunks adds metadata, but .get() returns raw docs.\n",
    "                # Let's assume for now that the order from .get() is sufficient or re-query with sort if needed.\n",
    "                # The current query in `create_vector_database` uses IDs like `doc_{i+j}` which are sequential.\n",
    "                # If we rely on implicit order of `get`, it should be mostly fine.\n",
    "                \n",
    "                # The metadata is not directly available here to sort by chunk_id without another call or more complex query\n",
    "                # For simplicity, joining in received order.\n",
    "                standard_content = \"\\n\\n\".join(doc for doc in results['documents'] if doc)\n",
    "                if not standard_content:\n",
    "                    return f\"No document content found for standard: {standard_name}, though metadatas might exist.\"\n",
    "                return standard_content\n",
    "            else:\n",
    "                return f\"No content found for standard: {standard_name}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error querying for standard {standard_name}: {str(e)}\"\n",
    "\n",
    "    def list_available_standards(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all available standards in the database.\n",
    "\n",
    "        Returns:\n",
    "            A list of standard names, or an error message list.\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            return [\"Error: Vector database collection not properly initialized.\"]\n",
    "\n",
    "        try:\n",
    "            results = self.collection.get(include=[\"metadatas\"]) # Fetch only metadatas\n",
    "            if results and results['metadatas']:\n",
    "                standards = set()\n",
    "                for metadata in results['metadatas']:\n",
    "                    if metadata and 'source' in metadata: # Check if metadata is not None\n",
    "                        standards.add(metadata['source'])\n",
    "                if not standards:\n",
    "                    return [\"No standards found in the database. Metadatas might be empty or lack 'source'.\"]\n",
    "                return sorted(list(standards))\n",
    "            else:\n",
    "                return [\"No standards found in the database. Please process PDFs first.\"]\n",
    "        except Exception as e:\n",
    "            return [f\"Error listing standards: {str(e)}\"]\n",
    "\n",
    "\n",
    "class AAOIFIStandardsEnhancementSystem:\n",
    "    \"\"\"Main system coordinating the multi-agent process.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize vector database manager\n",
    "        # It will use config.DB_DIRECTORY which might be updated by process_pdfs_safe\n",
    "        self.db_manager = VectorDBManager()\n",
    "\n",
    "        # Initialize agents\n",
    "        self.review_agent = ReviewAgent()\n",
    "        self.enhancement_agent = EnhancementAgent()\n",
    "        self.validation_agent = ValidationAgent()\n",
    "        self.report_agent = FinalReportAgent()\n",
    "\n",
    "        # Track processing results for a single standard run\n",
    "        self.current_processing_results = {}\n",
    "\n",
    "    def list_available_standards(self) -> List[str]:\n",
    "        \"\"\"List all available standards in the system.\"\"\"\n",
    "        return self.db_manager.list_available_standards()\n",
    "\n",
    "    def process_standard(self, standard_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a single standard through the complete pipeline.\n",
    "\n",
    "        Args:\n",
    "            standard_name: The name of the standard to process.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the final results for this standard.\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing standard: {standard_name}\")\n",
    "        self.current_processing_results = {\"standard_name\": standard_name}\n",
    "\n",
    "        # Step 1: Get standard content from the vector database\n",
    "        print(\"  Retrieving standard content...\")\n",
    "        content = self.db_manager.get_standard_content(standard_name)\n",
    "        if content.startswith(\"Error:\") or content.startswith(\"No content found\"):\n",
    "            print(f\"  Error retrieving content for {standard_name}: {content}\")\n",
    "            self.current_processing_results[\"error\"] = content\n",
    "            return self.current_processing_results\n",
    "        \n",
    "        standard_doc = StandardDocument(name=standard_name, content=content)\n",
    "        self.current_processing_results[\"original_content_excerpt\"] = content[:500] + \"...\" if len(content) > 500 else content\n",
    "\n",
    "        # Step 2: Review and extract key elements\n",
    "        print(\"  Reviewing standard and extracting key elements...\")\n",
    "        review_output = self.review_agent.execute(standard_doc)\n",
    "        self.current_processing_results[\"review_output\"] = review_output # Store the whole dict\n",
    "\n",
    "        # Step 3: Propose enhancements\n",
    "        print(\"  Proposing enhancements...\")\n",
    "        enhancement_output = self.enhancement_agent.execute(review_output) # Pass the dict\n",
    "        self.current_processing_results[\"enhancement_output\"] = enhancement_output\n",
    "\n",
    "        # Step 4: Validate proposed changes\n",
    "        print(\"  Validating proposed changes...\")\n",
    "        # Validation agent needs enhancement_output (dict) and review_output (dict)\n",
    "        validation_output = self.validation_agent.execute(enhancement_output, review_output)\n",
    "        self.current_processing_results[\"validation_output\"] = validation_output\n",
    "\n",
    "        # Step 5: Generate final report\n",
    "        print(\"  Generating final report...\")\n",
    "        # Prepare data for report agent\n",
    "        report_input_data = {\n",
    "            \"standard_name\": standard_name,\n",
    "            \"review_text\": review_output.get(\"review_result\", \"N/A\"),\n",
    "            \"enhancements_text\": enhancement_output.get(\"enhancement_proposals\", \"N/A\"),\n",
    "            \"validation_text\": validation_output.get(\"validation_result\", \"N/A\")\n",
    "        }\n",
    "        final_report_output = self.report_agent.execute(report_input_data)\n",
    "        self.current_processing_results[\"final_report_output\"] = final_report_output\n",
    "\n",
    "        print(f\"Processing completed for standard: {standard_name}\")\n",
    "        return self.current_processing_results\n",
    "\n",
    "    def save_results(self, results_to_save: Dict[str, Any], output_dir: str = config.OUTPUT_DIR):\n",
    "        \"\"\"Save all results to output files.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        if not results_to_save:\n",
    "            print(\"No results to save.\")\n",
    "            return\n",
    "\n",
    "        standard_name = results_to_save.get(\"standard_name\", \"unknown_standard\")\n",
    "        # Sanitize standard_name for filename\n",
    "        safe_standard_name = re.sub(r'[^\\w\\-_\\. ]', '_', standard_name)\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        filename_json = os.path.join(output_dir, f\"{safe_standard_name}_full_results_{timestamp}.json\")\n",
    "        with open(filename_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_to_save, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Full results saved to {filename_json}\")\n",
    "\n",
    "        # Also save the final report separately as Markdown\n",
    "        final_report_text = results_to_save.get(\"final_report_output\", {}).get(\"final_report\")\n",
    "        if final_report_text:\n",
    "            filename_md = os.path.join(output_dir, f\"{safe_standard_name}_final_report_{timestamp}.md\")\n",
    "            with open(filename_md, 'w', encoding='utf-8') as f:\n",
    "                f.write(final_report_text)\n",
    "            print(f\"Final report saved to {filename_md}\")\n",
    "\n",
    "\n",
    "# --- Functions for recreating vector database safely ---\n",
    "def safely_recreate_vector_database(documents):\n",
    "    \"\"\"Create a new vector database from document chunks with proper handling of locked files.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "\n",
    "    # Create a new directory with a unique name to avoid conflicts\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
    "    # Base directory for all DB versions\n",
    "    base_db_parent_dir = \"aaoifi_vector_db_versions\"\n",
    "    os.makedirs(base_db_parent_dir, exist_ok=True)\n",
    "    new_db_directory = os.path.join(base_db_parent_dir, f\"db_{timestamp}_{random_str}\")\n",
    "\n",
    "    print(f\"Creating new database in directory: {new_db_directory}\")\n",
    "    os.makedirs(new_db_directory, exist_ok=True)\n",
    "\n",
    "    client = chromadb.PersistentClient(path=new_db_directory)\n",
    "    # Always create a new collection in a new DB path\n",
    "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
    "    \n",
    "    batch_size = 100 # As used in create_vector_database\n",
    "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_documents = documents[i:i+batch_size]\n",
    "        current_batch_num = (i // batch_size) + 1\n",
    "        print(f\"Processing batch {current_batch_num}/{num_batches} for new vector database...\")\n",
    "\n",
    "        texts = [doc[\"content\"] for doc in batch_documents]\n",
    "        # Use more robust IDs tied to document source and chunk\n",
    "        ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}\" for doc in batch_documents]\n",
    "        # Check for ID uniqueness within the batch (essential for Chroma)\n",
    "        if len(ids) != len(set(ids)):\n",
    "            print(f\"Warning: Duplicate IDs generated in batch {current_batch_num}. This may cause issues.\")\n",
    "            # Simple fix: append index within batch to ensure uniqueness for this add op\n",
    "            ids = [f\"{id_}_{j}\" for j, id_ in enumerate(ids)]\n",
    "\n",
    "        metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
    "\n",
    "        if not texts:\n",
    "            print(f\"Skipping empty batch {current_batch_num}.\")\n",
    "            continue\n",
    "        try:\n",
    "            embeds = embeddings.embed_documents(texts)\n",
    "            collection.add(embeddings=embeds, documents=texts, ids=ids, metadatas=metadatas)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {current_batch_num} starting at index {i}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Created new vector database with consistent embedding dimensions in '{new_db_directory}'\")\n",
    "    \n",
    "    # Update the global config to point to the new directory for the current session\n",
    "    config.DB_DIRECTORY = new_db_directory\n",
    "    print(f\"Updated global config.DB_DIRECTORY to: {config.DB_DIRECTORY}\")\n",
    "    \n",
    "    return client\n",
    "\n",
    "def process_pdfs_safe():\n",
    "    \"\"\"Main function to process PDFs and create vector database safely.\"\"\"\n",
    "    PDF_FOLDER = config.PDF_FOLDER\n",
    "    if not os.path.exists(PDF_FOLDER):\n",
    "        print(f\"Error: The PDF folder '{PDF_FOLDER}' does not exist.\")\n",
    "        return None\n",
    "    if not os.listdir(PDF_FOLDER):\n",
    "        print(f\"The PDF folder '{PDF_FOLDER}' is empty. Please add PDF files to process.\")\n",
    "        return None\n",
    "\n",
    "    all_documents = []\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
    "        standard_name = os.path.splitext(pdf_file)[0]\n",
    "        print(f\"Processing {pdf_file}...\")\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
    "        all_documents.extend(chunks)\n",
    "        print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
    "\n",
    "    if not all_documents:\n",
    "        print(\"No documents were extracted from PDFs. Cannot create vector database.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"Total chunks extracted: {len(all_documents)}\")\n",
    "    print(\"Creating/Recreating vector database safely...\")\n",
    "    client = safely_recreate_vector_database(all_documents) # This updates config.DB_DIRECTORY\n",
    "    print(f\"Vector database operations completed. Current DB directory: '{config.DB_DIRECTORY}'\")\n",
    "    return client\n",
    "\n",
    "# --- START OF NEW DEMONSTRATION CODE ---\n",
    "\"\"\"\n",
    "AAOIFI Standards Enhancement System Demo\n",
    "\n",
    "This script demonstrates how to use the multi-agent architecture to:\n",
    "1.  Select an AAOIFI standard\n",
    "2.  Review and extract key elements using ReviewAgent\n",
    "3.  Propose AI-driven modifications/enhancements using EnhancementAgent\n",
    "4.  Validate proposed changes based on Shariah principles using ValidationAgent\n",
    "5.  Generate a comprehensive report using FinalReportAgent\n",
    "6.  Optionally, use additional agents for visualization and feedback simulation.\n",
    "\"\"\"\n",
    "\n",
    "class DemoRunner:\n",
    "    \"\"\"Class to run a complete demonstration of the AAOIFI Standards Enhancement System.\"\"\"\n",
    "    \n",
    "    def __init__(self, system: AAOIFIStandardsEnhancementSystem):\n",
    "        self.system = system\n",
    "        self.selected_standard_name: Optional[str] = None\n",
    "        self.current_demo_results: Dict[str, Any] = {} # Stores results for the current demo run\n",
    "        \n",
    "    def _get_excerpt(self, text: Optional[str], max_length: int = 300) -> str:\n",
    "        \"\"\"Get an excerpt of text with maximum length.\"\"\"\n",
    "        if not text:\n",
    "            return \"N/A\"\n",
    "        text = str(text) # Ensure it's a string\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        return text[:max_length] + \"...\"\n",
    "\n",
    "    def list_and_select_standard(self) -> bool:\n",
    "        \"\"\"List all available standards and allow selection.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"AAOIFI STANDARDS ENHANCEMENT SYSTEM DEMONSTRATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\nAvailable standards:\")\n",
    "        standards = self.system.list_available_standards()\n",
    "        \n",
    "        if not standards or any(s.startswith(\"Error:\") or s.startswith(\"No standards found\") for s in standards):\n",
    "            print(\"No standards available or error listing standards.\")\n",
    "            print(\"Please ensure PDFs are processed and a vector database exists.\")\n",
    "            if standards: print(f\"Details: {standards[0]}\")\n",
    "            return False\n",
    "            \n",
    "        for i, standard_name in enumerate(standards):\n",
    "            print(f\"{i+1}. {standard_name}\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                choice_str = input(f\"\\nSelect a standard to process (enter number 1-{len(standards)}): \")\n",
    "                if not choice_str: continue # Handle empty input\n",
    "                choice = int(choice_str)\n",
    "                if 1 <= choice <= len(standards):\n",
    "                    self.selected_standard_name = standards[choice-1]\n",
    "                    print(f\"\\nSelected standard: {self.selected_standard_name}\")\n",
    "                    self.current_demo_results = {\"standard_name\": self.selected_standard_name} # Reset for new selection\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"Invalid selection. Please enter a number between 1 and {len(standards)}.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "    \n",
    "    def run_review_phase(self) -> bool:\n",
    "        \"\"\"Run the review phase and display results.\"\"\"\n",
    "        if not self.selected_standard_name:\n",
    "            print(\"Error: No standard selected for review phase.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 1: STANDARD REVIEW AND ANALYSIS (ReviewAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        print(f\"\\nRetrieving content for standard: {self.selected_standard_name}...\")\n",
    "        content = self.system.db_manager.get_standard_content(self.selected_standard_name)\n",
    "        if content.startswith(\"Error:\") or content.startswith(\"No content found\"):\n",
    "            print(f\"  Error retrieving content: {content}\")\n",
    "            self.current_demo_results[\"review_error\"] = content\n",
    "            return False\n",
    "        \n",
    "        standard_document = StandardDocument(name=self.selected_standard_name, content=content)\n",
    "        \n",
    "        print(f\"Reviewing standard using ReviewAgent...\")\n",
    "        start_time = time.time()\n",
    "        review_output = self.system.review_agent.execute(standard_document)\n",
    "        self.current_demo_results[\"review_output\"] = review_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"ReviewAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nREVIEW SUMMARY:\")\n",
    "        print(f\"- Standard: {review_output.get('standard_name', 'N/A')}\")\n",
    "        print(f\"\\nCore Principles (excerpt):\\n{self._get_excerpt(review_output.get('core_principles'))}\")\n",
    "        print(f\"\\nKey Definitions (excerpt):\\n{self._get_excerpt(review_output.get('key_definitions'))}\")\n",
    "        # print(f\"\\nFull Review Text (excerpt):\\n{self._get_excerpt(review_output.get('review_result'), 500)}\")\n",
    "        return True\n",
    "    \n",
    "    def run_enhancement_phase(self) -> bool:\n",
    "        \"\"\"Run the enhancement phase and display results.\"\"\"\n",
    "        if \"review_output\" not in self.current_demo_results:\n",
    "            print(\"Error: Review phase must complete successfully before enhancement.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 2: AI-DRIVEN ENHANCEMENT PROPOSALS (EnhancementAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        review_output = self.current_demo_results[\"review_output\"]\n",
    "        print(f\"\\nGenerating enhancement proposals for: {self.selected_standard_name} using EnhancementAgent...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        enhancement_output = self.system.enhancement_agent.execute(review_output) # Pass full review dict\n",
    "        self.current_demo_results[\"enhancement_output\"] = enhancement_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"EnhancementAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nENHANCEMENT PROPOSALS (excerpt):\")\n",
    "        print(self._get_excerpt(enhancement_output.get(\"enhancement_proposals\"), 500))\n",
    "        return True\n",
    "\n",
    "    def run_validation_phase(self) -> bool:\n",
    "        \"\"\"Run the validation phase and display results.\"\"\"\n",
    "        if \"enhancement_output\" not in self.current_demo_results or \\\n",
    "           \"review_output\" not in self.current_demo_results:\n",
    "            print(\"Error: Review and Enhancement phases must complete successfully before validation.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 3: SHARIAH COMPLIANCE VALIDATION (ValidationAgent)\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        review_output = self.current_demo_results[\"review_output\"]\n",
    "        enhancement_output = self.current_demo_results[\"enhancement_output\"]\n",
    "        \n",
    "        print(f\"\\nValidating proposed enhancements for: {self.selected_standard_name} using ValidationAgent...\")\n",
    "        start_time = time.time()\n",
    "        validation_output = self.system.validation_agent.execute(enhancement_output, review_output)\n",
    "        self.current_demo_results[\"validation_output\"] = validation_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"ValidationAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nVALIDATION RESULTS (excerpt):\")\n",
    "        print(self._get_excerpt(validation_output.get(\"validation_result\"), 500))\n",
    "        return True\n",
    "    \n",
    "    def run_report_generation_phase(self) -> bool:\n",
    "        \"\"\"Generate the final comprehensive report.\"\"\"\n",
    "        if \"validation_output\" not in self.current_demo_results: # Implies previous phases also ran\n",
    "            print(\"Error: All previous phases (Review, Enhance, Validate) must complete before report generation.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 4: COMPREHENSIVE REPORT GENERATION (FinalReportAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        print(f\"\\nGenerating comprehensive report for: {self.selected_standard_name} using FinalReportAgent...\")\n",
    "        \n",
    "        report_input_data = {\n",
    "            \"standard_name\": self.selected_standard_name,\n",
    "            \"review_text\": self.current_demo_results.get(\"review_output\", {}).get(\"review_result\", \"N/A\"),\n",
    "            \"enhancements_text\": self.current_demo_results.get(\"enhancement_output\", {}).get(\"enhancement_proposals\", \"N/A\"),\n",
    "            \"validation_text\": self.current_demo_results.get(\"validation_output\", {}).get(\"validation_result\", \"N/A\")\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        final_report_output = self.system.report_agent.execute(report_input_data)\n",
    "        self.current_demo_results[\"final_report_output\"] = final_report_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"FinalReportAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nFINAL REPORT (excerpt):\")\n",
    "        print(self._get_excerpt(final_report_output.get(\"final_report\"), 600))\n",
    "        return True\n",
    "\n",
    "    def save_demo_results(self):\n",
    "        \"\"\"Save all results from the demonstration.\"\"\"\n",
    "        if not self.current_demo_results or not self.selected_standard_name:\n",
    "            print(\"No results to save or standard not selected.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"SAVING DEMONSTRATION RESULTS\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Use the system's save_results method, passing the demo's collected results\n",
    "        self.system.save_results(self.current_demo_results) # It handles directory creation and naming\n",
    "    \n",
    "    def run_complete_demo(self):\n",
    "        \"\"\"Run the complete demonstration process.\"\"\"\n",
    "        if not self.list_and_select_standard():\n",
    "            print(\"Demo terminated: Standard selection failed or was aborted.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_review_phase():\n",
    "            print(\"Demo terminated: Review phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_enhancement_phase():\n",
    "            print(\"Demo terminated: Enhancement phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_validation_phase():\n",
    "            print(\"Demo terminated: Validation phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_report_generation_phase():\n",
    "            print(\"Demo terminated: Report generation phase failed.\")\n",
    "            return\n",
    "        \n",
    "        self.save_demo_results()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"DEMONSTRATION COMPLETED SUCCESSFULLY FOR STANDARD: {self.selected_standard_name}\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"All results and the final report have been saved to the '{config.OUTPUT_DIR}' directory.\")\n",
    "\n",
    "# --- Additional Agents for Enhanced Demo ---\n",
    "\n",
    "class VisualizationAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for generating textual descriptions for visualizations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"VisualizationAgent\",\n",
    "            description=\"Creates textual summaries suitable for generating visual representations of standard changes.\",\n",
    "            model_name=Config.GPT35_MODEL # Faster model for summarization\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a data visualization assistant. Your task is to process the provided summaries of AAOIFI standard reviews,\n",
    "        enhancement proposals, and validation results, and then generate concise textual descriptions that could be used\n",
    "        to create visual elements (like tables or charts).\n",
    "\n",
    "        Focus on:\n",
    "        1.  A summary table of key proposed changes: Section | Original Concept | Proposed Change | Benefit.\n",
    "        2.  A Shariah compliance assessment summary: Proposal Area | Compliance Status | Key Shariah Considerations.\n",
    "        3.  A benefits overview: Key Enhancement | Primary Benefit to Stakeholders.\n",
    "\n",
    "        Present this information clearly using Markdown tables or structured lists.\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_excerpt(self, text: Optional[str], max_length: int = 300) -> str: # Helper from DemoRunner\n",
    "        if not text: return \"N/A\"\n",
    "        text = str(text)\n",
    "        if len(text) <= max_length: return text\n",
    "        return text[:max_length] + \"...\"\n",
    "\n",
    "    def execute(self, demo_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        standard_name = demo_results.get('standard_name', 'N/A')\n",
    "        review_text = self._get_excerpt(demo_results.get('review_output', {}).get('review_result', 'N/A'), 1000)\n",
    "        enhancements_text = self._get_excerpt(demo_results.get('enhancement_output', {}).get('enhancement_proposals', 'N/A'), 1000)\n",
    "        validation_text = self._get_excerpt(demo_results.get('validation_output', {}).get('validation_result', 'N/A'), 1000)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {standard_name}\n",
    "            \n",
    "            Summary of Standard Review:\n",
    "            {review_text}\n",
    "            \n",
    "            Summary of Proposed Enhancements:\n",
    "            {enhancements_text}\n",
    "            \n",
    "            Summary of Validation Results:\n",
    "            {validation_text}\n",
    "            \n",
    "            Please generate textual descriptions suitable for visualization based on the above.\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        visualization_text_data = chain.run({})\n",
    "        \n",
    "        return {\n",
    "            \"standard_name\": standard_name,\n",
    "            \"visualization_text_data\": visualization_text_data\n",
    "        }\n",
    "\n",
    "class FeedbackAgent(BaseAgent):\n",
    "    \"\"\"Agent for collecting and analyzing simulated feedback on proposed changes.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FeedbackAgent\",\n",
    "            description=\"Processes simulated feedback on proposed standard changes and generates insights.\",\n",
    "            model_name=Config.GPT35_MODEL\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an expert in qualitative feedback analysis for financial standards.\n",
    "        Given a set of simulated feedback entries on proposed changes to an AAOIFI standard,\n",
    "        your task is to:\n",
    "\n",
    "        1.  Summarize the overall sentiment (Positive, Negative, Mixed).\n",
    "        2.  Identify 2-3 key themes or concerns raised by stakeholders.\n",
    "        3.  Highlight 2-3 constructive suggestions for further improvement, if any.\n",
    "        4.  Note any areas of strong consensus or significant disagreement.\n",
    "\n",
    "        Provide a concise analysis.\n",
    "        \"\"\"\n",
    "    \n",
    "    def execute(self, standard_name: str, simulated_feedback_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        formatted_feedback = \"\\n\\n\".join([\n",
    "            f\"Feedback Entry #{i+1}:\\nStakeholder Role: {item.get('role', 'N/A')}\\nRating: {item.get('rating', 'N/A')}/5\\nComment: {item.get('comment', 'N/A')}\"\n",
    "            for i, item in enumerate(simulated_feedback_list)\n",
    "        ])\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {standard_name}\n",
    "            \n",
    "            Simulated Stakeholder Feedback:\n",
    "            {formatted_feedback}\n",
    "            \n",
    "            Please analyze this feedback.\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        analysis_result = chain.run({})\n",
    "        \n",
    "        avg_rating = sum(item.get('rating', 0) for item in simulated_feedback_list) / len(simulated_feedback_list) if simulated_feedback_list else 0\n",
    "        \n",
    "        return {\n",
    "            \"standard_name\": standard_name,\n",
    "            \"feedback_count\": len(simulated_feedback_list),\n",
    "            \"average_simulated_rating\": f\"{avg_rating:.2f}/5.00\",\n",
    "            \"feedback_analysis_summary\": analysis_result\n",
    "        }\n",
    "\n",
    "class EnhancedDemoRunner(DemoRunner):\n",
    "    \"\"\"Enhanced demo runner with visualization and feedback capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, system: AAOIFIStandardsEnhancementSystem):\n",
    "        super().__init__(system)\n",
    "        self.visualization_agent = VisualizationAgent()\n",
    "        self.feedback_agent = FeedbackAgent()\n",
    "        \n",
    "    def run_visualization_phase(self) -> bool:\n",
    "        \"\"\"Generate textual descriptions for visualizations based on the results.\"\"\"\n",
    "        if not self.current_demo_results.get(\"final_report_output\"): # Check if prior phases are done\n",
    "            print(\"Error: Core demo phases must complete before visualization.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 5: GENERATING VISUALIZATION DATA (VisualizationAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        print(f\"\\nGenerating visualization text for: {self.selected_standard_name} using VisualizationAgent...\")\n",
    "        start_time = time.time()\n",
    "        visualization_output = self.visualization_agent.execute(self.current_demo_results)\n",
    "        self.current_demo_results[\"visualization_output\"] = visualization_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"VisualizationAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nVISUALIZATION TEXT DATA (excerpt):\")\n",
    "        print(self._get_excerpt(visualization_output.get(\"visualization_text_data\"), 500))\n",
    "        return True\n",
    "    \n",
    "    def run_simulated_feedback_phase(self) -> bool:\n",
    "        \"\"\"Simulate stakeholder feedback and analyze it.\"\"\"\n",
    "        if not self.current_demo_results.get(\"final_report_output\"):\n",
    "            print(\"Error: Core demo phases must complete before feedback simulation.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 6: SIMULATED FEEDBACK ANALYSIS (FeedbackAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        print(f\"\\nSimulating stakeholder feedback for: {self.selected_standard_name}...\")\n",
    "        simulated_feedback = [\n",
    "            {\"role\": \"Shariah Scholar\", \"rating\": 4, \"comment\": \"The proposals are largely compliant, but clarification is needed on the application of 'gharar yaseer' in proposed digital contracts.\"},\n",
    "            {\"role\": \"Banker\", \"rating\": 5, \"comment\": \"These changes will significantly improve operational efficiency and reduce ambiguity in sukuk issuance.\"},\n",
    "            {\"role\": \"Regulator\", \"rating\": 3, \"comment\": \"While the intent is good, the proposed technological integrations might pose supervisory challenges. We need more detailed risk management guidelines.\"},\n",
    "            {\"role\": \"Academic\", \"rating\": 4, \"comment\": \"A solid step forward. I suggest including references to contemporary research on fintech in Islamic finance for a more robust theoretical underpinning.\"}\n",
    "        ]\n",
    "        print(f\"Generated {len(simulated_feedback)} simulated feedback entries.\")\n",
    "\n",
    "        print(f\"Analyzing simulated feedback using FeedbackAgent...\")\n",
    "        start_time = time.time()\n",
    "        feedback_analysis_output = self.feedback_agent.execute(self.selected_standard_name, simulated_feedback)\n",
    "        self.current_demo_results[\"feedback_analysis_output\"] = feedback_analysis_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"FeedbackAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "        print(\"\\nSIMULATED FEEDBACK ANALYSIS (excerpt):\")\n",
    "        print(self._get_excerpt(feedback_analysis_output.get(\"feedback_analysis_summary\"), 500))\n",
    "        return True\n",
    "\n",
    "    def run_complete_enhanced_demo(self):\n",
    "        \"\"\"Run the complete enhanced demonstration process.\"\"\"\n",
    "        if not self.list_and_select_standard():\n",
    "            print(\"Enhanced Demo terminated: Standard selection failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_review_phase():\n",
    "            print(\"Enhanced Demo terminated: Review phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_enhancement_phase():\n",
    "            print(\"Enhanced Demo terminated: Enhancement phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_validation_phase():\n",
    "            print(\"Enhanced Demo terminated: Validation phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_report_generation_phase():\n",
    "            print(\"Enhanced Demo terminated: Report generation phase failed.\")\n",
    "            return\n",
    "        \n",
    "        # Enhanced Steps\n",
    "        if not self.run_visualization_phase():\n",
    "            print(\"Enhanced Demo warning: Visualization phase failed, but core demo completed.\")\n",
    "        \n",
    "        if not self.run_simulated_feedback_phase():\n",
    "            print(\"Enhanced Demo warning: Feedback simulation phase failed, but core demo completed.\")\n",
    "            \n",
    "        self.save_demo_results()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"ENHANCED DEMONSTRATION COMPLETED SUCCESSFULLY FOR STANDARD: {self.selected_standard_name}\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"All results, including enhanced analyses, have been saved to the '{config.OUTPUT_DIR}' directory.\")\n",
    "\n",
    "\n",
    "# Main execution block for the demo\n",
    "def main_demo():\n",
    "    \"\"\"Main function to run the demonstration.\"\"\"\n",
    "    print(\"Initializing AAOIFI Standards Enhancement System for Demo...\")\n",
    "    \n",
    "    # Step 1: Ensure Vector DB is populated (or try to populate it)\n",
    "    # Try to initialize system first. If it fails badly (e.g. DB dir issue), then try to process PDFs.\n",
    "    try:\n",
    "        system_check = AAOIFIStandardsEnhancementSystem()\n",
    "        standards_check = system_check.list_available_standards()\n",
    "        if not standards_check or any(s.startswith(\"Error:\") or s.startswith(\"No standards found\") for s in standards_check):\n",
    "            print(\"\\nNo standards readily available or error accessing existing DB.\")\n",
    "            run_pdf_processing = input(\"Attempt to process PDFs in 'pdf_eng' folder to create/recreate database? (yes/no): \").strip().lower()\n",
    "            if run_pdf_processing == 'yes':\n",
    "                print(\"Processing PDFs using safe recreate method...\")\n",
    "                process_pdfs_safe() # This updates config.DB_DIRECTORY\n",
    "                print(\"Re-initializing system with the new/updated database...\")\n",
    "                # System will be initialized below with the potentially new config.DB_DIRECTORY\n",
    "            else:\n",
    "                print(\"PDF processing skipped. Demo may not function if no standards are available.\")\n",
    "        else:\n",
    "            print(f\"Found existing standards: {standards_check[:3]}...\") # Print first few\n",
    "    except Exception as e:\n",
    "        print(f\"Initial system/DB check failed: {e}\")\n",
    "        run_pdf_processing = input(\"Attempt to process PDFs in 'pdf_eng' folder to create/recreate database? (yes/no): \").strip().lower()\n",
    "        if run_pdf_processing == 'yes':\n",
    "            print(\"Processing PDFs using safe recreate method...\")\n",
    "            process_pdfs_safe() # This updates config.DB_DIRECTORY\n",
    "            print(\"Re-initializing system with the new/updated database...\")\n",
    "        else:\n",
    "            print(\"PDF processing skipped. Demo cannot continue without a functional database.\")\n",
    "            return\n",
    "\n",
    "    # Initialize the main system for the demo (picks up current config.DB_DIRECTORY)\n",
    "    try:\n",
    "        system = AAOIFIStandardsEnhancementSystem()\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error initializing AAOIFIStandardsEnhancementSystem: {e}\")\n",
    "        print(\"Please check your database setup and OpenAI API key.\")\n",
    "        return\n",
    "\n",
    "    # Choose which demo to run\n",
    "    print(\"\\nSelect Demo Type:\")\n",
    "    print(\"1. Basic Demo (Review, Enhance, Validate, Report)\")\n",
    "    print(\"2. Enhanced Demo (includes Visualization and Feedback Analysis)\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice_str = input(\"Enter your choice (1 or 2, or 'exit'): \").strip()\n",
    "            if choice_str.lower() == 'exit':\n",
    "                print(\"Exiting demo.\")\n",
    "                break\n",
    "            if not choice_str: continue\n",
    "\n",
    "            choice = int(choice_str)\n",
    "            if choice == 1:\n",
    "                demo = DemoRunner(system)\n",
    "                demo.run_complete_demo()\n",
    "                break\n",
    "            elif choice == 2:\n",
    "                demo = EnhancedDemoRunner(system)\n",
    "                demo.run_complete_enhanced_demo()\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid selection. Please enter 1 or 2.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number (1 or 2) or 'exit'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during demo execution: {e}\")\n",
    "            # traceback.print_exc() # For debugging\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure OPENAI_API_KEY is set\n",
    "    if not os.environ.get(\"OPENAI_API_KEY\") or \"sk-proj-\" not in os.environ.get(\"OPENAI_API_KEY\"): # Basic check\n",
    "        print(\"Error: OPENAI_API_KEY is not set or appears invalid in the script.\")\n",
    "        print(\"Please set it near the top of the script (line 24 approx).\")\n",
    "        # You might want to exit here if the key is critical for all operations\n",
    "        # exit() # Uncomment if you want to force exit\n",
    "\n",
    "    # Check if pdf_eng folder exists and has PDFs, offer to run process_pdfs_safe if empty\n",
    "    pdf_folder = Config.PDF_FOLDER\n",
    "    if not os.path.exists(pdf_folder):\n",
    "        print(f\"PDF folder '{pdf_folder}' does not exist. Please create it and add AAOIFI standard PDFs.\")\n",
    "    elif not [f for f in os.listdir(pdf_folder) if f.lower().endswith('.pdf')]:\n",
    "        print(f\"PDF folder '{pdf_folder}' is empty. Please add AAOIFI standard PDFs to process.\")\n",
    "    \n",
    "    # Run the main demonstration logic\n",
    "    main_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f925d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc6d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f12df203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing AAOIFI Standards Enhancement System for Demo...\n",
      "Initializing VectorDBManager with database directory: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Successfully created PersistentClient for database at: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Successfully accessed/created collection: aaoifi_standards\n",
      "Found existing standards: ['AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9', 'AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR', 'AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean']...\n",
      "Initializing VectorDBManager with database directory: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Successfully created PersistentClient for database at: aaoifi_vector_db_20250510_171304_gqfbl3\n",
      "Successfully accessed/created collection: aaoifi_standards\n",
      "\n",
      "Select Demo Type:\n",
      "1. Basic Demo (Review, Enhance, Validate, Report)\n",
      "2. Enhanced Demo (includes Visualization and Feedback Analysis)\n",
      "\n",
      "==================================================\n",
      "AAOIFI STANDARDS ENHANCEMENT SYSTEM DEMONSTRATION\n",
      "==================================================\n",
      "\n",
      "Available standards:\n",
      "1. AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
      "2. AAOIFI-20th-SB-Conf.-Arabic-Agenda-V5LR\n",
      "3. AAOIFI-Conceptual-Framework-for-Financial-Reporting-Revised-2020-Final-clean\n",
      "4. AAOIFI-SB-Conf.-17-Booklet-A-Rev-5-LR\n",
      "5. AAOIFI-SB-Conf.-17-Final-Recommendations\n",
      "6. AAOIFI-WB-Conf.-13th-Ara-AgendaFinal-2\n",
      "7. Conceptual-Framework-for-Financial-Reporting-by-Islamic-Financial-Institutions\n",
      "8. FAS-1-General-Presentation-and-Disclosure-in-the-Financial-Statements-of-Islamic-Banks-and-Financial-Institutions\n",
      "9. FAS-1-General-Presentation-and-Disclosures-in-the-Financial-Statements-_-v7-clean-17-October-2022\n",
      "10. FAS-28-Murabaha-and-Other-Deferred-Payment-Sales-Formatted-2021-clean-1\n",
      "11. FAS-30-Impairment-Credit-Losses-and-Onerous-Commitments-Formatted-2021-clean-13-Nov-22-1\n",
      "12. FAS-31-Investment-Agency-Al-Wakala-Bi-Al-Istithmar-Final-format-2021-updated-clean-1\n",
      "13. FAS-32-Ijarah-Formatted-2021-clean-April-2023-1\n",
      "14. FAS-33-Investment-in-Sukuk-Shares-and-Similar-Instrutments-formatted-clean-updated-April-2023\n",
      "15. FAS-34-Financial-Reporting-for-Sukuk-holders-Final-Clean-updated-April-2023\n",
      "16. FAS-35-Risk-Reserve-Finalization-Formatted-2021-clean-16-Aug-2022\n",
      "17. FAS-36-First-Time-Adoption-of-AAOIFI-Financial-Accounting-Standards-Final-30-Nov-20-Clean\n",
      "18. FAS-37-Financial-reporting-by-Waqf-institutions-Final-15-December-2020-v2-clean\n",
      "19. FAS-38-Waad-Khiyar-and-Tahawwut-Final-15-December-2020-clean\n",
      "20. FAS-39-Financial-reporting-for-Zakah-clean-june-2022\n",
      "21. FAS-40-Financial-Reporting-for-Islamic-Windows\n",
      "22. FAS-41-Interim-Financial-Reporting-v16-final-for-issuance_clean\n",
      "23. FAS-42-Presentation-and-Disclosures-in-the-Financial-Statements-of-Takaful-Institutions-final-clean\n",
      "\n",
      "Selected standard: AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9\n",
      "\n",
      "--------------------------------------------------\n",
      "PHASE 1: STANDARD REVIEW AND ANALYSIS (ReviewAgent)\n",
      "--------------------------------------------------\n",
      "\n",
      "Retrieving content for standard: AAOIFI-19th-SB-Conf.-Arabic-Agenda-V9...\n",
      "Reviewing standard using ReviewAgent...\n",
      "An unexpected error occurred during demo execution: name 'time' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class StandardDocument:\n",
    "    \"\"\"Class to represent an AAOIFI standard document.\"\"\"\n",
    "    def __init__(self, name: str, content: str):\n",
    "        self.name = name\n",
    "        self.content = content\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"Base class for all agents in the system.\"\"\"\n",
    "    def __init__(self, name: str, description: str, model_name: str = Config.GPT4_MODEL):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.model_name = model_name\n",
    "        # Ensure API key is set before initializing ChatOpenAI\n",
    "        if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "            raise ValueError(\"OPENAI_API_KEY not set in environment.\")\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0.2)\n",
    "\n",
    "\n",
    "    def execute(self, input_data: Any) -> Any:\n",
    "        \"\"\"Execute the agent's task.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "class ReviewAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for reviewing standards and extracting key elements.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"ReviewAgent\",\n",
    "            description=\"Analyzes AAOIFI standards to extract key elements, principles, and requirements.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an expert in Islamic finance and AAOIFI standards. Your task is to carefully review\n",
    "        the provided standard document and extract the following key elements:\n",
    "\n",
    "        1. Core principles and objectives of the standard\n",
    "        2. Key definitions and terminology\n",
    "        3. Main requirements and procedures\n",
    "        4. Compliance criteria and guidelines\n",
    "        5. Practical implementation considerations\n",
    "\n",
    "        Organize your analysis in a structured format with these categories. Be thorough but concise\n",
    "        in your extraction of the essential components. Respond with the extracted information directly,\n",
    "        using markdown for headings for each category.\n",
    "        \"\"\"\n",
    "\n",
    "    def execute(self, standard: StandardDocument) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a standard document and extract its key elements.\n",
    "\n",
    "        Args:\n",
    "            standard: The standard document to analyze.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the extracted key elements.\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"Standard Name: {standard.name}\\n\\nContent:\\n{standard.content}\")\n",
    "        ])\n",
    "\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        # The chain.run({}) is problematic if the prompt itself needs variables.\n",
    "        # Here, standard.name and standard.content are already formatted into the HumanMessage.\n",
    "        # If the prompt template had input_variables, they would be passed to run().\n",
    "        result_text = chain.run({}) # Assuming the prompt is self-contained after formatting.\n",
    "\n",
    "        # Try to parse the result into structured data (simple approach)\n",
    "        parsed_result = {\n",
    "            \"standard_name\": standard.name,\n",
    "            \"review_result\": result_text, # This is the full text from LLM\n",
    "            \"core_principles\": self._extract_section(result_text, \"Core principles and objectives\"),\n",
    "            \"key_definitions\": self._extract_section(result_text, \"Key definitions and terminology\"),\n",
    "            \"main_requirements\": self._extract_section(result_text, \"Main requirements and procedures\"),\n",
    "            \"compliance_criteria\": self._extract_section(result_text, \"Compliance criteria and guidelines\"), # \"guidelines\" was missing\n",
    "            \"implementation_considerations\": self._extract_section(result_text, \"Practical implementation considerations\") # \"Practical\" was missing\n",
    "        }\n",
    "        return parsed_result\n",
    "\n",
    "\n",
    "    def _extract_section(self, text: str, section_name: str) -> str:\n",
    "        \"\"\"Helper method to extract specific sections from the review result using regex.\"\"\"\n",
    "        # Regex to find a heading (e.g., \"1. Section Name\" or \"Section Name:\") and capture text until the next heading or end of string\n",
    "        # This assumes headings are followed by a newline.\n",
    "        # It also makes section_name matching case-insensitive.\n",
    "        pattern = re.compile(\n",
    "            r\"(?i)(?:^\\s*\\d*\\.?\\s*|\\n\\s*\\d*\\.?\\s*)\" + re.escape(section_name) + r\"\\s*[:\\n](.*?)(?=\\n\\s*\\d*\\.?\\s*\\w+.*?\\s*[:\\n]|\\Z)\",\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        return f\"Section '{section_name}' not found or parsing error.\"\n",
    "\n",
    "\n",
    "class EnhancementAgent(BaseAgent):\n",
    "    \"\"\"Agent tasked with proposing modifications or enhancements to standards.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"EnhancementAgent\",\n",
    "            description=\"Proposes AI-driven modifications and enhancements to AAOIFI standards.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an AI expert specializing in Islamic finance and AAOIFI standards enhancement.\n",
    "        Your task is to propose thoughtful modifications and enhancements to the standard based\n",
    "        on the review provided.\n",
    "\n",
    "        Consider the following aspects in your proposals:\n",
    "\n",
    "        1. Clarity improvements: Suggest clearer language or better organization where appropriate\n",
    "        2. Modern context adaptations: Propose updates to address contemporary financial practices\n",
    "        3. Technological integration: Recommend ways to incorporate digital technologies and fintech\n",
    "        4. Cross-reference enhancements: Suggest improved links to related standards or principles\n",
    "        5. Practical implementation: Provide more actionable guidance for practitioners\n",
    "\n",
    "        For each suggestion, provide:\n",
    "        - The specific section or clause being enhanced (if applicable, otherwise general proposal)\n",
    "        - The current text or concept (if applicable, very brief summary)\n",
    "        - Your proposed modification or addition\n",
    "        - A brief justification explaining the benefit of your enhancement\n",
    "\n",
    "        Structure your response clearly, perhaps using bullet points for each proposal.\n",
    "        Ensure all suggestions maintain strict compliance with Shariah principles.\n",
    "        \"\"\"\n",
    "\n",
    "    def execute(self, review_result: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Propose enhancements to a standard based on the review.\n",
    "\n",
    "        Args:\n",
    "            review_result: The result from the ReviewAgent (the full dictionary).\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing proposed enhancements.\n",
    "        \"\"\"\n",
    "        # We need the text of the review, not the whole dict, for the prompt\n",
    "        review_text_summary = review_result.get(\"review_result\", \"No review summary available.\")\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"Standard Name: {review_result['standard_name']}\\n\\nSummary of Standard Review:\\n{review_text_summary}\")\n",
    "        ])\n",
    "\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        result_text = chain.run({})\n",
    "\n",
    "        return {\n",
    "            \"standard_name\": review_result[\"standard_name\"],\n",
    "            \"enhancement_proposals\": result_text # This is the LLM's textual output\n",
    "        }\n",
    "\n",
    "class ValidationAgent(BaseAgent):\n",
    "    \"\"\"Agent for validating and approving proposed changes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"ValidationAgent\",\n",
    "            description=\"Validates proposed changes based on Shariah compliance and practicality.\",\n",
    "            model_name=Config.GPT4_MODEL\n",
    "        )\n",
    "\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a senior Shariah scholar and AAOIFI standards expert. Your role is to validate\n",
    "        proposed enhancements to ensure they maintain compliance with Islamic principles and\n",
    "        practical applicability.\n",
    "\n",
    "        For each proposed enhancement, evaluate:\n",
    "\n",
    "        1. Shariah Compliance: Does the proposal align with Islamic principles and AAOIFI's mission?\n",
    "        2. Technical Accuracy: Is the proposed language precise and technically sound?\n",
    "        3. Practical Applicability: Can the enhancement be practically implemented by Islamic financial institutions?\n",
    "        4. Consistency: Does it maintain consistency with other standards and established practices?\n",
    "        5. Value Addition: Does it meaningfully improve the standard?\n",
    "\n",
    "        For each proposal, provide:\n",
    "        - Your assessment (e.g., Approved, Rejected, Needs Modification)\n",
    "        - A detailed justification for your decision, referencing the evaluation criteria above.\n",
    "        - Suggested refinements if \"Needs Modification\".\n",
    "\n",
    "        Be thorough in your analysis and maintain the highest standards of Islamic finance integrity.\n",
    "        Structure your response clearly, addressing each proposal from the input.\n",
    "        \"\"\"\n",
    "\n",
    "    def execute(self, enhancement_result: Dict[str, Any], original_review: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Validate proposed enhancements based on Shariah compliance and practicality.\n",
    "\n",
    "        Args:\n",
    "            enhancement_result: The result from the EnhancementAgent (dictionary).\n",
    "            original_review: The original review from the ReviewAgent (dictionary).\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing validation results.\n",
    "        \"\"\"\n",
    "        review_text_summary = original_review.get(\"review_result\", \"No review summary available.\")\n",
    "        enhancement_proposals_text = enhancement_result.get(\"enhancement_proposals\", \"No enhancement proposals available.\")\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {enhancement_result['standard_name']}\n",
    "\n",
    "            Original Standard Review Summary:\n",
    "            {review_text_summary}\n",
    "\n",
    "            Proposed Enhancements to Validate:\n",
    "            {enhancement_proposals_text}\n",
    "            \"\"\")\n",
    "        ])\n",
    "\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        result_text = chain.run({})\n",
    "\n",
    "        return {\n",
    "            \"standard_name\": enhancement_result[\"standard_name\"],\n",
    "            \"validation_result\": result_text # This is the LLM's textual output\n",
    "        }\n",
    "\n",
    "class FinalReportAgent(BaseAgent):\n",
    "    \"\"\"Agent for generating a comprehensive final report.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FinalReportAgent\",\n",
    "            description=\"Compiles all findings and recommendations into a comprehensive report.\",\n",
    "            model_name=Config.GPT4_MODEL # GPT-4 for high-quality report generation\n",
    "        )\n",
    "\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a professional report writer specializing in Islamic finance standards. Your task is to\n",
    "        compile all the findings, enhancements, and validations into a comprehensive, well-structured report\n",
    "        in Markdown format.\n",
    "\n",
    "        Your report should include the following sections:\n",
    "\n",
    "        1.  **Executive Summary**: Brief overview of the standard reviewed, key findings, summary of proposed changes, and validation outcomes.\n",
    "        2.  **Standard Overview**: Summary of the original standard's purpose and core components (based on the review).\n",
    "        3.  **Key Findings from Review**: Major elements and considerations identified during the initial review.\n",
    "        4.  **Proposed Enhancements**: Clear presentation of all proposed modifications.\n",
    "        5.  **Validation Results**: Summary of the validation process and outcomes for each proposal.\n",
    "        6.  **Consolidated Recommendations**: A final list of approved or modified enhancements.\n",
    "        7.  **Implementation Considerations**: Practical next steps for adopting approved changes.\n",
    "        8.  **Conclusion**: Final thoughts on the impact of the proposed enhancements.\n",
    "\n",
    "        Write in a professional, clear, and objective style appropriate for AAOIFI stakeholders and Islamic finance professionals.\n",
    "        Use Markdown for formatting (headings, lists, bolding).\n",
    "        \"\"\"\n",
    "\n",
    "    def execute(self, all_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive final report.\n",
    "\n",
    "        Args:\n",
    "            all_results: Combined results from all previous agents.\n",
    "                         Expected keys: 'standard_name', 'review_text',\n",
    "                                        'enhancements_text', 'validation_text'.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the final report text.\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {all_results['standard_name']}\n",
    "\n",
    "            Full Text of Standard Review:\n",
    "            {all_results['review_text']}\n",
    "\n",
    "            Full Text of Proposed Enhancements:\n",
    "            {all_results['enhancements_text']}\n",
    "\n",
    "            Full Text of Validation of Enhancements:\n",
    "            {all_results['validation_text']}\n",
    "            \"\"\")\n",
    "        ])\n",
    "\n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        report_text = chain.run({})\n",
    "\n",
    "        return {\n",
    "            \"standard_name\": all_results[\"standard_name\"],\n",
    "            \"final_report\": report_text # This is the LLM's textual output (Markdown)\n",
    "        }\n",
    "\n",
    "\n",
    "class VectorDBManager:\n",
    "    \"\"\"Manager for interacting with the vector database.\"\"\"\n",
    "\n",
    "    def __init__(self, db_directory: str = None, collection_name: str = config.COLLECTION_NAME):\n",
    "        self.db_directory = db_directory if db_directory else config.DB_DIRECTORY\n",
    "        self.collection_name = collection_name\n",
    "        self.embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "\n",
    "        print(f\"Initializing VectorDBManager with database directory: {self.db_directory}\")\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(self.db_directory):\n",
    "                print(f\"Database directory {self.db_directory} does not exist. Creating it.\")\n",
    "                os.makedirs(self.db_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.db_directory)\n",
    "            print(f\"Successfully created PersistentClient for database at: {self.db_directory}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fatal error creating PersistentClient for {self.db_directory}: {str(e)}\")\n",
    "            self.client = None\n",
    "            self.collection = None\n",
    "            raise  # Reraise critical error\n",
    "\n",
    "        if self.client:\n",
    "            try:\n",
    "                # Try to get the collection; if it fails, try to create it.\n",
    "                self.collection = self.client.get_or_create_collection(name=self.collection_name)\n",
    "                print(f\"Successfully accessed/created collection: {self.collection_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing/creating collection '{self.collection_name}': {str(e)}\")\n",
    "                self.collection = None\n",
    "        else:\n",
    "            self.collection = None\n",
    "\n",
    "\n",
    "    def get_standard_content(self, standard_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the content for a specific standard.\n",
    "\n",
    "        Args:\n",
    "            standard_name: The name of the standard to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            The combined content of the standard, or an error message.\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            return \"Error: Vector database collection not properly initialized.\"\n",
    "\n",
    "        try:\n",
    "            # Query for all chunks belonging to this standard\n",
    "            # A large n_results is needed if a standard has many chunks\n",
    "            # Chroma's default n_results is 10.\n",
    "            # To get ALL documents matching a filter, it's better to use collection.get()\n",
    "            results = self.collection.get(\n",
    "                where={\"source\": standard_name},\n",
    "                include=[\"documents\"] # Only fetch documents\n",
    "            )\n",
    "\n",
    "            if results and results['documents']:\n",
    "                # Sort by chunk_id if available in metadata, though .get() doesn't easily allow sorting\n",
    "                # For now, just join them. If order is critical, store chunk_id and sort post-retrieval.\n",
    "                # The current split_text_into_chunks adds metadata, but .get() returns raw docs.\n",
    "                # Let's assume for now that the order from .get() is sufficient or re-query with sort if needed.\n",
    "                # The current query in `create_vector_database` uses IDs like `doc_{i+j}` which are sequential.\n",
    "                # If we rely on implicit order of `get`, it should be mostly fine.\n",
    "                \n",
    "                # The metadata is not directly available here to sort by chunk_id without another call or more complex query\n",
    "                # For simplicity, joining in received order.\n",
    "                standard_content = \"\\n\\n\".join(doc for doc in results['documents'] if doc)\n",
    "                if not standard_content:\n",
    "                    return f\"No document content found for standard: {standard_name}, though metadatas might exist.\"\n",
    "                return standard_content\n",
    "            else:\n",
    "                return f\"No content found for standard: {standard_name}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error querying for standard {standard_name}: {str(e)}\"\n",
    "\n",
    "    def list_available_standards(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        List all available standards in the database.\n",
    "\n",
    "        Returns:\n",
    "            A list of standard names, or an error message list.\n",
    "        \"\"\"\n",
    "        if not self.collection:\n",
    "            return [\"Error: Vector database collection not properly initialized.\"]\n",
    "\n",
    "        try:\n",
    "            results = self.collection.get(include=[\"metadatas\"]) # Fetch only metadatas\n",
    "            if results and results['metadatas']:\n",
    "                standards = set()\n",
    "                for metadata in results['metadatas']:\n",
    "                    if metadata and 'source' in metadata: # Check if metadata is not None\n",
    "                        standards.add(metadata['source'])\n",
    "                if not standards:\n",
    "                    return [\"No standards found in the database. Metadatas might be empty or lack 'source'.\"]\n",
    "                return sorted(list(standards))\n",
    "            else:\n",
    "                return [\"No standards found in the database. Please process PDFs first.\"]\n",
    "        except Exception as e:\n",
    "            return [f\"Error listing standards: {str(e)}\"]\n",
    "\n",
    "\n",
    "class AAOIFIStandardsEnhancementSystem:\n",
    "    \"\"\"Main system coordinating the multi-agent process.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize vector database manager\n",
    "        # It will use config.DB_DIRECTORY which might be updated by process_pdfs_safe\n",
    "        self.db_manager = VectorDBManager()\n",
    "\n",
    "        # Initialize agents\n",
    "        self.review_agent = ReviewAgent()\n",
    "        self.enhancement_agent = EnhancementAgent()\n",
    "        self.validation_agent = ValidationAgent()\n",
    "        self.report_agent = FinalReportAgent()\n",
    "\n",
    "        # Track processing results for a single standard run\n",
    "        self.current_processing_results = {}\n",
    "\n",
    "    def list_available_standards(self) -> List[str]:\n",
    "        \"\"\"List all available standards in the system.\"\"\"\n",
    "        return self.db_manager.list_available_standards()\n",
    "\n",
    "    def process_standard(self, standard_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a single standard through the complete pipeline.\n",
    "\n",
    "        Args:\n",
    "            standard_name: The name of the standard to process.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the final results for this standard.\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing standard: {standard_name}\")\n",
    "        self.current_processing_results = {\"standard_name\": standard_name}\n",
    "\n",
    "        # Step 1: Get standard content from the vector database\n",
    "        print(\"  Retrieving standard content...\")\n",
    "        content = self.db_manager.get_standard_content(standard_name)\n",
    "        if content.startswith(\"Error:\") or content.startswith(\"No content found\"):\n",
    "            print(f\"  Error retrieving content for {standard_name}: {content}\")\n",
    "            self.current_processing_results[\"error\"] = content\n",
    "            return self.current_processing_results\n",
    "        \n",
    "        standard_doc = StandardDocument(name=standard_name, content=content)\n",
    "        self.current_processing_results[\"original_content_excerpt\"] = content[:500] + \"...\" if len(content) > 500 else content\n",
    "\n",
    "        # Step 2: Review and extract key elements\n",
    "        print(\"  Reviewing standard and extracting key elements...\")\n",
    "        review_output = self.review_agent.execute(standard_doc)\n",
    "        self.current_processing_results[\"review_output\"] = review_output # Store the whole dict\n",
    "\n",
    "        # Step 3: Propose enhancements\n",
    "        print(\"  Proposing enhancements...\")\n",
    "        enhancement_output = self.enhancement_agent.execute(review_output) # Pass the dict\n",
    "        self.current_processing_results[\"enhancement_output\"] = enhancement_output\n",
    "\n",
    "        # Step 4: Validate proposed changes\n",
    "        print(\"  Validating proposed changes...\")\n",
    "        # Validation agent needs enhancement_output (dict) and review_output (dict)\n",
    "        validation_output = self.validation_agent.execute(enhancement_output, review_output)\n",
    "        self.current_processing_results[\"validation_output\"] = validation_output\n",
    "\n",
    "        # Step 5: Generate final report\n",
    "        print(\"  Generating final report...\")\n",
    "        # Prepare data for report agent\n",
    "        report_input_data = {\n",
    "            \"standard_name\": standard_name,\n",
    "            \"review_text\": review_output.get(\"review_result\", \"N/A\"),\n",
    "            \"enhancements_text\": enhancement_output.get(\"enhancement_proposals\", \"N/A\"),\n",
    "            \"validation_text\": validation_output.get(\"validation_result\", \"N/A\")\n",
    "        }\n",
    "        final_report_output = self.report_agent.execute(report_input_data)\n",
    "        self.current_processing_results[\"final_report_output\"] = final_report_output\n",
    "\n",
    "        print(f\"Processing completed for standard: {standard_name}\")\n",
    "        return self.current_processing_results\n",
    "\n",
    "    def save_results(self, results_to_save: Dict[str, Any], output_dir: str = config.OUTPUT_DIR):\n",
    "        \"\"\"Save all results to output files.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        if not results_to_save:\n",
    "            print(\"No results to save.\")\n",
    "            return\n",
    "\n",
    "        standard_name = results_to_save.get(\"standard_name\", \"unknown_standard\")\n",
    "        # Sanitize standard_name for filename\n",
    "        safe_standard_name = re.sub(r'[^\\w\\-_\\. ]', '_', standard_name)\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        filename_json = os.path.join(output_dir, f\"{safe_standard_name}_full_results_{timestamp}.json\")\n",
    "        with open(filename_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_to_save, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Full results saved to {filename_json}\")\n",
    "\n",
    "        # Also save the final report separately as Markdown\n",
    "        final_report_text = results_to_save.get(\"final_report_output\", {}).get(\"final_report\")\n",
    "        if final_report_text:\n",
    "            filename_md = os.path.join(output_dir, f\"{safe_standard_name}_final_report_{timestamp}.md\")\n",
    "            with open(filename_md, 'w', encoding='utf-8') as f:\n",
    "                f.write(final_report_text)\n",
    "            print(f\"Final report saved to {filename_md}\")\n",
    "\n",
    "\n",
    "# --- Functions for recreating vector database safely ---\n",
    "def safely_recreate_vector_database(documents):\n",
    "    \"\"\"Create a new vector database from document chunks with proper handling of locked files.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=config.EMBEDDING_MODEL)\n",
    "\n",
    "    # Create a new directory with a unique name to avoid conflicts\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))\n",
    "    # Base directory for all DB versions\n",
    "    base_db_parent_dir = \"aaoifi_vector_db_versions\"\n",
    "    os.makedirs(base_db_parent_dir, exist_ok=True)\n",
    "    new_db_directory = os.path.join(base_db_parent_dir, f\"db_{timestamp}_{random_str}\")\n",
    "\n",
    "    print(f\"Creating new database in directory: {new_db_directory}\")\n",
    "    os.makedirs(new_db_directory, exist_ok=True)\n",
    "\n",
    "    client = chromadb.PersistentClient(path=new_db_directory)\n",
    "    # Always create a new collection in a new DB path\n",
    "    collection = client.create_collection(name=config.COLLECTION_NAME)\n",
    "    \n",
    "    batch_size = 100 # As used in create_vector_database\n",
    "    num_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_documents = documents[i:i+batch_size]\n",
    "        current_batch_num = (i // batch_size) + 1\n",
    "        print(f\"Processing batch {current_batch_num}/{num_batches} for new vector database...\")\n",
    "\n",
    "        texts = [doc[\"content\"] for doc in batch_documents]\n",
    "        # Use more robust IDs tied to document source and chunk\n",
    "        ids = [f\"{doc['metadata']['source']}_chunk_{doc['metadata']['chunk_id']}\" for doc in batch_documents]\n",
    "        # Check for ID uniqueness within the batch (essential for Chroma)\n",
    "        if len(ids) != len(set(ids)):\n",
    "            print(f\"Warning: Duplicate IDs generated in batch {current_batch_num}. This may cause issues.\")\n",
    "            # Simple fix: append index within batch to ensure uniqueness for this add op\n",
    "            ids = [f\"{id_}_{j}\" for j, id_ in enumerate(ids)]\n",
    "\n",
    "        metadatas = [doc[\"metadata\"] for doc in batch_documents]\n",
    "\n",
    "        if not texts:\n",
    "            print(f\"Skipping empty batch {current_batch_num}.\")\n",
    "            continue\n",
    "        try:\n",
    "            embeds = embeddings.embed_documents(texts)\n",
    "            collection.add(embeddings=embeds, documents=texts, ids=ids, metadatas=metadatas)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {current_batch_num} starting at index {i}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Created new vector database with consistent embedding dimensions in '{new_db_directory}'\")\n",
    "    \n",
    "    # Update the global config to point to the new directory for the current session\n",
    "    config.DB_DIRECTORY = new_db_directory\n",
    "    print(f\"Updated global config.DB_DIRECTORY to: {config.DB_DIRECTORY}\")\n",
    "    \n",
    "    return client\n",
    "\n",
    "def process_pdfs_safe():\n",
    "    \"\"\"Main function to process PDFs and create vector database safely.\"\"\"\n",
    "    PDF_FOLDER = config.PDF_FOLDER\n",
    "    if not os.path.exists(PDF_FOLDER):\n",
    "        print(f\"Error: The PDF folder '{PDF_FOLDER}' does not exist.\")\n",
    "        return None\n",
    "    if not os.listdir(PDF_FOLDER):\n",
    "        print(f\"The PDF folder '{PDF_FOLDER}' is empty. Please add PDF files to process.\")\n",
    "        return None\n",
    "\n",
    "    all_documents = []\n",
    "    pdf_files = [f for f in os.listdir(PDF_FOLDER) if f.lower().endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        print(f\"No PDF files found in '{PDF_FOLDER}'.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files.\")\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(PDF_FOLDER, pdf_file)\n",
    "        standard_name = os.path.splitext(pdf_file)[0]\n",
    "        print(f\"Processing {pdf_file}...\")\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        chunks = split_text_into_chunks(cleaned_text, standard_name)\n",
    "        all_documents.extend(chunks)\n",
    "        print(f\"Extracted {len(chunks)} chunks from {pdf_file}\")\n",
    "\n",
    "    if not all_documents:\n",
    "        print(\"No documents were extracted from PDFs. Cannot create vector database.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"Total chunks extracted: {len(all_documents)}\")\n",
    "    print(\"Creating/Recreating vector database safely...\")\n",
    "    client = safely_recreate_vector_database(all_documents) # This updates config.DB_DIRECTORY\n",
    "    print(f\"Vector database operations completed. Current DB directory: '{config.DB_DIRECTORY}'\")\n",
    "    return client\n",
    "\n",
    "# --- START OF NEW DEMONSTRATION CODE ---\n",
    "\"\"\"\n",
    "AAOIFI Standards Enhancement System Demo\n",
    "\n",
    "This script demonstrates how to use the multi-agent architecture to:\n",
    "1.  Select an AAOIFI standard\n",
    "2.  Review and extract key elements using ReviewAgent\n",
    "3.  Propose AI-driven modifications/enhancements using EnhancementAgent\n",
    "4.  Validate proposed changes based on Shariah principles using ValidationAgent\n",
    "5.  Generate a comprehensive report using FinalReportAgent\n",
    "6.  Optionally, use additional agents for visualization and feedback simulation.\n",
    "\"\"\"\n",
    "\n",
    "class DemoRunner:\n",
    "    \"\"\"Class to run a complete demonstration of the AAOIFI Standards Enhancement System.\"\"\"\n",
    "    \n",
    "    def __init__(self, system: AAOIFIStandardsEnhancementSystem):\n",
    "        self.system = system\n",
    "        self.selected_standard_name: Optional[str] = None\n",
    "        self.current_demo_results: Dict[str, Any] = {} # Stores results for the current demo run\n",
    "        \n",
    "    def _get_excerpt(self, text: Optional[str], max_length: int = 300) -> str:\n",
    "        \"\"\"Get an excerpt of text with maximum length.\"\"\"\n",
    "        if not text:\n",
    "            return \"N/A\"\n",
    "        text = str(text) # Ensure it's a string\n",
    "        if len(text) <= max_length:\n",
    "            return text\n",
    "        return text[:max_length] + \"...\"\n",
    "\n",
    "    def list_and_select_standard(self) -> bool:\n",
    "        \"\"\"List all available standards and allow selection.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"AAOIFI STANDARDS ENHANCEMENT SYSTEM DEMONSTRATION\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\nAvailable standards:\")\n",
    "        standards = self.system.list_available_standards()\n",
    "        \n",
    "        if not standards or any(s.startswith(\"Error:\") or s.startswith(\"No standards found\") for s in standards):\n",
    "            print(\"No standards available or error listing standards.\")\n",
    "            print(\"Please ensure PDFs are processed and a vector database exists.\")\n",
    "            if standards: print(f\"Details: {standards[0]}\")\n",
    "            return False\n",
    "            \n",
    "        for i, standard_name in enumerate(standards):\n",
    "            print(f\"{i+1}. {standard_name}\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                choice_str = input(f\"\\nSelect a standard to process (enter number 1-{len(standards)}): \")\n",
    "                if not choice_str: continue # Handle empty input\n",
    "                choice = int(choice_str)\n",
    "                if 1 <= choice <= len(standards):\n",
    "                    self.selected_standard_name = standards[choice-1]\n",
    "                    print(f\"\\nSelected standard: {self.selected_standard_name}\")\n",
    "                    self.current_demo_results = {\"standard_name\": self.selected_standard_name} # Reset for new selection\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"Invalid selection. Please enter a number between 1 and {len(standards)}.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "    \n",
    "    def run_review_phase(self) -> bool:\n",
    "        \"\"\"Run the review phase and display results.\"\"\"\n",
    "        if not self.selected_standard_name:\n",
    "            print(\"Error: No standard selected for review phase.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 1: STANDARD REVIEW AND ANALYSIS (ReviewAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        print(f\"\\nRetrieving content for standard: {self.selected_standard_name}...\")\n",
    "        content = self.system.db_manager.get_standard_content(self.selected_standard_name)\n",
    "        if content.startswith(\"Error:\") or content.startswith(\"No content found\"):\n",
    "            print(f\"  Error retrieving content: {content}\")\n",
    "            self.current_demo_results[\"review_error\"] = content\n",
    "            return False\n",
    "        \n",
    "        standard_document = StandardDocument(name=self.selected_standard_name, content=content)\n",
    "        \n",
    "        print(f\"Reviewing standard using ReviewAgent...\")\n",
    "        start_time = time.time()\n",
    "        review_output = self.system.review_agent.execute(standard_document)\n",
    "        self.current_demo_results[\"review_output\"] = review_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"ReviewAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nREVIEW SUMMARY:\")\n",
    "        print(f\"- Standard: {review_output.get('standard_name', 'N/A')}\")\n",
    "        print(f\"\\nCore Principles (excerpt):\\n{self._get_excerpt(review_output.get('core_principles'))}\")\n",
    "        print(f\"\\nKey Definitions (excerpt):\\n{self._get_excerpt(review_output.get('key_definitions'))}\")\n",
    "        # print(f\"\\nFull Review Text (excerpt):\\n{self._get_excerpt(review_output.get('review_result'), 500)}\")\n",
    "        return True\n",
    "    \n",
    "    def run_enhancement_phase(self) -> bool:\n",
    "        \"\"\"Run the enhancement phase and display results.\"\"\"\n",
    "        if \"review_output\" not in self.current_demo_results:\n",
    "            print(\"Error: Review phase must complete successfully before enhancement.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 2: AI-DRIVEN ENHANCEMENT PROPOSALS (EnhancementAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        review_output = self.current_demo_results[\"review_output\"]\n",
    "        print(f\"\\nGenerating enhancement proposals for: {self.selected_standard_name} using EnhancementAgent...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        enhancement_output = self.system.enhancement_agent.execute(review_output) # Pass full review dict\n",
    "        self.current_demo_results[\"enhancement_output\"] = enhancement_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"EnhancementAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nENHANCEMENT PROPOSALS (excerpt):\")\n",
    "        print(self._get_excerpt(enhancement_output.get(\"enhancement_proposals\"), 500))\n",
    "        return True\n",
    "\n",
    "    def run_validation_phase(self) -> bool:\n",
    "        \"\"\"Run the validation phase and display results.\"\"\"\n",
    "        if \"enhancement_output\" not in self.current_demo_results or \\\n",
    "           \"review_output\" not in self.current_demo_results:\n",
    "            print(\"Error: Review and Enhancement phases must complete successfully before validation.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 3: SHARIAH COMPLIANCE VALIDATION (ValidationAgent)\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        review_output = self.current_demo_results[\"review_output\"]\n",
    "        enhancement_output = self.current_demo_results[\"enhancement_output\"]\n",
    "        \n",
    "        print(f\"\\nValidating proposed enhancements for: {self.selected_standard_name} using ValidationAgent...\")\n",
    "        start_time = time.time()\n",
    "        validation_output = self.system.validation_agent.execute(enhancement_output, review_output)\n",
    "        self.current_demo_results[\"validation_output\"] = validation_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"ValidationAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nVALIDATION RESULTS (excerpt):\")\n",
    "        print(self._get_excerpt(validation_output.get(\"validation_result\"), 500))\n",
    "        return True\n",
    "    \n",
    "    def run_report_generation_phase(self) -> bool:\n",
    "        \"\"\"Generate the final comprehensive report.\"\"\"\n",
    "        if \"validation_output\" not in self.current_demo_results: # Implies previous phases also ran\n",
    "            print(\"Error: All previous phases (Review, Enhance, Validate) must complete before report generation.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 4: COMPREHENSIVE REPORT GENERATION (FinalReportAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        print(f\"\\nGenerating comprehensive report for: {self.selected_standard_name} using FinalReportAgent...\")\n",
    "        \n",
    "        report_input_data = {\n",
    "            \"standard_name\": self.selected_standard_name,\n",
    "            \"review_text\": self.current_demo_results.get(\"review_output\", {}).get(\"review_result\", \"N/A\"),\n",
    "            \"enhancements_text\": self.current_demo_results.get(\"enhancement_output\", {}).get(\"enhancement_proposals\", \"N/A\"),\n",
    "            \"validation_text\": self.current_demo_results.get(\"validation_output\", {}).get(\"validation_result\", \"N/A\")\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        final_report_output = self.system.report_agent.execute(report_input_data)\n",
    "        self.current_demo_results[\"final_report_output\"] = final_report_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"FinalReportAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nFINAL REPORT (excerpt):\")\n",
    "        print(self._get_excerpt(final_report_output.get(\"final_report\"), 600))\n",
    "        return True\n",
    "\n",
    "    def save_demo_results(self):\n",
    "        \"\"\"Save all results from the demonstration.\"\"\"\n",
    "        if not self.current_demo_results or not self.selected_standard_name:\n",
    "            print(\"No results to save or standard not selected.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"SAVING DEMONSTRATION RESULTS\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        # Use the system's save_results method, passing the demo's collected results\n",
    "        self.system.save_results(self.current_demo_results) # It handles directory creation and naming\n",
    "    \n",
    "    def run_complete_demo(self):\n",
    "        \"\"\"Run the complete demonstration process.\"\"\"\n",
    "        if not self.list_and_select_standard():\n",
    "            print(\"Demo terminated: Standard selection failed or was aborted.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_review_phase():\n",
    "            print(\"Demo terminated: Review phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_enhancement_phase():\n",
    "            print(\"Demo terminated: Enhancement phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_validation_phase():\n",
    "            print(\"Demo terminated: Validation phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_report_generation_phase():\n",
    "            print(\"Demo terminated: Report generation phase failed.\")\n",
    "            return\n",
    "        \n",
    "        self.save_demo_results()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"DEMONSTRATION COMPLETED SUCCESSFULLY FOR STANDARD: {self.selected_standard_name}\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"All results and the final report have been saved to the '{config.OUTPUT_DIR}' directory.\")\n",
    "\n",
    "# --- Additional Agents for Enhanced Demo ---\n",
    "\n",
    "class VisualizationAgent(BaseAgent):\n",
    "    \"\"\"Agent responsible for generating textual descriptions for visualizations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"VisualizationAgent\",\n",
    "            description=\"Creates textual summaries suitable for generating visual representations of standard changes.\",\n",
    "            model_name=Config.GPT35_MODEL # Faster model for summarization\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a data visualization assistant. Your task is to process the provided summaries of AAOIFI standard reviews,\n",
    "        enhancement proposals, and validation results, and then generate concise textual descriptions that could be used\n",
    "        to create visual elements (like tables or charts).\n",
    "\n",
    "        Focus on:\n",
    "        1.  A summary table of key proposed changes: Section | Original Concept | Proposed Change | Benefit.\n",
    "        2.  A Shariah compliance assessment summary: Proposal Area | Compliance Status | Key Shariah Considerations.\n",
    "        3.  A benefits overview: Key Enhancement | Primary Benefit to Stakeholders.\n",
    "\n",
    "        Present this information clearly using Markdown tables or structured lists.\n",
    "        \"\"\"\n",
    "    \n",
    "    def _get_excerpt(self, text: Optional[str], max_length: int = 300) -> str: # Helper from DemoRunner\n",
    "        if not text: return \"N/A\"\n",
    "        text = str(text)\n",
    "        if len(text) <= max_length: return text\n",
    "        return text[:max_length] + \"...\"\n",
    "\n",
    "    def execute(self, demo_results: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        standard_name = demo_results.get('standard_name', 'N/A')\n",
    "        review_text = self._get_excerpt(demo_results.get('review_output', {}).get('review_result', 'N/A'), 1000)\n",
    "        enhancements_text = self._get_excerpt(demo_results.get('enhancement_output', {}).get('enhancement_proposals', 'N/A'), 1000)\n",
    "        validation_text = self._get_excerpt(demo_results.get('validation_output', {}).get('validation_result', 'N/A'), 1000)\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {standard_name}\n",
    "            \n",
    "            Summary of Standard Review:\n",
    "            {review_text}\n",
    "            \n",
    "            Summary of Proposed Enhancements:\n",
    "            {enhancements_text}\n",
    "            \n",
    "            Summary of Validation Results:\n",
    "            {validation_text}\n",
    "            \n",
    "            Please generate textual descriptions suitable for visualization based on the above.\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        visualization_text_data = chain.run({})\n",
    "        \n",
    "        return {\n",
    "            \"standard_name\": standard_name,\n",
    "            \"visualization_text_data\": visualization_text_data\n",
    "        }\n",
    "\n",
    "class FeedbackAgent(BaseAgent):\n",
    "    \"\"\"Agent for collecting and analyzing simulated feedback on proposed changes.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"FeedbackAgent\",\n",
    "            description=\"Processes simulated feedback on proposed standard changes and generates insights.\",\n",
    "            model_name=Config.GPT35_MODEL\n",
    "        )\n",
    "        \n",
    "        self.system_prompt = \"\"\"\n",
    "        You are an expert in qualitative feedback analysis for financial standards.\n",
    "        Given a set of simulated feedback entries on proposed changes to an AAOIFI standard,\n",
    "        your task is to:\n",
    "\n",
    "        1.  Summarize the overall sentiment (Positive, Negative, Mixed).\n",
    "        2.  Identify 2-3 key themes or concerns raised by stakeholders.\n",
    "        3.  Highlight 2-3 constructive suggestions for further improvement, if any.\n",
    "        4.  Note any areas of strong consensus or significant disagreement.\n",
    "\n",
    "        Provide a concise analysis.\n",
    "        \"\"\"\n",
    "    \n",
    "    def execute(self, standard_name: str, simulated_feedback_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        formatted_feedback = \"\\n\\n\".join([\n",
    "            f\"Feedback Entry #{i+1}:\\nStakeholder Role: {item.get('role', 'N/A')}\\nRating: {item.get('rating', 'N/A')}/5\\nComment: {item.get('comment', 'N/A')}\"\n",
    "            for i, item in enumerate(simulated_feedback_list)\n",
    "        ])\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=self.system_prompt),\n",
    "            HumanMessage(content=f\"\"\"\n",
    "            Standard Name: {standard_name}\n",
    "            \n",
    "            Simulated Stakeholder Feedback:\n",
    "            {formatted_feedback}\n",
    "            \n",
    "            Please analyze this feedback.\n",
    "            \"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = LLMChain(llm=self.llm, prompt=prompt)\n",
    "        analysis_result = chain.run({})\n",
    "        \n",
    "        avg_rating = sum(item.get('rating', 0) for item in simulated_feedback_list) / len(simulated_feedback_list) if simulated_feedback_list else 0\n",
    "        \n",
    "        return {\n",
    "            \"standard_name\": standard_name,\n",
    "            \"feedback_count\": len(simulated_feedback_list),\n",
    "            \"average_simulated_rating\": f\"{avg_rating:.2f}/5.00\",\n",
    "            \"feedback_analysis_summary\": analysis_result\n",
    "        }\n",
    "\n",
    "class EnhancedDemoRunner(DemoRunner):\n",
    "    \"\"\"Enhanced demo runner with visualization and feedback capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, system: AAOIFIStandardsEnhancementSystem):\n",
    "        super().__init__(system)\n",
    "        self.visualization_agent = VisualizationAgent()\n",
    "        self.feedback_agent = FeedbackAgent()\n",
    "        \n",
    "    def run_visualization_phase(self) -> bool:\n",
    "        \"\"\"Generate textual descriptions for visualizations based on the results.\"\"\"\n",
    "        if not self.current_demo_results.get(\"final_report_output\"): # Check if prior phases are done\n",
    "            print(\"Error: Core demo phases must complete before visualization.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 5: GENERATING VISUALIZATION DATA (VisualizationAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        print(f\"\\nGenerating visualization text for: {self.selected_standard_name} using VisualizationAgent...\")\n",
    "        start_time = time.time()\n",
    "        visualization_output = self.visualization_agent.execute(self.current_demo_results)\n",
    "        self.current_demo_results[\"visualization_output\"] = visualization_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"VisualizationAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        print(\"\\nVISUALIZATION TEXT DATA (excerpt):\")\n",
    "        print(self._get_excerpt(visualization_output.get(\"visualization_text_data\"), 500))\n",
    "        return True\n",
    "    \n",
    "    def run_simulated_feedback_phase(self) -> bool:\n",
    "        \"\"\"Simulate stakeholder feedback and analyze it.\"\"\"\n",
    "        if not self.current_demo_results.get(\"final_report_output\"):\n",
    "            print(\"Error: Core demo phases must complete before feedback simulation.\")\n",
    "            return False\n",
    "\n",
    "        print(\"\\n\" + \"-\"*50)\n",
    "        print(\"PHASE 6: SIMULATED FEEDBACK ANALYSIS (FeedbackAgent)\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        print(f\"\\nSimulating stakeholder feedback for: {self.selected_standard_name}...\")\n",
    "        simulated_feedback = [\n",
    "            {\"role\": \"Shariah Scholar\", \"rating\": 4, \"comment\": \"The proposals are largely compliant, but clarification is needed on the application of 'gharar yaseer' in proposed digital contracts.\"},\n",
    "            {\"role\": \"Banker\", \"rating\": 5, \"comment\": \"These changes will significantly improve operational efficiency and reduce ambiguity in sukuk issuance.\"},\n",
    "            {\"role\": \"Regulator\", \"rating\": 3, \"comment\": \"While the intent is good, the proposed technological integrations might pose supervisory challenges. We need more detailed risk management guidelines.\"},\n",
    "            {\"role\": \"Academic\", \"rating\": 4, \"comment\": \"A solid step forward. I suggest including references to contemporary research on fintech in Islamic finance for a more robust theoretical underpinning.\"}\n",
    "        ]\n",
    "        print(f\"Generated {len(simulated_feedback)} simulated feedback entries.\")\n",
    "\n",
    "        print(f\"Analyzing simulated feedback using FeedbackAgent...\")\n",
    "        start_time = time.time()\n",
    "        feedback_analysis_output = self.feedback_agent.execute(self.selected_standard_name, simulated_feedback)\n",
    "        self.current_demo_results[\"feedback_analysis_output\"] = feedback_analysis_output\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"FeedbackAgent completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "        print(\"\\nSIMULATED FEEDBACK ANALYSIS (excerpt):\")\n",
    "        print(self._get_excerpt(feedback_analysis_output.get(\"feedback_analysis_summary\"), 500))\n",
    "        return True\n",
    "\n",
    "    def run_complete_enhanced_demo(self):\n",
    "        \"\"\"Run the complete enhanced demonstration process.\"\"\"\n",
    "        if not self.list_and_select_standard():\n",
    "            print(\"Enhanced Demo terminated: Standard selection failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_review_phase():\n",
    "            print(\"Enhanced Demo terminated: Review phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_enhancement_phase():\n",
    "            print(\"Enhanced Demo terminated: Enhancement phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_validation_phase():\n",
    "            print(\"Enhanced Demo terminated: Validation phase failed.\")\n",
    "            return\n",
    "        \n",
    "        if not self.run_report_generation_phase():\n",
    "            print(\"Enhanced Demo terminated: Report generation phase failed.\")\n",
    "            return\n",
    "        \n",
    "        # Enhanced Steps\n",
    "        if not self.run_visualization_phase():\n",
    "            print(\"Enhanced Demo warning: Visualization phase failed, but core demo completed.\")\n",
    "        \n",
    "        if not self.run_simulated_feedback_phase():\n",
    "            print(\"Enhanced Demo warning: Feedback simulation phase failed, but core demo completed.\")\n",
    "            \n",
    "        self.save_demo_results()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"ENHANCED DEMONSTRATION COMPLETED SUCCESSFULLY FOR STANDARD: {self.selected_standard_name}\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"All results, including enhanced analyses, have been saved to the '{config.OUTPUT_DIR}' directory.\")\n",
    "\n",
    "\n",
    "# Main execution block for the demo\n",
    "def main_demo():\n",
    "    \"\"\"Main function to run the demonstration.\"\"\"\n",
    "    print(\"Initializing AAOIFI Standards Enhancement System for Demo...\")\n",
    "    \n",
    "    # Step 1: Ensure Vector DB is populated (or try to populate it)\n",
    "    # Try to initialize system first. If it fails badly (e.g. DB dir issue), then try to process PDFs.\n",
    "    try:\n",
    "        system_check = AAOIFIStandardsEnhancementSystem()\n",
    "        standards_check = system_check.list_available_standards()\n",
    "        if not standards_check or any(s.startswith(\"Error:\") or s.startswith(\"No standards found\") for s in standards_check):\n",
    "            print(\"\\nNo standards readily available or error accessing existing DB.\")\n",
    "            run_pdf_processing = input(\"Attempt to process PDFs in 'pdf_eng' folder to create/recreate database? (yes/no): \").strip().lower()\n",
    "            if run_pdf_processing == 'yes':\n",
    "                print(\"Processing PDFs using safe recreate method...\")\n",
    "                process_pdfs_safe() # This updates config.DB_DIRECTORY\n",
    "                print(\"Re-initializing system with the new/updated database...\")\n",
    "                # System will be initialized below with the potentially new config.DB_DIRECTORY\n",
    "            else:\n",
    "                print(\"PDF processing skipped. Demo may not function if no standards are available.\")\n",
    "        else:\n",
    "            print(f\"Found existing standards: {standards_check[:3]}...\") # Print first few\n",
    "    except Exception as e:\n",
    "        print(f\"Initial system/DB check failed: {e}\")\n",
    "        run_pdf_processing = input(\"Attempt to process PDFs in 'pdf_eng' folder to create/recreate database? (yes/no): \").strip().lower()\n",
    "        if run_pdf_processing == 'yes':\n",
    "            print(\"Processing PDFs using safe recreate method...\")\n",
    "            process_pdfs_safe() # This updates config.DB_DIRECTORY\n",
    "            print(\"Re-initializing system with the new/updated database...\")\n",
    "        else:\n",
    "            print(\"PDF processing skipped. Demo cannot continue without a functional database.\")\n",
    "            return\n",
    "\n",
    "    # Initialize the main system for the demo (picks up current config.DB_DIRECTORY)\n",
    "    try:\n",
    "        system = AAOIFIStandardsEnhancementSystem()\n",
    "    except Exception as e:\n",
    "        print(f\"Fatal error initializing AAOIFIStandardsEnhancementSystem: {e}\")\n",
    "        print(\"Please check your database setup and OpenAI API key.\")\n",
    "        return\n",
    "\n",
    "    # Choose which demo to run\n",
    "    print(\"\\nSelect Demo Type:\")\n",
    "    print(\"1. Basic Demo (Review, Enhance, Validate, Report)\")\n",
    "    print(\"2. Enhanced Demo (includes Visualization and Feedback Analysis)\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice_str = input(\"Enter your choice (1 or 2, or 'exit'): \").strip()\n",
    "            if choice_str.lower() == 'exit':\n",
    "                print(\"Exiting demo.\")\n",
    "                break\n",
    "            if not choice_str: continue\n",
    "\n",
    "            choice = int(choice_str)\n",
    "            if choice == 1:\n",
    "                demo = DemoRunner(system)\n",
    "                demo.run_complete_demo()\n",
    "                break\n",
    "            elif choice == 2:\n",
    "                demo = EnhancedDemoRunner(system)\n",
    "                demo.run_complete_enhanced_demo()\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid selection. Please enter 1 or 2.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number (1 or 2) or 'exit'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during demo execution: {e}\")\n",
    "            # traceback.print_exc() # For debugging\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure OPENAI_API_KEY is set\n",
    "    if not os.environ.get(\"OPENAI_API_KEY\") or \"sk-proj-\" not in os.environ.get(\"OPENAI_API_KEY\"): # Basic check\n",
    "        print(\"Error: OPENAI_API_KEY is not set or appears invalid in the script.\")\n",
    "        print(\"Please set it near the top of the script (line 24 approx).\")\n",
    "        # You might want to exit here if the key is critical for all operations\n",
    "        # exit() # Uncomment if you want to force exit\n",
    "\n",
    "    # Check if pdf_eng folder exists and has PDFs, offer to run process_pdfs_safe if empty\n",
    "    pdf_folder = Config.PDF_FOLDER\n",
    "    if not os.path.exists(pdf_folder):\n",
    "        print(f\"PDF folder '{pdf_folder}' does not exist. Please create it and add AAOIFI standard PDFs.\")\n",
    "    elif not [f for f in os.listdir(pdf_folder) if f.lower().endswith('.pdf')]:\n",
    "        print(f\"PDF folder '{pdf_folder}' is empty. Please add AAOIFI standard PDFs to process.\")\n",
    "    \n",
    "    # Run the main demonstration logic\n",
    "    main_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58e1d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Add these imports which are referenced but not imported in the original code\n",
    "import chromadb\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Configure logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(\"aaoifi_standards_system.log\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"AAOIFI_MAS\")\n",
    "\n",
    "# Define the Config class that's referenced throughout the code\n",
    "class Config:\n",
    "    OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "    MODEL_NAME = \"gpt-4\"\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    DB_DIRECTORY = \"./vector_db\"\n",
    "    COLLECTION_NAME = \"aaoifi_standards\"\n",
    "    OUTPUT_DIR = \"./output\"\n",
    "\n",
    "# Define the StandardDocument class referenced in the code\n",
    "class StandardDocument:\n",
    "    def __init__(self, name: str, content: str):\n",
    "        self.name = name\n",
    "        self.content = content\n",
    "\n",
    "# Define the BaseAgent class that other agents inherit from\n",
    "class BaseAgent:\n",
    "    def __init__(self, name: str, description: str, agent_type: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.agent_type = agent_type\n",
    "        self.system_prompt = \"\"\n",
    "    \n",
    "    def _run_chain(self, messages: List):\n",
    "        \"\"\"Run the LLM chain with the given messages.\"\"\"\n",
    "        # In a real implementation, this would use the LLM\n",
    "        # For now, just return a placeholder\n",
    "        return \"This is a placeholder for LLM response\"\n",
    "    \n",
    "    def log_execution(self, input_name: str, output_name: str, start_time: float):\n",
    "        \"\"\"Log the execution details.\"\"\"\n",
    "        execution_time = time.time() - start_time\n",
    "        logger.info(f\"{self.name} processed {input_name} into {output_name} in {execution_time:.2f} seconds\")\n",
    "\n",
    "# Define the DocumentProcessor class referenced in the code\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def split_text_into_chunks(text: str, doc_name: str, chunk_size: int = 1000):\n",
    "        \"\"\"Split text into chunks with metadata.\"\"\"\n",
    "        # Simple chunking by character count\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), chunk_size):\n",
    "            chunk_text = text[i:i+chunk_size]\n",
    "            chunks.append({\n",
    "                \"content\": chunk_text,\n",
    "                \"metadata\": {\n",
    "                    \"source\": doc_name,\n",
    "                    \"chunk\": i // chunk_size\n",
    "                }\n",
    "            })\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27e973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fcb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze> requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60b0ba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Solution\n",
      "\n",
      "### Calculations\n",
      "\n",
      "\n",
      "### Journal Entries\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "class IslamicAccountingStandard:\n",
    "    \"\"\"Base class for Islamic accounting standards.\"\"\"\n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.use_cases = []\n",
    "    \n",
    "    def add_use_case(self, use_case):\n",
    "        \"\"\"Add a use case to this standard.\"\"\"\n",
    "        self.use_cases.append(use_case)\n",
    "        \n",
    "    def get_use_cases(self):\n",
    "        \"\"\"Get all use cases for this standard.\"\"\"\n",
    "        return self.use_cases\n",
    "\n",
    "\n",
    "class AccountingUseCase:\n",
    "    \"\"\"Class representing an accounting use case with scenario and solution.\"\"\"\n",
    "    def __init__(self, \n",
    "                 category: str, \n",
    "                 subcategory: str,\n",
    "                 scenario: str, \n",
    "                 question: str,\n",
    "                 correct_solution: str,\n",
    "                 metadata: Optional[Dict] = None):\n",
    "        self.category = category\n",
    "        self.subcategory = subcategory\n",
    "        self.scenario = scenario\n",
    "        self.question = question\n",
    "        self.correct_solution = correct_solution\n",
    "        self.metadata = metadata or {}\n",
    "        self.solution_components = self._extract_solution_components()\n",
    "    \n",
    "    def _extract_solution_components(self):\n",
    "        \"\"\"Extract key components from the correct solution.\"\"\"\n",
    "        components = {}\n",
    "        \n",
    "        # Extract journal entries from the solution\n",
    "        journal_entries = []\n",
    "        dr_cr_pattern = r'Dr\\.\\s+([^USD]+)\\s+USD\\s+([\\d,]+)[\\r\\n]+Cr\\.\\s+([^USD]+)\\s+USD\\s+([\\d,]+)'\n",
    "        matches = re.findall(dr_cr_pattern, self.correct_solution)\n",
    "        \n",
    "        for match in matches:\n",
    "            debit_account = match[0].strip()\n",
    "            debit_amount = float(match[1].replace(',', ''))\n",
    "            credit_account = match[2].strip()\n",
    "            credit_amount = float(match[3].replace(',', ''))\n",
    "            \n",
    "            journal_entries.append({\n",
    "                'debit_account': debit_account,\n",
    "                'debit_amount': debit_amount,\n",
    "                'credit_account': credit_account,\n",
    "                'credit_amount': credit_amount\n",
    "            })\n",
    "        \n",
    "        components['journal_entries'] = journal_entries\n",
    "        \n",
    "        # Extract calculations and values\n",
    "        amount_pattern = r'([A-Za-z\\s]+)[:=]\\s*\\$?USD?\\s*([\\d,]+)'\n",
    "        calculations = re.findall(amount_pattern, self.correct_solution)\n",
    "        for calc in calculations:\n",
    "            key = calc[0].strip().lower().replace(' ', '_')\n",
    "            value = float(calc[1].replace(',', ''))\n",
    "            components[key] = value\n",
    "            \n",
    "        return components\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert the use case to a dictionary.\"\"\"\n",
    "        return {\n",
    "            'category': self.category,\n",
    "            'subcategory': self.subcategory,\n",
    "            'scenario': self.scenario,\n",
    "            'question': self.question,\n",
    "            'correct_solution': self.correct_solution,\n",
    "            'metadata': self.metadata,\n",
    "            'solution_components': self.solution_components\n",
    "        }\n",
    "\n",
    "\n",
    "class IjarahAccountingAgent:\n",
    "    \"\"\"Agent for handling Ijarah accounting use cases.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"Ijarah Accounting Expert\"\n",
    "        self.description = \"Specialized agent for Islamic finance Ijarah accounting scenarios\"\n",
    "        self.standards = {}\n",
    "        self.use_cases = []\n",
    "        \n",
    "    def add_standard(self, standard: IslamicAccountingStandard):\n",
    "        \"\"\"Add an accounting standard to the agent's knowledge.\"\"\"\n",
    "        self.standards[standard.name] = standard\n",
    "        \n",
    "    def add_use_case(self, use_case: AccountingUseCase, standard_name: Optional[str] = None):\n",
    "        \"\"\"Add a use case to the agent and optionally to a specific standard.\"\"\"\n",
    "        self.use_cases.append(use_case)\n",
    "        if standard_name and standard_name in self.standards:\n",
    "            self.standards[standard_name].add_use_case(use_case)\n",
    "            \n",
    "    def load_use_cases_from_json(self, json_file_path: str):\n",
    "        \"\"\"Load use cases from a JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            for item in data:\n",
    "                use_case = AccountingUseCase(\n",
    "                    category=item.get('category', ''),\n",
    "                    subcategory=item.get('subcategory', ''),\n",
    "                    scenario=item.get('scenario', ''),\n",
    "                    question=item.get('question', ''),\n",
    "                    correct_solution=item.get('correct_solution', ''),\n",
    "                    metadata=item.get('metadata', {})\n",
    "                )\n",
    "                self.add_use_case(use_case, item.get('standard', None))\n",
    "                \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading use cases: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def save_use_cases_to_json(self, json_file_path: str):\n",
    "        \"\"\"Save all use cases to a JSON file.\"\"\"\n",
    "        try:\n",
    "            data = [use_case.to_dict() for use_case in self.use_cases]\n",
    "            with open(json_file_path, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving use cases: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def find_similar_use_case(self, query: str, top_n: int = 1):\n",
    "        \"\"\"Find use cases similar to the query using simple keyword matching.\"\"\"\n",
    "        # This is a simple implementation; in a real system, you would use embeddings\n",
    "        query_words = set(query.lower().split())\n",
    "        scored_cases = []\n",
    "        \n",
    "        for use_case in self.use_cases:\n",
    "            # Combine all text fields for searching\n",
    "            use_case_text = f\"{use_case.category} {use_case.subcategory} {use_case.scenario} {use_case.question}\"\n",
    "            use_case_words = set(use_case_text.lower().split())\n",
    "            \n",
    "            # Calculate simple overlap score\n",
    "            overlap = len(query_words.intersection(use_case_words))\n",
    "            if overlap > 0:\n",
    "                scored_cases.append((use_case, overlap))\n",
    "        \n",
    "        # Sort by score (highest first) and return top_n\n",
    "        scored_cases.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [case for case, score in scored_cases[:top_n]]\n",
    "\n",
    "\n",
    "class IjarahMBTCalculator:\n",
    "    \"\"\"\n",
    "    Calculator for Ijarah Muntahia Bittamleek (MBT) accounting calculations.\n",
    "    Handles calculations for both lessor and lessee accounting.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_rou_asset(asset_cost: float, \n",
    "                          import_tax: float,\n",
    "                          freight: float,\n",
    "                          terminal_value: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the Right of Use (ROU) asset amount.\n",
    "        \n",
    "        Args:\n",
    "            asset_cost: The base cost of the asset\n",
    "            import_tax: Import tax paid on the asset\n",
    "            freight: Freight charges for the asset\n",
    "            terminal_value: Expected terminal/residual value\n",
    "            \n",
    "        Returns:\n",
    "            The calculated ROU asset value\n",
    "        \"\"\"\n",
    "        prime_cost = asset_cost + import_tax + freight\n",
    "        rou = prime_cost - terminal_value\n",
    "        return rou\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_deferred_ijarah_cost(total_rentals: float, \n",
    "                                      rou_value: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the deferred Ijarah cost.\n",
    "        \n",
    "        Args:\n",
    "            total_rentals: Total rental payments over the Ijarah term\n",
    "            rou_value: Value of the Right of Use asset\n",
    "            \n",
    "        Returns:\n",
    "            The calculated deferred Ijarah cost\n",
    "        \"\"\"\n",
    "        return total_rentals - rou_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_amortizable_amount(rou_value: float, \n",
    "                                   residual_value: float,\n",
    "                                   purchase_price: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the amortizable amount.\n",
    "        \n",
    "        Args:\n",
    "            rou_value: Value of the Right of Use asset\n",
    "            residual_value: Expected residual value of the asset\n",
    "            purchase_price: Price to purchase the asset at the end of term\n",
    "            \n",
    "        Returns:\n",
    "            The calculated amortizable amount\n",
    "        \"\"\"\n",
    "        terminal_value_difference = residual_value - purchase_price\n",
    "        return rou_value - terminal_value_difference\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_annual_amortization(amortizable_amount: float, \n",
    "                                    ijarah_term_years: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the annual amortization amount.\n",
    "        \n",
    "        Args:\n",
    "            amortizable_amount: Amount to be amortized\n",
    "            ijarah_term_years: Duration of the Ijarah term in years\n",
    "            \n",
    "        Returns:\n",
    "            The annual amortization amount\n",
    "        \"\"\"\n",
    "        return amortizable_amount / ijarah_term_years\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_lessee_initial_recognition_entry(rou_value: float, \n",
    "                                               deferred_cost: float,\n",
    "                                               total_liability: float) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate journal entry for lessee's initial recognition.\n",
    "        \n",
    "        Args:\n",
    "            rou_value: Value of the Right of Use asset\n",
    "            deferred_cost: Deferred Ijarah cost\n",
    "            total_liability: Total Ijarah liability\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the journal entry details\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"entry_type\": \"Initial Recognition (Lessee)\",\n",
    "            \"debit_entries\": [\n",
    "                {\"account\": \"Right of Use Asset (ROU)\", \"amount\": round(rou_value, 2)},\n",
    "                {\"account\": \"Deferred Ijarah Cost\", \"amount\": round(deferred_cost, 2)}\n",
    "            ],\n",
    "            \"credit_entries\": [\n",
    "                {\"account\": \"Ijarah Liability\", \"amount\": round(total_liability, 2)}\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_lessee_rental_payment_entry(rental_amount: float) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate journal entry for lessee's rental payment.\n",
    "        \n",
    "        Args:\n",
    "            rental_amount: Periodic rental payment amount\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the journal entry details\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"entry_type\": \"Rental Payment (Lessee)\",\n",
    "            \"debit_entries\": [\n",
    "                {\"account\": \"Ijarah Liability\", \"amount\": round(rental_amount, 2)}\n",
    "            ],\n",
    "            \"credit_entries\": [\n",
    "                {\"account\": \"Cash/Bank\", \"amount\": round(rental_amount, 2)}\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_lessee_amortization_entry(amortization_amount: float) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate journal entry for lessee's amortization of ROU asset.\n",
    "        \n",
    "        Args:\n",
    "            amortization_amount: Amount of amortization for the period\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the journal entry details\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"entry_type\": \"Amortization (Lessee)\",\n",
    "            \"debit_entries\": [\n",
    "                {\"account\": \"Ijarah Expense\", \"amount\": round(amortization_amount, 2)}\n",
    "            ],\n",
    "            \"credit_entries\": [\n",
    "                {\"account\": \"Right of Use Asset (ROU)\", \"amount\": round(amortization_amount, 2)}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "class IjarahUseCaseProcessor:\n",
    "    \"\"\"\n",
    "    Processes Ijarah use cases and generates standardized solutions.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.calculator = IjarahMBTCalculator()\n",
    "        \n",
    "    def process_ijarah_mbt_lessee_case(self, use_case_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Process an Ijarah MBT use case for lessee accounting.\n",
    "        \n",
    "        Args:\n",
    "            use_case_data: Dictionary containing the use case details\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with the processed solution\n",
    "        \"\"\"\n",
    "        # Extract relevant parameters from the use case\n",
    "        asset_cost = use_case_data.get('asset_cost')\n",
    "        import_tax = use_case_data.get('import_tax', 0)\n",
    "        freight = use_case_data.get('freight', 0)\n",
    "        ijarah_term = use_case_data.get('ijarah_term')\n",
    "        residual_value = use_case_data.get('residual_value', 0)\n",
    "        purchase_price = use_case_data.get('purchase_price', 0)\n",
    "        annual_rental = use_case_data.get('annual_rental')\n",
    "        \n",
    "        # Calculate key values\n",
    "        prime_cost = asset_cost + import_tax + freight\n",
    "        rou_value = self.calculator.calculate_rou_asset(\n",
    "            asset_cost, import_tax, freight, purchase_price)\n",
    "        \n",
    "        total_rentals = annual_rental * ijarah_term\n",
    "        deferred_cost = self.calculator.calculate_deferred_ijarah_cost(\n",
    "            total_rentals, rou_value)\n",
    "        \n",
    "        terminal_value_difference = residual_value - purchase_price\n",
    "        amortizable_amount = self.calculator.calculate_amortizable_amount(\n",
    "            rou_value, residual_value, purchase_price)\n",
    "        \n",
    "        # Generate journal entries\n",
    "        initial_entry = self.calculator.generate_lessee_initial_recognition_entry(\n",
    "            rou_value, deferred_cost, total_rentals)\n",
    "        \n",
    "        # Format the solution\n",
    "        solution = {\n",
    "            \"calculations\": {\n",
    "                \"prime_cost\": prime_cost,\n",
    "                \"right_of_use_asset\": rou_value,\n",
    "                \"total_rentals\": total_rentals,\n",
    "                \"deferred_ijarah_cost\": deferred_cost,\n",
    "                \"terminal_value_difference\": terminal_value_difference,\n",
    "                \"amortizable_amount\": amortizable_amount\n",
    "            },\n",
    "            \"journal_entries\": [initial_entry]\n",
    "        }\n",
    "        \n",
    "        return solution\n",
    "    \n",
    "    def format_solution_markdown(self, solution: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Format the solution into a readable markdown string.\n",
    "        \n",
    "        Args:\n",
    "            solution: Dictionary containing the solution details\n",
    "            \n",
    "        Returns:\n",
    "            Formatted solution as markdown string\n",
    "        \"\"\"\n",
    "        md = \"## Solution\\n\\n\"\n",
    "        \n",
    "        # Format calculations\n",
    "        md += \"### Calculations\\n\\n\"\n",
    "        calcs = solution.get('calculations', {})\n",
    "        for key, value in calcs.items():\n",
    "            md += f\"**{key.replace('_', ' ').title()}**: USD {value:,.2f}\\n\"\n",
    "        \n",
    "        # Format journal entries\n",
    "        md += \"\\n### Journal Entries\\n\\n\"\n",
    "        for entry in solution.get('journal_entries', []):\n",
    "            md += f\"**{entry.get('entry_type')}**:\\n\\n\"\n",
    "            \n",
    "            for debit in entry.get('debit_entries', []):\n",
    "                md += f\"Dr. {debit.get('account')} USD {debit.get('amount'):,.2f}\\n\"\n",
    "                \n",
    "            for credit in entry.get('credit_entries', []):\n",
    "                md += f\"Cr. {credit.get('account')} USD {credit.get('amount'):,.2f}\\n\"\n",
    "            \n",
    "            md += \"\\n\"\n",
    "        \n",
    "        return md\n",
    "\n",
    "\n",
    "class IslamicFinanceAccountingSystem:\n",
    "    \"\"\"Main system for Islamic finance accounting use cases.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ijarah_agent = IjarahAccountingAgent()\n",
    "        self.use_case_processor = IjarahUseCaseProcessor()\n",
    "        self.standards_db = {}\n",
    "        self.initialize_system()\n",
    "        \n",
    "    def initialize_system(self):\n",
    "        \"\"\"Initialize the system with built-in standards and use cases.\"\"\"\n",
    "        # Create standard definitions\n",
    "        aaoifi_fas_8 = IslamicAccountingStandard(\n",
    "            name=\"AAOIFI FAS 8\",\n",
    "            description=\"Financial Accounting Standard 8: Ijarah and Ijarah Muntahia Bittamleek\"\n",
    "        )\n",
    "        \n",
    "        # Add standards to the system\n",
    "        self.standards_db[\"AAOIFI FAS 8\"] = aaoifi_fas_8\n",
    "        self.ijarah_agent.add_standard(aaoifi_fas_8)\n",
    "        \n",
    "        # Add built-in use cases\n",
    "        self._add_builtin_use_cases()\n",
    "    \n",
    "    def _add_builtin_use_cases(self):\n",
    "        \"\"\"Add built-in use cases to the system.\"\"\"\n",
    "        # Add the example use case from the problem statement\n",
    "        ijarah_mbt_use_case = AccountingUseCase(\n",
    "            category=\"Ijarah MBT Accounting\",\n",
    "            subcategory=\"Lessee's Books\",\n",
    "            scenario=\"\"\"\n",
    "            On 1 January 2019 Alpha Islamic bank (Lessee) entered into an Ijarah MBT arrangement with\n",
    "            Super Generators for Ijarah of a heavy-duty generator purchase by Super Generators at a price\n",
    "            of USD 450,000. Super Generators has also paid USD 12,000 as import tax and US 30,000 for\n",
    "            freight charges. The Ijarah Term is 02 years and expected residual value at the end USD 5,000.\n",
    "            At the end of Ijarah Term, it is highly likely that the option of transfer of ownership of the\n",
    "            underlying asset to the lessee shall be exercised through purchase at a price of USD 3,000.\n",
    "            Alpha Islamic Bank will amortize the 'right of use' on yearly basis and it is required to pay\n",
    "            yearly rental of USD 300,000.\n",
    "            \"\"\",\n",
    "            question=\"\"\"\n",
    "            Provide the following accounting entry in the books of Alpha Islamic Bank:\n",
    "            Initial Recognition at the time of commencement of Ijarah (using Underlying Asset Cost Method).\n",
    "            \"\"\",\n",
    "            correct_solution=\"\"\"\n",
    "            Initial Recognition at the Time of Commencement of Ijarah (1 January 2019):\n",
    "            The cost of the underlying asset is the basis for recognizing the \"Right of Use\" (ROU) asset.\n",
    "            \n",
    "            Determine the Right-of-Use Asset (ROU)\n",
    "             Prime cost (Purchase + Import tax + Freight): = 450,000 + 12,000 + 30,000 = 492,000\n",
    "             Less: Terminal value (i.e., Purchase price of USD 3,000 to acquire ownership) = 492,000  3,000 = 489,000 (ROU)\n",
    "            \n",
    "            Add Deferred Ijarah Cost:\n",
    "             Total rentals over 2 years = 300,000  2 = 600,000\n",
    "             Less: Present value of ROU = 489,000\n",
    "             Deferred Ijarah Cost = 600,000  489,000 = 111,000\n",
    "            \n",
    "            Journal Entry:\n",
    "            Dr. Right of Use Asset (ROU) USD 489,000\n",
    "            Dr. Deferred Ijarah Cost USD 111,000\n",
    "            Cr. Ijarah Liability USD 600,000\n",
    "            \n",
    "            Amortizable Amount Calculation\n",
    "            Description USD\n",
    "            Cost of ROU 489,000\n",
    "            Less: Terminal value difference (Residual 5,000  Purchase 3,000) 2,000\n",
    "            Amortizable Amount 487,000\n",
    "            We deduct this 2,000 since the Lessee is expected to gain ownership\n",
    "            \"\"\",\n",
    "            metadata={\n",
    "                \"asset_cost\": 450000,\n",
    "                \"import_tax\": 12000,\n",
    "                \"freight\": 30000,\n",
    "                \"ijarah_term\": 2,\n",
    "                \"residual_value\": 5000,\n",
    "                \"purchase_price\": 3000,\n",
    "                \"annual_rental\": 300000,\n",
    "                \"standard\": \"AAOIFI FAS 8\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.ijarah_agent.add_use_case(ijarah_mbt_use_case, \"AAOIFI FAS 8\")\n",
    "    \n",
    "    def parse_user_query(self, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Parse a user query to extract relevant parameters.\n",
    "        This is a simplified implementation that would be enhanced with NLP.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query string\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing extracted parameters\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        \n",
    "        # Extract numbers using regex\n",
    "        numbers = re.findall(r'USD\\s*([\\d,]+)', query)\n",
    "        numbers = [float(num.replace(',', '')) for num in numbers]\n",
    "        \n",
    "        # Look for specific keywords and associate with parameters\n",
    "        if 'ijarah mbt' in query.lower():\n",
    "            params['transaction_type'] = 'ijarah_mbt'\n",
    "        \n",
    "        if 'lessee' in query.lower():\n",
    "            params['party'] = 'lessee'\n",
    "        elif 'lessor' in query.lower():\n",
    "            params['party'] = 'lessor'\n",
    "            \n",
    "        if 'initial recognition' in query.lower():\n",
    "            params['entry_type'] = 'initial_recognition'\n",
    "            \n",
    "        if 'rental payment' in query.lower():\n",
    "            params['entry_type'] = 'rental_payment'\n",
    "            \n",
    "        if 'amortization' in query.lower():\n",
    "            params['entry_type'] = 'amortization'\n",
    "            \n",
    "        # Attempt to extract financial values based on context\n",
    "        if len(numbers) >= 3:\n",
    "            # This is very simplified and would need enhancement in a real system\n",
    "            if len(numbers) >= 5:\n",
    "                params['asset_cost'] = numbers[0]\n",
    "            \n",
    "        return params\n",
    "    \n",
    "    def generate_accounting_entry(self, use_case_data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate accounting entries based on the use case data.\n",
    "        \n",
    "        Args:\n",
    "            use_case_data: Dictionary with parameters for the use case\n",
    "            \n",
    "        Returns:\n",
    "            Solution dictionary with calculations and journal entries\n",
    "        \"\"\"\n",
    "        transaction_type = use_case_data.get('transaction_type', '')\n",
    "        party = use_case_data.get('party', '')\n",
    "        \n",
    "        if transaction_type == 'ijarah_mbt' and party == 'lessee':\n",
    "            solution = self.use_case_processor.process_ijarah_mbt_lessee_case(use_case_data)\n",
    "            return solution\n",
    "        \n",
    "        # Handle other cases as needed\n",
    "        return {\"error\": \"Unsupported transaction type or party\"}\n",
    "    \n",
    "    def find_matching_use_case(self, query: str) -> Optional[AccountingUseCase]:\n",
    "        \"\"\"\n",
    "        Find a matching use case from the database based on the query.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query string\n",
    "            \n",
    "        Returns:\n",
    "            Matching use case if found, None otherwise\n",
    "        \"\"\"\n",
    "        similar_cases = self.ijarah_agent.find_similar_use_case(query)\n",
    "        if similar_cases:\n",
    "            return similar_cases[0]\n",
    "        return None\n",
    "    \n",
    "    def process_query(self, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Process a user query and generate a response.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query string\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with the response\n",
    "        \"\"\"\n",
    "        # First try to find a matching use case\n",
    "        matching_case = self.find_matching_use_case(query)\n",
    "        if matching_case:\n",
    "            # Use the matching case's metadata for processing\n",
    "            solution = self.generate_accounting_entry(matching_case.metadata)\n",
    "            formatted_solution = self.use_case_processor.format_solution_markdown(solution)\n",
    "            \n",
    "            return {\n",
    "                \"found_matching_case\": True,\n",
    "                \"use_case\": matching_case.to_dict(),\n",
    "                \"generated_solution\": solution,\n",
    "                \"formatted_solution\": formatted_solution,\n",
    "                \"correct_solution\": matching_case.correct_solution\n",
    "            }\n",
    "        \n",
    "        # If no matching case, try to parse the query and generate a solution\n",
    "        params = self.parse_user_query(query)\n",
    "        if params:\n",
    "            solution = self.generate_accounting_entry(params)\n",
    "            formatted_solution = self.use_case_processor.format_solution_markdown(solution)\n",
    "            \n",
    "            return {\n",
    "                \"found_matching_case\": False,\n",
    "                \"parsed_params\": params,\n",
    "                \"generated_solution\": solution,\n",
    "                \"formatted_solution\": formatted_solution\n",
    "            }\n",
    "        \n",
    "        return {\"error\": \"Could not parse query or find matching use case\"}\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the system\n",
    "    islamic_finance_system = IslamicFinanceAccountingSystem()\n",
    "    \n",
    "    # Example query\n",
    "    query = \"\"\"\n",
    "    I need the initial recognition entry for an Ijarah MBT arrangement where the lessee (Islamic bank)\n",
    "    is leasing a generator purchased at USD 450,000 with USD 12,000 import tax and USD 30,000 freight.\n",
    "    The Ijarah term is 2 years with yearly rental of USD 300,000 and expected residual value of USD 5,000.\n",
    "    The lessee will purchase the asset at the end for USD 3,000.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process the query\n",
    "    result = islamic_finance_system.process_query(query)\n",
    "    \n",
    "    # Print the result\n",
    "    if \"formatted_solution\" in result:\n",
    "        print(result[\"formatted_solution\"])\n",
    "    elif \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7fb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, scrolledtext, messagebox, filedialog\n",
    "\n",
    "\n",
    "class IslamicFinanceAccountingApp:\n",
    "    \"\"\"GUI application for Islamic Finance Accounting Agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Islamic Finance Accounting Agent\")\n",
    "        self.root.geometry(\"1200x800\")\n",
    "        \n",
    "        # Initialize the accounting system\n",
    "        self.system = IslamicFinanceAccountingSystem()\n",
    "        \n",
    "        # Set up the main frame\n",
    "        self.setup_ui()\n",
    "        \n",
    "        # Load existing use cases\n",
    "        self.load_use_cases()\n",
    "    \n",
    "    def setup_ui(self):\n",
    "        \"\"\"Set up the user interface.\"\"\"\n",
    "        # Create notebook (tabbed interface)\n",
    "        self.notebook = ttk.Notebook(self.root)\n",
    "        self.notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Create tabs\n",
    "        self.query_tab = ttk.Frame(self.notebook)\n",
    "        self.use_cases_tab = ttk.Frame(self.notebook)\n",
    "        self.add_use_case_tab = ttk.Frame(self.notebook)\n",
    "        \n",
    "        self.notebook.add(self.query_tab, text=\"Query Processor\")\n",
    "        self.notebook.add(self.use_cases_tab, text=\"Use Cases\")\n",
    "        self.notebook.add(self.add_use_case_tab, text=\"Add Use Case\")\n",
    "        \n",
    "        # Set up each tab\n",
    "        self.setup_query_tab()\n",
    "        self.setup_use_cases_tab()\n",
    "        self.setup_add_use_case_tab()\n",
    "    \n",
    "    def setup_query_tab(self):\n",
    "        \"\"\"Set up the query processing tab.\"\"\"\n",
    "        # Query input frame\n",
    "        input_frame = ttk.LabelFrame(self.query_tab, text=\"Enter Your Query\")\n",
    "        input_frame.pack(fill=tk.BOTH, expand=False, padx=10, pady=10)\n",
    "        \n",
    "        self.query_input = scrolledtext.ScrolledText(input_frame, wrap=tk.WORD, height=8)\n",
    "        self.query_input.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Sample query button\n",
    "        sample_button = ttk.Button(input_frame, text=\"Load Sample Query\", command=self.load_sample_query)\n",
    "        sample_button.pack(side=tk.LEFT, padx=5, pady=5)\n",
    "        \n",
    "        # Process button\n",
    "        process_button = ttk.Button(input_frame, text=\"Process Query\", command=self.process_query)\n",
    "        process_button.pack(side=tk.RIGHT, padx=5, pady=5)\n",
    "        \n",
    "        # Results frame\n",
    "        results_frame = ttk.LabelFrame(self.query_tab, text=\"Results\")\n",
    "        results_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Split the results frame into two panels\n",
    "        self.results_paned = ttk.PanedWindow(results_frame, orient=tk.HORIZONTAL)\n",
    "        self.results_paned.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Generated solution frame\n",
    "        gen_solution_frame = ttk.LabelFrame(self.results_paned, text=\"Generated Solution\")\n",
    "        self.generated_solution = scrolledtext.ScrolledText(gen_solution_frame, wrap=tk.WORD)\n",
    "        self.generated_solution.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Correct solution frame\n",
    "        correct_solution_frame = ttk.LabelFrame(self.results_paned, text=\"Correct Solution\")\n",
    "        self.correct_solution = scrolledtext.ScrolledText(correct_solution_frame, wrap=tk.WORD)\n",
    "        self.correct_solution.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Add both frames to the paned window\n",
    "        self.results_paned.add(gen_solution_frame, weight=1)\n",
    "        self.results_paned.add(correct_solution_frame, weight=1)\n",
    "    \n",
    "    def setup_use_cases_tab(self):\n",
    "        \"\"\"Set up the use cases tab.\"\"\"\n",
    "        # Filter frame\n",
    "        filter_frame = ttk.Frame(self.use_cases_tab)\n",
    "        filter_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        ttk.Label(filter_frame, text=\"Filter by Category:\").pack(side=tk.LEFT, padx=5)\n",
    "        self.category_filter = ttk.Combobox(filter_frame, width=30)\n",
    "        self.category_filter.pack(side=tk.LEFT, padx=5)\n",
    "        self.category_filter.bind(\"<<ComboboxSelected>>\", self.filter_use_cases)\n",
    "        \n",
    "        ttk.Button(filter_frame, text=\"Export Use Cases\", command=self.export_use_cases).pack(side=tk.RIGHT, padx=5)\n",
    "        ttk.Button(filter_frame, text=\"Import Use Cases\", command=self.import_use_cases).pack(side=tk.RIGHT, padx=5)\n",
    "        \n",
    "        # Use cases table\n",
    "        table_frame = ttk.Frame(self.use_cases_tab)\n",
    "        table_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)\n",
    "        \n",
    "        # Create treeview\n",
    "        self.use_cases_tree = ttk.Treeview(\n",
    "            table_frame, \n",
    "            columns=(\"category\", \"subcategory\", \"scenario\", \"question\"),\n",
    "            show=\"headings\"\n",
    "        )\n",
    "        \n",
    "        # Set column headings\n",
    "        self.use_cases_tree.heading(\"category\", text=\"Category\")\n",
    "        self.use_cases_tree.heading(\"subcategory\", text=\"Subcategory\")\n",
    "        self.use_cases_tree.heading(\"scenario\", text=\"Scenario\")\n",
    "        self.use_cases_tree.heading(\"question\", text=\"Question\")\n",
    "        \n",
    "        # Set column widths\n",
    "        self.use_cases_tree.column(\"category\", width=150)\n",
    "        self.use_cases_tree.column(\"subcategory\", width=150)\n",
    "        self.use_cases_tree.column(\"scenario\", width=300)\n",
    "        self.use_cases_tree.column(\"question\", width=300)\n",
    "        \n",
    "        # Add scrollbars\n",
    "        vsb = ttk.Scrollbar(table_frame, orient=\"vertical\", command=self.use_cases_tree.yview)\n",
    "        hsb = ttk.Scrollbar(table_frame, orient=\"horizontal\", command=self.use_cases_tree.xview)\n",
    "        self.use_cases_tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)\n",
    "        \n",
    "        # Grid layout\n",
    "        self.use_cases_tree.grid(column=0, row=0, sticky='nsew')\n",
    "        vsb.grid(column=1, row=0, sticky='ns')\n",
    "        hsb.grid(column=0, row=1, sticky='ew')\n",
    "        \n",
    "        # Configure the grid\n",
    "        table_frame.grid_columnconfigure(0, weight=1)\n",
    "        table_frame.grid_rowconfigure(0, weight=1)\n",
    "        \n",
    "        # Bind double-click event to view use case details\n",
    "        self.use_cases_tree.bind(\"<Double-1>\", self.view_use_case_details)\n",
    "    \n",
    "    def setup_add_use_case_tab(self):\n",
    "        \"\"\"Set up the tab for adding new use cases.\"\"\"\n",
    "        # Form frame\n",
    "        form_frame = ttk.Frame(self.add_use_case_tab)\n",
    "        form_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Create a notebook for the form sections\n",
    "        form_notebook = ttk.Notebook(form_frame)\n",
    "        form_notebook.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Basic info tab\n",
    "        basic_info_tab = ttk.Frame(form_notebook)\n",
    "        form_notebook.add(basic_info_tab, text=\"Basic Information\")\n",
    "        \n",
    "        # Scenario tab\n",
    "        scenario_tab = ttk.Frame(form_notebook)\n",
    "        form_notebook.add(scenario_tab, text=\"Scenario & Question\")\n",
    "        \n",
    "        # Solution tab\n",
    "        solution_tab = ttk.Frame(form_notebook)\n",
    "        form_notebook.add(solution_tab, text=\"Solution & Metadata\")\n",
    "        \n",
    "        # Setup each tab\n",
    "        self.setup_basic_info_tab(basic_info_tab)\n",
    "        self.setup_scenario_tab(scenario_tab)\n",
    "        self.setup_solution_tab(solution_tab)\n",
    "        \n",
    "        # Buttons frame\n",
    "        button_frame = ttk.Frame(self.add_use_case_tab)\n",
    "        button_frame.pack(fill=tk.X, padx=10, pady=5)\n",
    "        \n",
    "        ttk.Button(button_frame, text=\"Clear Form\", command=self.clear_form).pack(side=tk.LEFT, padx=5)\n",
    "        ttk.Button(button_frame, text=\"Add Use Case\", command=self.add_new_use_case).pack(side=tk.RIGHT, padx=5)\n",
    "    \n",
    "    def setup_basic_info_tab(self, parent):\n",
    "        \"\"\"Set up the basic information form tab.\"\"\"\n",
    "        # Add fields with labels\n",
    "        ttk.Label(parent, text=\"Category:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        self.category_entry = ttk.Combobox(parent, width=40)\n",
    "        self.category_entry['values'] = [\"Ijarah MBT Accounting\", \"Murabaha Accounting\", \"Sukuk Accounting\", \"Takaful Accounting\"]\n",
    "        self.category_entry.grid(row=0, column=1, sticky=tk.W, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Label(parent, text=\"Subcategory:\").grid(row=1, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        self.subcategory_entry = ttk.Combobox(parent, width=40)\n",
    "        self.subcategory_entry['values'] = [\"Lessee's Books\", \"Lessor's Books\", \"Buyer's Books\", \"Seller's Books\"]\n",
    "        self.subcategory_entry.grid(row=1, column=1, sticky=tk.W, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Label(parent, text=\"Related Standard:\").grid(row=2, column=0, sticky=tk.W, padx=5, pady=5)\n",
    "        self.standard_entry = ttk.Combobox(parent, width=40)\n",
    "        self.standard_entry['values'] = [\"AAOIFI FAS 8\", \"AAOIFI FAS 7\", \"IFRS 16\", \"AAOIFI FAS 30\"]\n",
    "        self.standard_entry.grid(row=2, column=1, sticky=tk.W, padx=5, pady=5)\n",
    "    \n",
    "    def setup_scenario_tab(self, parent):\n",
    "        \"\"\"Set up the scenario and question form tab.\"\"\"\n",
    "        ttk.Label(parent, text=\"Scenario:\").grid(row=0, column=0, sticky=tk.NW, padx=5, pady=5)\n",
    "        self.scenario_text = scrolledtext.ScrolledText(parent, wrap=tk.WORD, height=10)\n",
    "        self.scenario_text.grid(row=0, column=1, sticky=tk.NSEW, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Label(parent, text=\"Question:\").grid(row=1, column=0, sticky=tk.NW, padx=5, pady=5)\n",
    "        self.question_text = scrolledtext.ScrolledText(parent, wrap=tk.WORD, height=6)\n",
    "        self.question_text.grid(row=1, column=1, sticky=tk.NSEW, padx=5, pady=5)\n",
    "        \n",
    "        # Configure grid to expand with window\n",
    "        parent.grid_columnconfigure(1, weight=1)\n",
    "        parent.grid_rowconfigure(0, weight=2)\n",
    "        parent.grid_rowconfigure(1, weight=1)\n",
    "    \n",
    "    def setup_solution_tab(self, parent):\n",
    "        \"\"\"Set up the solution and metadata form tab.\"\"\"\n",
    "        ttk.Label(parent, text=\"Correct Solution:\").grid(row=0, column=0, sticky=tk.NW, padx=5, pady=5)\n",
    "        self.solution_text = scrolledtext.ScrolledText(parent, wrap=tk.WORD, height=10)\n",
    "        self.solution_text.grid(row=0, column=1, sticky=tk.NSEW, padx=5, pady=5)\n",
    "        \n",
    "        # Metadata Frame\n",
    "        metadata_frame = ttk.LabelFrame(parent, text=\"Metadata\")\n",
    "        metadata_frame.grid(row=1, column=0, columnspan=2, sticky=tk.NSEW, padx=5, pady=5)\n",
    "        \n",
    "        # Add metadata fields\n",
    "        ttk.Label(metadata_frame, text=\"Asset Cost:\").grid(row=0, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.asset_cost_entry = ttk.Entry(metadata_frame, width=15)\n",
    "        self.asset_cost_entry.grid(row=0, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(metadata_frame, text=\"Import Tax:\").grid(row=1, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.import_tax_entry = ttk.Entry(metadata_frame, width=15)\n",
    "        self.import_tax_entry.grid(row=1, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(metadata_frame, text=\"Freight:\").grid(row=2, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.freight_entry = ttk.Entry(metadata_frame, width=15)\n",
    "        self.freight_entry.grid(row=2, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(metadata_frame, text=\"Ijarah Term (years):\").grid(row=0, column=2, sticky=tk.W, padx=5, pady=2)\n",
    "        self.ijarah_term_entry = ttk.Entry(metadata_frame, width=15)\n",
    "        self.ijarah_term_entry.grid(row=0, column=3, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(metadata_frame, text=\"Residual Value:\").grid(row=1, column=2, sticky=tk.W, padx=5, pady=2)\n",
    "        self.residual_value_entry = ttk.Entry(metadata_frame, width=15)\n",
    "        self.residual_value_entry.grid(row=1, column=3, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(metadata_frame, text=\"Purchase Price:\").grid(row=2, column=2, sticky=tk.W, padx=5, pady=2)\n",
    "        self.purchase_price_entry = ttk.Entry(metadata_frame, width=15)\n",
    "        self.purchase_price_entry.grid(row=2, column=3, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        ttk.Label(metadata_frame, text=\"Annual Rental:\").grid(row=3, column=0, sticky=tk.W, padx=5, pady=2)\n",
    "        self.annual_rental_entry = ttk.Entry(metadata_frame, width=15)\n",
    "        self.annual_rental_entry.grid(row=3, column=1, sticky=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        # Configure grid to expand with window\n",
    "        parent.grid_columnconfigure(1, weight=1)\n",
    "        parent.grid_rowconfigure(0, weight=1)\n",
    "    \n",
    "    def view_use_case_details(self, event=None):\n",
    "        \"\"\"View details of the selected use case in a pop-up window.\"\"\"\n",
    "        # Get selected item\n",
    "        selection = self.use_cases_tree.selection()\n",
    "        if not selection:\n",
    "            return\n",
    "            \n",
    "        item_id = selection[0]\n",
    "        item_index = int(item_id)\n",
    "        \n",
    "        # Get the use case\n",
    "        use_case = self.system.ijarah_agent.use_cases[item_index]\n",
    "        \n",
    "        # Create a pop-up window\n",
    "        details_window = tk.Toplevel(self.root)\n",
    "        details_window.title(f\"Use Case Details: {use_case.category} - {use_case.subcategory}\")\n",
    "        details_window.geometry(\"800x600\")\n",
    "        \n",
    "        # Create a notebook for organized display\n",
    "        notebook = ttk.Notebook(details_window)\n",
    "        notebook.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Scenario tab\n",
    "        scenario_tab = ttk.Frame(notebook)\n",
    "        notebook.add(scenario_tab, text=\"Scenario & Question\")\n",
    "        \n",
    "        ttk.Label(scenario_tab, text=\"Scenario:\").grid(row=0, column=0, sticky=tk.NW, padx=5, pady=5)\n",
    "        scenario_text = scrolledtext.ScrolledText(scenario_tab, wrap=tk.WORD, height=8)\n",
    "        scenario_text.grid(row=0, column=1, sticky=tk.NSEW, padx=5, pady=5)\n",
    "        scenario_text.insert(tk.END, use_case.scenario)\n",
    "        scenario_text.configure(state=\"disabled\")\n",
    "        \n",
    "        ttk.Label(scenario_tab, text=\"Question:\").grid(row=1, column=0, sticky=tk.NW, padx=5, pady=5)\n",
    "        question_text = scrolledtext.ScrolledText(scenario_tab, wrap=tk.WORD, height=4)\n",
    "        question_text.grid(row=1, column=1, sticky=tk.NSEW, padx=5, pady=5)\n",
    "        question_text.insert(tk.END, use_case.question)\n",
    "        question_text.configure(state=\"disabled\")\n",
    "        \n",
    "        # Solution tab\n",
    "        solution_tab = ttk.Frame(notebook)\n",
    "        notebook.add(solution_tab, text=\"Solution\")\n",
    "        \n",
    "        solution_text = scrolledtext.ScrolledText(solution_tab, wrap=tk.WORD)\n",
    "        solution_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        solution_text.insert(tk.END, use_case.correct_solution)\n",
    "        solution_text.configure(state=\"disabled\")\n",
    "        \n",
    "        # Metadata tab\n",
    "        metadata_tab = ttk.Frame(notebook)\n",
    "        notebook.add(metadata_tab, text=\"Metadata\")\n",
    "        \n",
    "        metadata_text = scrolledtext.ScrolledText(metadata_tab, wrap=tk.WORD)\n",
    "        metadata_text.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        metadata_str = \"Category: \" + use_case.category + \"\\n\"\n",
    "        metadata_str += \"Subcategory: \" + use_case.subcategory + \"\\n\\n\"\n",
    "        metadata_str += \"Standard: \" + use_case.metadata.get(\"standard\", \"Not specified\") + \"\\n\\n\"\n",
    "        \n",
    "        # Add numeric metadata\n",
    "        for key, value in use_case.metadata.items():\n",
    "            if key != \"standard\" and isinstance(value, (int, float)):\n",
    "                metadata_str += f\"{key.replace('_', ' ').title()}: {value:,.2f}\\n\"\n",
    "        \n",
    "        metadata_text.insert(tk.END, metadata_str)\n",
    "        metadata_text.configure(state=\"disabled\")\n",
    "        \n",
    "        # Configure grid layout\n",
    "        scenario_tab.grid_columnconfigure(1, weight=1)\n",
    "        scenario_tab.grid_rowconfigure(0, weight=2)\n",
    "        scenario_tab.grid_rowconfigure(1, weight=1)\n",
    "    \n",
    "    def load_sample_query(self):\n",
    "        \"\"\"Load a sample query into the input box.\"\"\"\n",
    "        sample_query = \"\"\"I need the initial recognition entry for an Ijarah MBT arrangement where the lessee (Islamic bank)\n",
    "is leasing a generator purchased at USD 450,000 with USD 12,000 import tax and USD 30,000 freight.\n",
    "The Ijarah term is 2 years with yearly rental of USD 300,000 and expected residual value of USD 5,000.\n",
    "The lessee will purchase the asset at the end for USD 3,000.\"\"\"\n",
    "        \n",
    "        self.query_input.delete(1.0, tk.END)\n",
    "        self.query_input.insert(tk.END, sample_query)\n",
    "    \n",
    "    def process_query(self):\n",
    "        \"\"\"Process the user's query and display results.\"\"\"\n",
    "        query = self.query_input.get(1.0, tk.END)\n",
    "        if not query.strip():\n",
    "            messagebox.showwarning(\"Empty Query\", \"Please enter a query before processing.\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            # Process the query using the Islamic Finance Accounting System\n",
    "            result = self.system.process_query(query)\n",
    "            \n",
    "            # Clear previous results\n",
    "            self.generated_solution.delete(1.0, tk.END)\n",
    "            self.correct_solution.delete(1.0, tk.END)\n",
    "            \n",
    "            # Display the generated solution\n",
    "            if \"formatted_solution\" in result:\n",
    "                self.generated_solution.insert(tk.END, result[\"formatted_solution\"])\n",
    "                \n",
    "            # Display the correct solution if available\n",
    "            if \"correct_solution\" in result:\n",
    "                self.correct_solution.insert(tk.END, result[\"correct_solution\"])\n",
    "                \n",
    "            # If no matching case was found, show notification\n",
    "            if result.get(\"found_matching_case\", False) == False:\n",
    "                messagebox.showinfo(\"No Exact Match\", \n",
    "                                  \"No exact matching use case was found. Generated solution is based on query parsing.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred while processing your query: {str(e)}\")\n",
    "    \n",
    "    def add_new_use_case(self):\n",
    "        \"\"\"Add a new use case to the system from the form data.\"\"\"\n",
    "        try:\n",
    "            # Collect form data\n",
    "            category = self.category_entry.get()\n",
    "            subcategory = self.subcategory_entry.get()\n",
    "            scenario = self.scenario_text.get(1.0, tk.END).strip()\n",
    "            question = self.question_text.get(1.0, tk.END).strip()\n",
    "            correct_solution = self.solution_text.get(1.0, tk.END).strip()\n",
    "            standard = self.standard_entry.get()\n",
    "            \n",
    "            # Validate required fields\n",
    "            if not all([category, subcategory, scenario, question, correct_solution]):\n",
    "                messagebox.showwarning(\"Missing Information\", \n",
    "                                     \"Please fill in all required fields (Category, Subcategory, Scenario, Question, and Solution).\")\n",
    "                return\n",
    "            \n",
    "            # Collect metadata\n",
    "            metadata = {}\n",
    "            \n",
    "            # Try to convert numeric fields\n",
    "            try:\n",
    "                if self.asset_cost_entry.get():\n",
    "                    metadata[\"asset_cost\"] = float(self.asset_cost_entry.get())\n",
    "                if self.import_tax_entry.get():\n",
    "                    metadata[\"import_tax\"] = float(self.import_tax_entry.get())\n",
    "                if self.freight_entry.get():\n",
    "                    metadata[\"freight\"] = float(self.freight_entry.get())\n",
    "                if self.ijarah_term_entry.get():\n",
    "                    metadata[\"ijarah_term\"] = int(self.ijarah_term_entry.get())\n",
    "                if self.residual_value_entry.get():\n",
    "                    metadata[\"residual_value\"] = float(self.residual_value_entry.get())\n",
    "                if self.purchase_price_entry.get():\n",
    "                    metadata[\"purchase_price\"] = float(self.purchase_price_entry.get())\n",
    "                if self.annual_rental_entry.get():\n",
    "                    metadata[\"annual_rental\"] = float(self.annual_rental_entry.get())\n",
    "            except ValueError:\n",
    "                messagebox.showwarning(\"Invalid Input\", \n",
    "                                     \"Please ensure all numeric fields contain valid numbers.\")\n",
    "                return\n",
    "                \n",
    "            # Add standard to metadata\n",
    "            if standard:\n",
    "                metadata[\"standard\"] = standard\n",
    "                \n",
    "            # Create the new use case\n",
    "            new_use_case = AccountingUseCase(\n",
    "                category=category,\n",
    "                subcategory=subcategory,\n",
    "                scenario=scenario,\n",
    "                question=question,\n",
    "                correct_solution=correct_solution,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            \n",
    "            # Add to the system\n",
    "            self.system.ijarah_agent.add_use_case(new_use_case, standard if standard else None)\n",
    "            \n",
    "            # Update the use cases display\n",
    "            self.refresh_use_cases_display()\n",
    "            \n",
    "            # Show success message\n",
    "            messagebox.showinfo(\"Success\", \"New use case added successfully.\")\n",
    "            \n",
    "            # Clear the form\n",
    "            self.clear_form()\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"An error occurred while adding the use case: {str(e)}\")\n",
    "    \n",
    "    def clear_form(self):\n",
    "        \"\"\"Clear all form fields in the add use case tab.\"\"\"\n",
    "        self.category_entry.set(\"\")\n",
    "        self.subcategory_entry.set(\"\")\n",
    "        self.standard_entry.set(\"\")\n",
    "        self.scenario_text.delete(1.0, tk.END)\n",
    "        self.question_text.delete(1.0, tk.END)\n",
    "        self.solution_text.delete(1.0, tk.END)\n",
    "        self.asset_cost_entry.delete(0, tk.END)\n",
    "        self.import_tax_entry.delete(0, tk.END)\n",
    "        self.freight_entry.delete(0, tk.END)\n",
    "        self.ijarah_term_entry.delete(0, tk.END)\n",
    "        self.residual_value_entry.delete(0, tk.END)\n",
    "        self.purchase_price_entry.delete(0, tk.END)\n",
    "        self.annual_rental_entry.delete(0, tk.END)\n",
    "    \n",
    "    def load_use_cases(self):\n",
    "        \"\"\"Load existing use cases and populate the UI.\"\"\"\n",
    "        # Get all use cases\n",
    "        use_cases = self.system.ijarah_agent.use_cases\n",
    "        \n",
    "        # Set up the display\n",
    "        self.refresh_use_cases_display()\n",
    "        \n",
    "        # Populate the category filter dropdown\n",
    "        categories = sorted(set(uc.category for uc in use_cases))\n",
    "        self.category_filter['values'] = [\"All Categories\"] + categories\n",
    "        self.category_filter.current(0)  # Default to \"All Categories\"\n",
    "    \n",
    "    def refresh_use_cases_display(self):\n",
    "        \"\"\"Refresh the display of use cases in the treeview.\"\"\"\n",
    "        # Clear existing items\n",
    "        for item in self.use_cases_tree.get_children():\n",
    "            self.use_cases_tree.delete(item)\n",
    "            \n",
    "        # Get filtered use cases\n",
    "        selected_category = self.category_filter.get()\n",
    "        use_cases = self.system.ijarah_agent.use_cases\n",
    "        \n",
    "        if selected_category and selected_category != \"All Categories\":\n",
    "            use_cases = [uc for uc in use_cases if uc.category == selected_category]\n",
    "            \n",
    "        # Add use cases to the treeview\n",
    "        for i, uc in enumerate(use_cases):\n",
    "            # Truncate long fields for display\n",
    "            scenario_preview = uc.scenario[:50] + \"...\" if len(uc.scenario) > 50 else uc.scenario\n",
    "            question_preview = uc.question[:50] + \"...\" if len(uc.question) > 50 else uc.question\n",
    "            \n",
    "            self.use_cases_tree.insert(\"\", \"end\", iid=i, text=str(i+1),\n",
    "                                     values=(uc.category, uc.subcategory, \n",
    "                                            scenario_preview, question_preview))\n",
    "    \n",
    "    def filter_use_cases(self, event=None):\n",
    "        \"\"\"Filter use cases based on the selected category.\"\"\"\n",
    "        self.refresh_use_cases_display()\n",
    "    \n",
    "    def export_use_cases(self):\n",
    "        \"\"\"Export use cases to a JSON file.\"\"\"\n",
    "        file_path = filedialog.asksaveasfilename(\n",
    "            defaultextension=\".json\",\n",
    "            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")],\n",
    "            title=\"Export Use Cases\"\n",
    "        )\n",
    "        \n",
    "        if not file_path:\n",
    "            return  # User cancelled\n",
    "            \n",
    "        try:\n",
    "            success = self.system.ijarah_agent.save_use_cases_to_json(file_path)\n",
    "            if success:\n",
    "                messagebox.showinfo(\"Export Successful\", \n",
    "                                  f\"Successfully exported {len(self.system.ijarah_agent.use_cases)} use cases to {file_path}\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Export Failed\", \n",
    "                                   \"Failed to export use cases. Please check console for details.\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Export Error\", \n",
    "                               f\"An error occurred during export: {str(e)}\")\n",
    "    \n",
    "    def import_use_cases(self):\n",
    "        \"\"\"Import use cases from a JSON file.\"\"\"\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            filetypes=[(\"JSON files\", \"*.json\"), (\"All files\", \"*.*\")],\n",
    "            title=\"Import Use Cases\"\n",
    "        )\n",
    "        \n",
    "        if not file_path:\n",
    "            return  # User cancelled\n",
    "            \n",
    "        try:\n",
    "            # Store current count for comparison\n",
    "            old_count = len(self.system.ijarah_agent.use_cases)\n",
    "            \n",
    "            # Import use cases\n",
    "            success = self.system.ijarah_agent.load_use_cases_from_json(file_path)\n",
    "            \n",
    "            if success:\n",
    "                # Refresh display\n",
    "                self.load_use_cases()\n",
    "                \n",
    "                # Calculate how many new use cases were added\n",
    "                new_count = len(self.system.ijarah_agent.use_cases)\n",
    "                added = new_count - old_count\n",
    "                \n",
    "                messagebox.showinfo(\"Import Successful\", \n",
    "                                  f\"Successfully imported {added} new use cases.\")\n",
    "            else:\n",
    "                messagebox.showerror(\"Import Failed\", \n",
    "                                   \"Failed to import use cases. Please check console for details.\")\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Import Error\", \n",
    "                               f\"An error occurred during import: {str(e)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to start the application.\"\"\"\n",
    "    root = tk.Tk()\n",
    "    app = IslamicFinanceAccountingApp(root)\n",
    "    root.mainloop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fa07a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 35)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:35\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef setup_basic_info_tab(self, parent):\u001b[39m\n                                           ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54d5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be6fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8bd9c08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e301f17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
